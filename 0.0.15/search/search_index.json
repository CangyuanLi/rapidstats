{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"rapidstats:","text":"<p> Documentation </p>"},{"location":"index.html#what-is-it","title":"What is it?","text":"<p>rapidstats is a minimal library that implements fast statistical routines in Rust and Polars. Currently, its core purpose is to provide the Bootstrap class. Unlike scipy.stats.bootstrap, the Bootstrap class contains specialized functions (for e.g. confusion matrix, ROC-AUC, and so on) implemented in Rust as well as a generic interface for any Python callable. This makes it significantly faster than passing in a callable on the Python side. For example, bootstrapping confusion matrix statistics can be up to 40x faster.</p> <p>This library is in an alpha state. Although all functions are tested against existing libraries, use at your own risk. The API is subject to change very frequently.</p>"},{"location":"index.html#usage","title":"Usage:","text":""},{"location":"index.html#dependencies","title":"Dependencies","text":"<p>rapidstats has a minimal set of dependencies. It only depends on Polars and tqdm (for progress bars). You may install pyarrow (<code>pip install rapidstats[pyarrow]</code>) to allow functions to take NumPy arrays, Pandas objects, and other objects that may be converted through Arrow.</p>"},{"location":"index.html#installing","title":"Installing","text":"<p>The easiest way is to install rapidstats is from PyPI using pip:</p> <pre><code>pip install rapidstats\n</code></pre>"},{"location":"index.html#notes","title":"Notes","text":"<p>The purpose of rapidstats is not to replace scipy or scikit-learn, which would be a massive undertaking. At the moment, only functions that are not easily achievable using existing libraries or functions that are significantly more performant (e.g. the core loop is implemented in Rust) will be added.</p>"},{"location":"bootstrap.html","title":"Bootstrap","text":""},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap","title":"<code>Bootstrap</code>","text":"<p>Computes a two-sided bootstrap confidence interval of a statistic. Note that \\( \\alpha \\) is then defined as \\( \\frac{1 - \\text{confidence}}{2} \\). Regardless of method, the result will be a three-tuple of (lower, mean, upper). The process is as follows:</p> <ul> <li>Resample 100% of the data with replacement for <code>iterations</code></li> <li>Compute the statistic on each resample</li> </ul> <p>If the method is <code>standard</code>,</p> <ul> <li>Compute the mean \\( \\hat{\\theta} \\) of the bootstrap statistics</li> <li> <p>Compute the standard error of the bootstrap statistics</p> \\[ \\hat{se} = \\frac{\\hat{\\sigma}}{\\sqrt{N}} \\] </li> <li> <p>Compute the Z-score</p> \\[ z_{\\alpha} = \\phi^{-1}(\\alpha) \\] </li> </ul> <p>where \\( \\phi^{-1} \\) is the quantile, inverse CDF, or percent-point function</p> <p>Then the \"Standard\" interval is</p> \\[ \\hat{\\theta} \\pm z_{\\alpha} \\times \\hat{se} \\] <p>If the method is <code>percentile</code>, we stop here and compute the interval of the bootstrap distribution that is symmetric about the median and contains <code>confidence</code> of the bootstrap statistics. Then the \"Percentile\" interval is</p> \\[     \\text{percentile}(\\hat{\\theta}^{*}, \\alpha),     \\text{percentile}(\\hat{\\theta}^{*}, 1 - \\alpha) \\] <p>where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics.</p> <p>If the method is <code>basic</code>,</p> <ul> <li>Compute the statistic on the original data</li> <li>Compute the \"Percentile\" interval</li> </ul> <p>Then the \"Basic\" or \"Reverse Percentile\" interval is</p> \\[     2\\hat{\\theta} - PCI_u,     2\\hat{\\theta} - PCI_l, \\] <p>where \\( \\hat{\\theta} \\) is the statistic on the original data, \\( PCI_u \\) is the upper bound of the \"Percentile\" interval, and \\( PCI_l \\) is the lower bound of the \"Percentile\" interval.</p> <p>If the method is <code>BCa</code>,</p> <ul> <li>Compute the statistic on the original data \\( \\hat{\\theta} \\)</li> <li>Compute the statistic on the data with the \\( i^{th} \\) row deleted (jacknife)</li> <li>Compute the bias correction factor as</li> </ul> \\[     \\hat{z_0} = \\phi^{-1}(         \\frac{\\sum_{i=1}^B \\hat{\\theta_i}^{*} \\le \\hat{\\theta}         + \\sum_{i=1}^B \\hat{\\theta_i}^{*} \\leq \\hat{\\theta}}{2 * B}     ) \\] <p>where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics and \\( B \\) is the length of that vector.</p> <ul> <li>Compute the acceleration factor as</li> </ul> \\[     \\hat{a} = \\frac{1}{6} \\frac{         \\sum_{i=1}^{N} (\\hat{\\theta_{(.)}} - \\hat{\\theta_i})^3     }{         \\sum_{i=1}^{N} [(\\hat{\\theta_{(.)}} - \\hat{\\theta_i})^2]^{1.5}     } \\] <p>where \\( \\hat{\\theta_{(.)}} \\) is the mean of the jacknife statistics and \\( \\hat{\\theta_i} \\) is the \\( i^{th} \\) element of the jacknife vector.</p> <ul> <li>Compute the lower and upper percentiles as</li> </ul> \\[     \\alpha_l = \\phi(         \\hat{z_0} + \\frac{\\hat{z_0} + z_{\\alpha}}{1 - \\hat{a}(\\hat{z} + z_{\\alpha})}     ) \\] <p>and</p> \\[     \\alpha_u = \\phi(         \\hat{z_0} + \\frac{             \\hat{z_0} + z_{1 - \\alpha}         }{             1 - \\hat{a}(\\hat{z} + z_{1-\\alpha})         }     ) \\] <p>Then the \"BCa\" or \"Bias-Corrected and Accelerated\" interval is</p> \\[     \\text{percentile}(\\hat{\\theta}^{*}, \\alpha_l),     \\text{percentile}(\\hat{\\theta}^{*}, \\alpha_u) \\] <p>where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>How many times to resample the data, by default 1_000</p> <code>1000</code> <code>confidence</code> <code>float</code> <p>The confidence level, by default 0.95</p> <code>0.95</code> <code>method</code> <code>Literal['standard', 'percentile', 'basic', 'BCa']</code> <p>Whether to return the Percentile, Basic / Reverse Percentile, or Bias Corrected and Accelerated Interval, by default \"percentile\"</p> <code>'percentile'</code> <code>seed</code> <code>Optional[int]</code> <p>Seed that controls resampling. Set this to any integer to make results reproducible, by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the method is not one of <code>standard</code>, <code>percentile</code>, <code>basic</code>, or <code>BCa</code></p> <p>Examples:</p> <pre><code>import rapidstats\nci = rapidstats.Bootstrap(seed=208).mean([1, 2, 3])\n</code></pre> <p>(1.0, 1.9783333333333328, 3.0)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>class Bootstrap:\n    r\"\"\"Computes a two-sided bootstrap confidence interval of a statistic. Note that\n    \\( \\alpha \\) is then defined as \\( \\frac{1 - \\text{confidence}}{2} \\). Regardless\n    of method, the result will be a three-tuple of (lower, mean, upper). The process is\n    as follows:\n\n    - Resample 100% of the data with replacement for `iterations`\n    - Compute the statistic on each resample\n\n    If the method is `standard`,\n\n    - Compute the mean \\( \\hat{\\theta} \\) of the bootstrap statistics\n    - Compute the standard error of the bootstrap statistics\n\n        \\[ \\hat{se} = \\frac{\\hat{\\sigma}}{\\sqrt{N}} \\]\n\n    - Compute the Z-score\n\n        \\[ z_{\\alpha} = \\phi^{-1}(\\alpha) \\]\n\n    where \\( \\phi^{-1} \\) is the quantile, inverse CDF, or percent-point function\n\n    Then the \"Standard\" interval is\n\n    \\[ \\hat{\\theta} \\pm z_{\\alpha} \\times \\hat{se} \\]\n\n    If the method is `percentile`, we stop here and compute the interval of the\n    bootstrap distribution that is symmetric about the median and contains\n    `confidence` of the bootstrap statistics. Then the \"Percentile\" interval is\n\n    \\[\n        \\text{percentile}(\\hat{\\theta}^{*}, \\alpha),\n        \\text{percentile}(\\hat{\\theta}^{*}, 1 - \\alpha)\n    \\]\n\n    where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics.\n\n    If the method is `basic`,\n\n    - Compute the statistic on the original data\n    - Compute the \"Percentile\" interval\n\n    Then the \"Basic\" or \"Reverse Percentile\" interval is\n\n    \\[\n        2\\hat{\\theta} - PCI_u,\n        2\\hat{\\theta} - PCI_l,\n    \\]\n\n    where \\( \\hat{\\theta} \\) is the statistic on the original data, \\( PCI_u \\) is the\n    upper bound of the \"Percentile\" interval, and \\( PCI_l \\) is the lower bound of the\n    \"Percentile\" interval.\n\n    If the method is `BCa`,\n\n    - Compute the statistic on the original data \\( \\hat{\\theta} \\)\n    - Compute the statistic on the data with the \\( i^{th} \\) row deleted (jacknife)\n    - Compute the bias correction factor as\n\n    \\[\n        \\hat{z_0} = \\phi^{-1}(\n            \\frac{\\sum_{i=1}^B \\hat{\\theta_i}^{*} \\le \\hat{\\theta}\n            + \\sum_{i=1}^B \\hat{\\theta_i}^{*} \\leq \\hat{\\theta}}{2 * B}\n        )\n    \\]\n\n    where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics and \\( B \\) is\n    the length of that vector.\n\n    - Compute the acceleration factor as\n\n    \\[\n        \\hat{a} = \\frac{1}{6} \\frac{\n            \\sum_{i=1}^{N} (\\hat{\\theta_{(.)}} - \\hat{\\theta_i})^3\n        }{\n            \\sum_{i=1}^{N} [(\\hat{\\theta_{(.)}} - \\hat{\\theta_i})^2]^{1.5}\n        }\n    \\]\n\n    where \\( \\hat{\\theta_{(.)}} \\) is the mean of the jacknife statistics and\n    \\( \\hat{\\theta_i} \\) is the \\( i^{th} \\) element of the jacknife vector.\n\n    - Compute the lower and upper percentiles as\n\n    \\[\n        \\alpha_l = \\phi(\n            \\hat{z_0} + \\frac{\\hat{z_0} + z_{\\alpha}}{1 - \\hat{a}(\\hat{z} + z_{\\alpha})}\n        )\n    \\]\n\n    and\n\n    \\[\n        \\alpha_u = \\phi(\n            \\hat{z_0} + \\frac{\n                \\hat{z_0} + z_{1 - \\alpha}\n            }{\n                1 - \\hat{a}(\\hat{z} + z_{1-\\alpha})\n            }\n        )\n    \\]\n\n    Then the \"BCa\" or \"Bias-Corrected and Accelerated\" interval is\n\n    \\[\n        \\text{percentile}(\\hat{\\theta}^{*}, \\alpha_l),\n        \\text{percentile}(\\hat{\\theta}^{*}, \\alpha_u)\n    \\]\n\n    where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics.\n\n    Parameters\n    ----------\n    iterations : int, optional\n        How many times to resample the data, by default 1_000\n    confidence : float, optional\n        The confidence level, by default 0.95\n    method : Literal[\"standard\", \"percentile\", \"basic\", \"BCa\"], optional\n        Whether to return the Percentile, Basic / Reverse Percentile, or\n        Bias Corrected and Accelerated Interval, by default \"percentile\"\n    seed : Optional[int], optional\n        Seed that controls resampling. Set this to any integer to make results\n        reproducible, by default None\n\n    Raises\n    ------\n    ValueError\n        If the method is not one of `standard`, `percentile`, `basic`, or `BCa`\n\n    Examples\n    --------\n    ``` py\n    import rapidstats\n    ci = rapidstats.Bootstrap(seed=208).mean([1, 2, 3])\n    ```\n    (1.0, 1.9783333333333328, 3.0)\n    \"\"\"\n\n    def __init__(\n        self,\n        iterations: int = 1_000,\n        confidence: float = 0.95,\n        method: Literal[\"standard\", \"percentile\", \"basic\", \"BCa\"] = \"percentile\",\n        seed: Optional[int] = None,\n    ) -&gt; None:\n        if method not in (\"standard\", \"percentile\", \"basic\", \"BCa\"):\n            raise ValueError(\n                f\"Invalid confidence interval method `{method}`, only `standard`, `percentile`, `basic`, and `BCa` are supported\",\n            )\n\n        self.iterations = iterations\n        self.confidence = confidence\n        self.seed = seed\n        self.alpha = (1 - confidence) / 2\n        self.method = method\n\n        self._params = {\n            \"iterations\": self.iterations,\n            \"alpha\": self.alpha,\n            \"method\": self.method,\n            \"seed\": self.seed,\n        }\n\n    def run(\n        self, df: pl.DataFrame, stat_func: StatFunc, **kwargs\n    ) -&gt; ConfidenceInterval:\n        \"\"\"Run bootstrap for an arbitrary function that accepts a Polars DataFrame and\n        returns a scalar real number.\n\n        Parameters\n        ----------\n        df : pl.DataFrame\n            The data to pass to `stat_func`\n        stat_func : StatFunc\n            A callable that takes a Polars DataFrame as its first argument and returns\n            a scalar real number.\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, higher)\n        \"\"\"\n        default = {\"executor\": \"threads\", \"preserve_order\": False}\n        for k, v in default.items():\n            if k not in kwargs:\n                kwargs[k] = v\n\n        func = functools.partial(_bs_func, df=df, stat_func=stat_func)\n\n        if self.seed is None:\n            iterable = (None for _ in range(self.iterations))\n        else:\n            iterable = (self.seed + i for i in range(self.iterations))\n\n        bootstrap_stats = [\n            x for x in _run_concurrent(func, iterable, **kwargs) if not math.isnan(x)\n        ]\n\n        if len(bootstrap_stats) == 0:\n            return (math.nan, math.nan, math.nan)\n\n        if self.method == \"standard\":\n            return _standard_interval(bootstrap_stats, self.alpha)\n        elif self.method == \"percentile\":\n            return _percentile_interval(bootstrap_stats, self.alpha)\n        elif self.method == \"basic\":\n            original_stat = stat_func(df)\n            return _basic_interval(original_stat, bootstrap_stats, self.alpha)\n        elif self.method == \"BCa\":\n            original_stat = stat_func(df)\n            jacknife_stats = _jacknife(df, stat_func)\n\n            return _bca_interval(\n                original_stat, bootstrap_stats, jacknife_stats, self.alpha\n            )\n\n    def confusion_matrix(\n        self,\n        y_true: ArrayLike,\n        y_pred: ArrayLike,\n    ) -&gt; BootstrappedConfusionMatrix:\n        \"\"\"Bootstrap confusion matrix. See [rapidstats.confusion_matrix][] for\n        more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_pred : ArrayLike\n            Predicted target\n\n        Returns\n        -------\n        BootstrappedConfusionMatrix\n            A dataclass of 25 confusion matrix metrics as (lower, mean, upper). See\n            [rapidstats._bootstrap.BootstrappedConfusionMatrix][] for more details.\n        \"\"\"\n        df = _y_true_y_pred_to_df(y_true, y_pred)\n\n        return BootstrappedConfusionMatrix(\n            *_bootstrap_confusion_matrix(df, **self._params)\n        )\n\n    def roc_auc(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap ROC-AUC. See [rapidstats.roc_auc][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, upper)\n        \"\"\"\n        df = _y_true_y_score_to_df(y_true, y_score)\n\n        return _bootstrap_roc_auc(df, **self._params)\n\n    def max_ks(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap Max-KS. See [rapidstats.max_ks][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, upper)\n        \"\"\"\n        df = _y_true_y_score_to_df(y_true, y_score)\n\n        return _bootstrap_max_ks(df, **self._params)\n\n    def brier_loss(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap Brier loss. See [rapidstats.brier_loss][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, upper)\n        \"\"\"\n        df = _y_true_y_score_to_df(y_true, y_score)\n\n        return _bootstrap_brier_loss(df, **self._params)\n\n    def mean(self, y: ArrayLike) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap mean.\n\n        Parameters\n        ----------\n        y : ArrayLike\n            A 1D-array\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, upper)\n        \"\"\"\n        df = pl.DataFrame({\"y\": y})\n\n        return _bootstrap_mean(df, **self._params)\n\n    def adverse_impact_ratio(\n        self, y_pred: ArrayLike, protected: ArrayLike, control: ArrayLike\n    ) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap AIR. See [rapidstats.adverse_impact_ratio][] for more details.\n\n        Parameters\n        ----------\n        y_pred : ArrayLike\n            Predicted target\n        protected : ArrayLike\n            An array of booleans identifying the protected class\n        control : ArrayLike\n            An array of booleans identifying the control class\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, upper)\n        \"\"\"\n        df = pl.DataFrame(\n            {\"y_pred\": y_pred, \"protected\": protected, \"control\": control}\n        ).cast(pl.Boolean)\n\n        return _bootstrap_adverse_impact_ratio(df, **self._params)\n\n    def mean_squared_error(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n        r\"\"\"Bootstrap MSE. See [rapidstats.mean_squared_error][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, upper)\n        \"\"\"\n        return _bootstrap_mean_squared_error(\n            _regression_to_df(y_true, y_score), **self._params\n        )\n\n    def root_mean_squared_error(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n        r\"\"\"Bootstrap RMSE. See [rapidstats.root_mean_squared_error][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, upper)\n        \"\"\"\n        return _bootstrap_root_mean_squared_error(\n            _regression_to_df(y_true, y_score), **self._params\n        )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.adverse_impact_ratio","title":"<code>adverse_impact_ratio(y_pred, protected, control)</code>","text":"<p>Bootstrap AIR. See rapidstats.adverse_impact_ratio for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>ArrayLike</code> <p>Predicted target</p> required <code>protected</code> <code>ArrayLike</code> <p>An array of booleans identifying the protected class</p> required <code>control</code> <code>ArrayLike</code> <p>An array of booleans identifying the control class</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, upper)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def adverse_impact_ratio(\n    self, y_pred: ArrayLike, protected: ArrayLike, control: ArrayLike\n) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap AIR. See [rapidstats.adverse_impact_ratio][] for more details.\n\n    Parameters\n    ----------\n    y_pred : ArrayLike\n        Predicted target\n    protected : ArrayLike\n        An array of booleans identifying the protected class\n    control : ArrayLike\n        An array of booleans identifying the control class\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, upper)\n    \"\"\"\n    df = pl.DataFrame(\n        {\"y_pred\": y_pred, \"protected\": protected, \"control\": control}\n    ).cast(pl.Boolean)\n\n    return _bootstrap_adverse_impact_ratio(df, **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.brier_loss","title":"<code>brier_loss(y_true, y_score)</code>","text":"<p>Bootstrap Brier loss. See rapidstats.brier_loss for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, upper)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def brier_loss(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap Brier loss. See [rapidstats.brier_loss][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, upper)\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score)\n\n    return _bootstrap_brier_loss(df, **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.confusion_matrix","title":"<code>confusion_matrix(y_true, y_pred)</code>","text":"<p>Bootstrap confusion matrix. See rapidstats.confusion_matrix for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_pred</code> <code>ArrayLike</code> <p>Predicted target</p> required <p>Returns:</p> Type Description <code>BootstrappedConfusionMatrix</code> <p>A dataclass of 25 confusion matrix metrics as (lower, mean, upper). See rapidstats._bootstrap.BootstrappedConfusionMatrix for more details.</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def confusion_matrix(\n    self,\n    y_true: ArrayLike,\n    y_pred: ArrayLike,\n) -&gt; BootstrappedConfusionMatrix:\n    \"\"\"Bootstrap confusion matrix. See [rapidstats.confusion_matrix][] for\n    more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_pred : ArrayLike\n        Predicted target\n\n    Returns\n    -------\n    BootstrappedConfusionMatrix\n        A dataclass of 25 confusion matrix metrics as (lower, mean, upper). See\n        [rapidstats._bootstrap.BootstrappedConfusionMatrix][] for more details.\n    \"\"\"\n    df = _y_true_y_pred_to_df(y_true, y_pred)\n\n    return BootstrappedConfusionMatrix(\n        *_bootstrap_confusion_matrix(df, **self._params)\n    )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.max_ks","title":"<code>max_ks(y_true, y_score)</code>","text":"<p>Bootstrap Max-KS. See rapidstats.max_ks for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, upper)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def max_ks(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap Max-KS. See [rapidstats.max_ks][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, upper)\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score)\n\n    return _bootstrap_max_ks(df, **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.mean","title":"<code>mean(y)</code>","text":"<p>Bootstrap mean.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ArrayLike</code> <p>A 1D-array</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, upper)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def mean(self, y: ArrayLike) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap mean.\n\n    Parameters\n    ----------\n    y : ArrayLike\n        A 1D-array\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, upper)\n    \"\"\"\n    df = pl.DataFrame({\"y\": y})\n\n    return _bootstrap_mean(df, **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.mean_squared_error","title":"<code>mean_squared_error(y_true, y_score)</code>","text":"<p>Bootstrap MSE. See rapidstats.mean_squared_error for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, upper)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def mean_squared_error(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    r\"\"\"Bootstrap MSE. See [rapidstats.mean_squared_error][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, upper)\n    \"\"\"\n    return _bootstrap_mean_squared_error(\n        _regression_to_df(y_true, y_score), **self._params\n    )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.roc_auc","title":"<code>roc_auc(y_true, y_score)</code>","text":"<p>Bootstrap ROC-AUC. See rapidstats.roc_auc for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, upper)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def roc_auc(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap ROC-AUC. See [rapidstats.roc_auc][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, upper)\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score)\n\n    return _bootstrap_roc_auc(df, **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.root_mean_squared_error","title":"<code>root_mean_squared_error(y_true, y_score)</code>","text":"<p>Bootstrap RMSE. See rapidstats.root_mean_squared_error for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, upper)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def root_mean_squared_error(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    r\"\"\"Bootstrap RMSE. See [rapidstats.root_mean_squared_error][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, upper)\n    \"\"\"\n    return _bootstrap_root_mean_squared_error(\n        _regression_to_df(y_true, y_score), **self._params\n    )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.run","title":"<code>run(df, stat_func, **kwargs)</code>","text":"<p>Run bootstrap for an arbitrary function that accepts a Polars DataFrame and returns a scalar real number.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The data to pass to <code>stat_func</code></p> required <code>stat_func</code> <code>StatFunc</code> <p>A callable that takes a Polars DataFrame as its first argument and returns a scalar real number.</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, higher)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def run(\n    self, df: pl.DataFrame, stat_func: StatFunc, **kwargs\n) -&gt; ConfidenceInterval:\n    \"\"\"Run bootstrap for an arbitrary function that accepts a Polars DataFrame and\n    returns a scalar real number.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        The data to pass to `stat_func`\n    stat_func : StatFunc\n        A callable that takes a Polars DataFrame as its first argument and returns\n        a scalar real number.\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, higher)\n    \"\"\"\n    default = {\"executor\": \"threads\", \"preserve_order\": False}\n    for k, v in default.items():\n        if k not in kwargs:\n            kwargs[k] = v\n\n    func = functools.partial(_bs_func, df=df, stat_func=stat_func)\n\n    if self.seed is None:\n        iterable = (None for _ in range(self.iterations))\n    else:\n        iterable = (self.seed + i for i in range(self.iterations))\n\n    bootstrap_stats = [\n        x for x in _run_concurrent(func, iterable, **kwargs) if not math.isnan(x)\n    ]\n\n    if len(bootstrap_stats) == 0:\n        return (math.nan, math.nan, math.nan)\n\n    if self.method == \"standard\":\n        return _standard_interval(bootstrap_stats, self.alpha)\n    elif self.method == \"percentile\":\n        return _percentile_interval(bootstrap_stats, self.alpha)\n    elif self.method == \"basic\":\n        original_stat = stat_func(df)\n        return _basic_interval(original_stat, bootstrap_stats, self.alpha)\n    elif self.method == \"BCa\":\n        original_stat = stat_func(df)\n        jacknife_stats = _jacknife(df, stat_func)\n\n        return _bca_interval(\n            original_stat, bootstrap_stats, jacknife_stats, self.alpha\n        )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.BootstrappedConfusionMatrix","title":"<code>BootstrappedConfusionMatrix</code>  <code>dataclass</code>","text":"<p>Result object returned by <code>rapidstats.Bootstrap().confusion_matrix</code>.</p> <p>See rapidstats._metrics.ConfusionMatrix for a detailed breakdown of the attributes stored in this class. However, instead of storing the statistic, it stores the bootstrapped confidence interval as (lower, mean, upper).</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>@dataclasses.dataclass\nclass BootstrappedConfusionMatrix:\n    \"\"\"Result object returned by `rapidstats.Bootstrap().confusion_matrix`.\n\n    See [rapidstats._metrics.ConfusionMatrix][] for a detailed breakdown of the attributes stored in\n    this class. However, instead of storing the statistic, it stores the bootstrapped\n    confidence interval as (lower, mean, upper).\n    \"\"\"\n\n    tn: ConfidenceInterval\n    fp: ConfidenceInterval\n    fn: ConfidenceInterval\n    tp: ConfidenceInterval\n    tpr: ConfidenceInterval\n    fpr: ConfidenceInterval\n    fnr: ConfidenceInterval\n    tnr: ConfidenceInterval\n    prevalence: ConfidenceInterval\n    prevalence_threshold: ConfidenceInterval\n    informedness: ConfidenceInterval\n    precision: ConfidenceInterval\n    false_omission_rate: ConfidenceInterval\n    plr: ConfidenceInterval\n    nlr: ConfidenceInterval\n    acc: ConfidenceInterval\n    balanced_accuracy: ConfidenceInterval\n    f1: ConfidenceInterval\n    folkes_mallows_index: ConfidenceInterval\n    mcc: ConfidenceInterval\n    threat_score: ConfidenceInterval\n    markedness: ConfidenceInterval\n    fdr: ConfidenceInterval\n    npv: ConfidenceInterval\n    dor: ConfidenceInterval\n\n    def to_polars(self) -&gt; pl.DataFrame:\n        \"\"\"Transform the dataclass to a long Polars DataFrame with columns\n        `metric`, `lower`, `mean`, and `upper`.\n\n        Returns\n        -------\n        pl.DataFrame\n            A DataFrame with columns `metric`, `lower`, `mean`, and `upper`\n        \"\"\"\n        dct = self.__dict__\n        lower = []\n        mean = []\n        upper = []\n        for l, m, u in dct.values():  # noqa: E741\n            lower.append(l)\n            mean.append(m)\n            upper.append(u)\n\n        return pl.DataFrame(\n            {\n                \"metric\": dct.keys(),\n                \"lower\": lower,\n                \"mean\": mean,\n                \"upper\": upper,\n            }\n        )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.BootstrappedConfusionMatrix.to_polars","title":"<code>to_polars()</code>","text":"<p>Transform the dataclass to a long Polars DataFrame with columns <code>metric</code>, <code>lower</code>, <code>mean</code>, and <code>upper</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with columns <code>metric</code>, <code>lower</code>, <code>mean</code>, and <code>upper</code></p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def to_polars(self) -&gt; pl.DataFrame:\n    \"\"\"Transform the dataclass to a long Polars DataFrame with columns\n    `metric`, `lower`, `mean`, and `upper`.\n\n    Returns\n    -------\n    pl.DataFrame\n        A DataFrame with columns `metric`, `lower`, `mean`, and `upper`\n    \"\"\"\n    dct = self.__dict__\n    lower = []\n    mean = []\n    upper = []\n    for l, m, u in dct.values():  # noqa: E741\n        lower.append(l)\n        mean.append(m)\n        upper.append(u)\n\n    return pl.DataFrame(\n        {\n            \"metric\": dct.keys(),\n            \"lower\": lower,\n            \"mean\": mean,\n            \"upper\": upper,\n        }\n    )\n</code></pre>"},{"location":"correlation.html","title":"Correlation","text":""},{"location":"correlation.html#rapidstats._corr.correlation_matrix","title":"<code>correlation_matrix(data, l1=None, l2=None, method='pearson')</code>","text":"<p>Compute the correlation matrix between two lists of columns. If both lists are None, then the correlation matrix is over all columns in the input DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[LazyFrame, DataFrame, ConvertibleToPolars]</code> <p>The input DataFrame. It must be either a Polars Frame or something convertible to a Polars Frame.</p> required <code>l1</code> <code>list[str]</code> <p>A list of columns to appear as the columns of the correlation matrix, by default None</p> <code>None</code> <code>l2</code> <code>list[str]</code> <p>A list of columns to appear as the rows of the correlation matrix, by default None</p> <code>None</code> <code>method</code> <code>CorrelationMethod</code> <p>How to calculate the correlation, by default \"pearson\"</p> <code>'pearson'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A correlation matrix with <code>l1</code> as the columns and <code>l2</code> as the rows</p> Source code in <code>python/rapidstats/_corr.py</code> <pre><code>def correlation_matrix(\n    data: Union[pl.LazyFrame, pl.DataFrame, ConvertibleToPolars],\n    l1: list[str] = None,\n    l2: list[str] = None,\n    method: CorrelationMethod = \"pearson\",\n) -&gt; pl.DataFrame:\n    \"\"\"Compute the correlation matrix between two lists of columns. If both lists are\n    None, then the correlation matrix is over all columns in the input DataFrame.\n\n    Parameters\n    ----------\n    data : Union[pl.LazyFrame, pl.DataFrame, ConvertibleToPolars]\n        The input DataFrame. It must be either a Polars Frame or something convertible\n        to a Polars Frame.\n    l1 : list[str], optional\n        A list of columns to appear as the columns of the correlation matrix,\n        by default None\n    l2 : list[str], optional\n        A list of columns to appear as the rows of the correlation matrix,\n        by default None\n    method : CorrelationMethod, optional\n        How to calculate the correlation, by default \"pearson\"\n\n    Returns\n    -------\n    pl.DataFrame\n        A correlation matrix with `l1` as the columns and `l2` as the rows\n    \"\"\"\n    pf = _to_polars(data)\n\n    if l1 is None and l2 is None:\n        original = pf.select(cs.numeric() | cs.boolean()).columns\n        new_columns = [f\"{i}\" for i, _ in enumerate(original)]\n        combinations = itertools.combinations(new_columns, r=2)\n        l1 = original[:-1]\n        l2 = original[1:]\n    else:\n        new_l1 = [f\"l{i}\" for i, _ in enumerate(l1)]\n        new_l2 = [f\"r{i}\" for i, _ in enumerate(l2)]\n        new_columns = new_l1 + new_l2\n        combinations = _pairs(new_l1, new_l2)\n        original = l1 + l2\n\n    corr_mat = (\n        pf.lazy()\n        .select(original)\n        .rename({old: new for old, new in zip(original, new_columns)})\n        .select(_corr_expr(c1, c2, method=method) for c1, c2 in combinations)\n        .melt()\n        .with_columns(pl.col(\"variable\").str.split(\"_\"))\n        .with_columns(\n            pl.col(\"variable\").list.get(0).alias(\"c1\"),\n            pl.col(\"variable\").list.get(1).alias(\"c2\"),\n        )\n        .drop(\"variable\")\n        .collect()\n        .pivot(index=\"c2\", columns=\"c1\", values=\"value\")\n        .drop(\"c2\")\n    )\n\n    corr_mat.columns = l1\n    corr_mat = corr_mat.with_columns(pl.Series(\"\", l2)).select(\"\", *l1)\n\n    return corr_mat\n</code></pre>"},{"location":"metrics.html","title":"Metrics","text":""},{"location":"metrics.html#rapidstats._metrics.ConfusionMatrix","title":"<code>ConfusionMatrix</code>  <code>dataclass</code>","text":"<p>Result object returned by <code>rapidstats.confusion_matrix</code></p> <p>Attributes:</p> Name Type Description <code>tn</code> <code>float</code> <p>\u2191Count of True Negatives; y_true == False and y_pred == False</p> <code>fp</code> <code>float</code> <p>\u2193Count of False Positives; y_true == False and y_pred == True</p> <code>fn</code> <code>float</code> <p>\u2193Count of False Negatives; y_true == True and y_pred == False</p> <code>tp</code> <code>float</code> <p>\u2191Count of True Positives; y_true == True, y_pred == True</p> <code>tpr</code> <code>float</code> <p>\u2191True Positive Rate, Recall, Sensitivity; Probability that an actual positive will be predicted positive; \\( \\frac{TP}{TP + FN} \\)</p> <code>fpr</code> <code>float</code> <p>\u2193False Positive Rate, Type I Error; Probability that an actual negative will be predicted positive; \\( \\frac{FP}{FP + TN} \\)</p> <code>fnr</code> <code>float</code> <p>\u2193False Negative Rate, Type II Error; Probability an actual positive will be predicted negative; \\( \\frac{FN}{TP + FN} \\)</p> <code>tnr</code> <code>float</code> <p>\u2191True Negative Rate, Specificity; Probability an actual negative will be predicted negative; \\( \\frac{TN}{FP + TN} \\)</p> <code>prevalence</code> <code>float</code> <p>Prevalence; Proportion of positive classes; \\( \\frac{TP + FN}{TN + FP + FN + TP} \\)</p> <code>prevalence_threshold</code> <code>float</code> <p>Prevalence Threshold; \\( \\frac{\\sqrt{TPR \\times FPR} - FPR}{TPR - FPR} \\)</p> <code>informedness</code> <code>float</code> <p>\u2191Informedness, Youden's J; \\( TPR + TNR - 1 \\)</p> <code>precision</code> <code>float</code> <p>\u2191Precision, Positive Predicted Value (PPV); Probability a predicted positive was actually correct; \\( \\frac{TP}{TP + FP} \\)</p> <code>false_omission_rate</code> <code>float</code> <p>\u2193False Omission Rate (FOR); Proportion of predicted negatives that were wrong \\( \\frac{FN}{FN + TN} \\)</p> <code>plr</code> <code>float</code> <p>\u2191Positive Likelihood Ratio, LR+; \\( \\frac{TPR}{FPR} \\)</p> <code>nlr</code> <code>float</code> <p>Negative Likelihood Ratio, LR-; \\( \\frac{FNR}{TNR} \\)</p> <code>acc</code> <code>float</code> <p>\u2191Accuracy (ACC); Probability of a correct prediction; \\( \\frac{TP + TN}{TN + FP + FN + TP} \\)</p> <code>balanced_accuracy</code> <code>float</code> <p>\u2191Balanced Accuracy (BA); \\( \\frac{TP + TN}{2} \\)</p> <code>f1</code> <code>float</code> <p>\u2191F1; Harmonic mean of Precision and Recall; \\( \\frac{2 \\times PPV \\times TPR}{PPV + TPR} \\)</p> <code>folkes_mallows_index</code> <code>float</code> <p>\u2191Folkes Mallows Index (FM); \\( \\sqrt{PPV \\times TPR} \\)</p> <code>mcc</code> <code>float</code> <p>\u2191Matthew Correlation Coefficient (MCC), Yule Phi Coefficient; \\( \\sqrt{TPR \\times TNR \\times PPV \\times NPV} - \\sqrt{FNR \\times FPR \\times FOR \\times FDR} \\)</p> <code>threat_score</code> <code>float</code> <p>\u2191Threat Score (TS), Critical Success Index (CSI), Jaccard Index; \\( \\frac{TP}{TP + FN + FP} \\)</p> <code>markedness</code> <code>float</code> <p>Markedness (MP), deltaP; \\( PPV + NPV - 1 \\)</p> <code>fdr</code> <code>float</code> <p>\u2193False Discovery Rate, Proportion of predicted positives that are wrong; \\( \\frac{FP}{TP + FP} \\)</p> <code>\u2191npv</code> <code>float</code> <p>Negative Predictive Value; Proportion of predicted negatives that are correct; \\( \\frac{TN}{FN + TN} \\)</p> <code>dor</code> <code>float</code> <p>Diagnostic Odds Ratio; \\( \\frac{LR+}{LR-} \\)</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>@dataclasses.dataclass\nclass ConfusionMatrix:\n    r\"\"\"Result object returned by `rapidstats.confusion_matrix`\n\n    Attributes\n    ----------\n    tn : float\n        \u2191Count of True Negatives; y_true == False and y_pred == False\n    fp : float\n        \u2193Count of False Positives; y_true == False and y_pred == True\n    fn : float\n        \u2193Count of False Negatives; y_true == True and y_pred == False\n    tp : float\n        \u2191Count of True Positives; y_true == True, y_pred == True\n    tpr : float\n        \u2191True Positive Rate, Recall, Sensitivity; Probability that an actual positive\n        will be predicted positive; \\( \\frac{TP}{TP + FN} \\)\n    fpr : float\n        \u2193False Positive Rate, Type I Error; Probability that an actual negative will\n        be predicted positive; \\( \\frac{FP}{FP + TN} \\)\n    fnr : float\n        \u2193False Negative Rate, Type II Error; Probability an actual positive will be\n        predicted negative; \\( \\frac{FN}{TP + FN} \\)\n    tnr : float\n        \u2191True Negative Rate, Specificity; Probability an actual negative will be\n        predicted negative; \\( \\frac{TN}{FP + TN} \\)\n    prevalence : float\n        Prevalence; Proportion of positive classes; \\( \\frac{TP + FN}{TN + FP + FN + TP} \\)\n    prevalence_threshold : float\n        Prevalence Threshold; \\( \\frac{\\sqrt{TPR \\times FPR} - FPR}{TPR - FPR} \\)\n    informedness : float\n        \u2191Informedness, Youden's J; \\( TPR + TNR - 1 \\)\n    precision : float\n        \u2191Precision, Positive Predicted Value (PPV); Probability a predicted positive was\n        actually correct; \\( \\frac{TP}{TP + FP} \\)\n    false_omission_rate : float\n        \u2193False Omission Rate (FOR); Proportion of predicted negatives that were wrong\n        \\( \\frac{FN}{FN + TN} \\)\n    plr : float\n        \u2191Positive Likelihood Ratio, LR+; \\( \\frac{TPR}{FPR} \\)\n    nlr : float\n        Negative Likelihood Ratio, LR-; \\( \\frac{FNR}{TNR} \\)\n    acc : float\n        \u2191Accuracy (ACC); Probability of a correct prediction; \\( \\frac{TP + TN}{TN + FP + FN + TP} \\)\n    balanced_accuracy : float\n        \u2191Balanced Accuracy (BA); \\( \\frac{TP + TN}{2} \\)\n    f1 : float\n        \u2191F1; Harmonic mean of Precision and Recall; \\( \\frac{2 \\times PPV \\times TPR}{PPV + TPR} \\)\n    folkes_mallows_index : float\n        \u2191Folkes Mallows Index (FM); \\( \\sqrt{PPV \\times TPR} \\)\n    mcc : float\n        \u2191Matthew Correlation Coefficient (MCC), Yule Phi Coefficient; \\( \\sqrt{TPR \\times TNR \\times PPV \\times NPV} - \\sqrt{FNR \\times FPR \\times FOR \\times FDR} \\)\n    threat_score : float\n        \u2191Threat Score (TS), Critical Success Index (CSI), Jaccard Index; \\( \\frac{TP}{TP + FN + FP} \\)\n    markedness : float\n        Markedness (MP), deltaP; \\( PPV + NPV - 1 \\)\n    fdr : float\n        \u2193False Discovery Rate, Proportion of predicted positives that are wrong; \\( \\frac{FP}{TP + FP} \\)\n    \u2191npv : float\n        Negative Predictive Value; Proportion of predicted negatives that are correct; \\( \\frac{TN}{FN + TN} \\)\n    dor : float\n        Diagnostic Odds Ratio; \\( \\frac{LR+}{LR-} \\)\n    \"\"\"\n\n    tn: float\n    fp: float\n    fn: float\n    tp: float\n    tpr: float\n    fpr: float\n    fnr: float\n    tnr: float\n    prevalence: float\n    prevalence_threshold: float\n    informedness: float\n    precision: float\n    false_omission_rate: float\n    plr: float\n    nlr: float\n    acc: float\n    balanced_accuracy: float\n    f1: float\n    folkes_mallows_index: float\n    mcc: float\n    threat_score: float\n    markedness: float\n    fdr: float\n    npv: float\n    dor: float\n\n    def to_polars(self) -&gt; pl.DataFrame:\n        \"\"\"Convert the dataclass to a long Polars DataFrame with columns `metric` and\n        `value`.\n\n        Returns\n        -------\n        pl.DataFrame\n            DataFrame with columns `metric` and `value`\n        \"\"\"\n        dct = self.__dict__\n\n        return pl.DataFrame({\"metric\": dct.keys(), \"value\": dct.values()})\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.ConfusionMatrix.to_polars","title":"<code>to_polars()</code>","text":"<p>Convert the dataclass to a long Polars DataFrame with columns <code>metric</code> and <code>value</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns <code>metric</code> and <code>value</code></p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def to_polars(self) -&gt; pl.DataFrame:\n    \"\"\"Convert the dataclass to a long Polars DataFrame with columns `metric` and\n    `value`.\n\n    Returns\n    -------\n    pl.DataFrame\n        DataFrame with columns `metric` and `value`\n    \"\"\"\n    dct = self.__dict__\n\n    return pl.DataFrame({\"metric\": dct.keys(), \"value\": dct.values()})\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.adverse_impact_ratio","title":"<code>adverse_impact_ratio(y_pred, protected, control)</code>","text":"<p>Computes the ratio of positive predictions for the protected class and the control class. The ideal ratio is 1. For example, in an underwriting context, this means that the model is equally as likely to approve protected applicants as it is unprotected applicants.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>ArrayLike</code> <p>Predicted target</p> required <code>protected</code> <code>ArrayLike</code> <p>An array of booleans identifying the protected class</p> required <code>control</code> <code>ArrayLike</code> <p>An array of booleans identifying the control class</p> required <p>Returns:</p> Type Description <code>float</code> <p>Adverse Impact Ratio (AIR)</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def adverse_impact_ratio(\n    y_pred: ArrayLike,\n    protected: ArrayLike,\n    control: ArrayLike,\n) -&gt; float:\n    \"\"\"Computes the ratio of positive predictions for the protected class and the\n    control class. The ideal ratio is 1. For example, in an underwriting context, this\n    means that the model is equally as likely to approve protected applicants as it is\n    unprotected applicants.\n\n    Parameters\n    ----------\n    y_pred : ArrayLike\n        Predicted target\n    protected : ArrayLike\n        An array of booleans identifying the protected class\n    control : ArrayLike\n        An array of booleans identifying the control class\n\n    Returns\n    -------\n    float\n        Adverse Impact Ratio (AIR)\n    \"\"\"\n    return _adverse_impact_ratio(\n        pl.DataFrame(\n            {\"y_pred\": y_pred, \"protected\": protected, \"control\": control}\n        ).cast(pl.Boolean)\n    )\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.brier_loss","title":"<code>brier_loss(y_true, y_score)</code>","text":"<p>Computes the Brier loss (smaller is better). The Brier loss measures the mean squared difference between the predicted scores and the ground truth target. Calculated as:</p> \\[ \\frac{1}{N} \\sum_{i=1}^N (yt_i - ys_i)^2 \\] <p>where \\( yt \\) is <code>y_true</code> and \\( ys \\) is <code>y_score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>float</code> <p>Brier loss</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def brier_loss(y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    r\"\"\"Computes the Brier loss (smaller is better). The Brier loss measures the mean\n    squared difference between the predicted scores and the ground truth target.\n    Calculated as:\n\n    \\[ \\frac{1}{N} \\sum_{i=1}^N (yt_i - ys_i)^2 \\]\n\n    where \\( yt \\) is `y_true` and \\( ys \\) is `y_score`.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    float\n        Brier loss\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score)\n\n    return _brier_loss(df)\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.confusion_matrix","title":"<code>confusion_matrix(y_true, y_pred)</code>","text":"<p>Computes the 25 confusion matrix metrics (TP, FP, TN, FN, TPR, F1, etc.). Please see https://en.wikipedia.org/wiki/Confusion_matrix for a list of all confusion matrix metrics and their formulas.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_pred</code> <code>ArrayLike</code> <p>Predicted target</p> required <p>Returns:</p> Type Description <code>ConfusionMatrix</code> <p>Dataclass of 25 confusion matrix metrics</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def confusion_matrix(y_true: ArrayLike, y_pred: ArrayLike) -&gt; ConfusionMatrix:\n    \"\"\"Computes the 25 confusion matrix metrics (TP, FP, TN, FN, TPR, F1, etc.). Please\n    see https://en.wikipedia.org/wiki/Confusion_matrix for a list of all confusion\n    matrix metrics and their formulas.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_pred : ArrayLike\n        Predicted target\n\n    Returns\n    -------\n    ConfusionMatrix\n        Dataclass of 25 confusion matrix metrics\n    \"\"\"\n    df = _y_true_y_pred_to_df(y_true, y_pred)\n\n    return ConfusionMatrix(*_confusion_matrix(df))\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.max_ks","title":"<code>max_ks(y_true, y_score)</code>","text":"<p>Performs the two-sample Kolmogorov-Smirnov test on the predicted scores of the ground truth positive and ground truth negative classes. The KS test measures the highest distance between two CDFs, so the Max-KS metric measures how well the model separates two classes. In pseucode:</p> <pre><code>df = Frame(y_true, y_score)\nclass0 = df.filter(~y_true)[\"y_score\"]\nclass1 = df.filter(y_true)[\"y_score\"]\n\nks(class0, class1)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>float</code> <p>Max-KS</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def max_ks(y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    \"\"\"Performs the two-sample Kolmogorov-Smirnov test on the predicted scores of the\n    ground truth positive and ground truth negative classes. The KS test measures the\n    highest distance between two CDFs, so the Max-KS metric measures how well the model\n    separates two classes. In pseucode:\n\n    ``` py\n    df = Frame(y_true, y_score)\n    class0 = df.filter(~y_true)[\"y_score\"]\n    class1 = df.filter(y_true)[\"y_score\"]\n\n    ks(class0, class1)\n    ```\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    float\n        Max-KS\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score)\n\n    return _max_ks(df)\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.mean","title":"<code>mean(y)</code>","text":"<p>Computes the mean of the input array.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ArrayLike</code> <p>A 1D-array of numbers</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def mean(y: ArrayLike) -&gt; float:\n    \"\"\"Computes the mean of the input array.\n\n    Parameters\n    ----------\n    y : ArrayLike\n        A 1D-array of numbers\n\n    Returns\n    -------\n    float\n        Mean\n    \"\"\"\n    return _mean(pl.DataFrame({\"y\": y}))\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.mean_squared_error","title":"<code>mean_squared_error(y_true, y_score)</code>","text":"<p>Computes Mean Squared Error (MSE) as</p> \\[ \\frac{1}{N} \\sum_{i=1}^{N} (yt_i - ys_i)^2 \\] <p>where \\( yt \\) is <code>y_true</code> and \\( ys \\) is <code>y_score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean Squared Error (MSE)</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def mean_squared_error(y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    r\"\"\"Computes Mean Squared Error (MSE) as\n\n    \\[ \\frac{1}{N} \\sum_{i=1}^{N} (yt_i - ys_i)^2 \\]\n\n    where \\( yt \\) is `y_true` and \\( ys \\) is `y_score`.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    float\n        Mean Squared Error (MSE)\n    \"\"\"\n    return _mean_squared_error(_regression_to_df(y_true, y_score))\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.roc_auc","title":"<code>roc_auc(y_true, y_score)</code>","text":"<p>Computes Area Under the Receiver Operating Characteristic Curve.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>float</code> <p>ROC-AUC</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def roc_auc(y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    \"\"\"Computes Area Under the Receiver Operating Characteristic Curve.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    float\n        ROC-AUC\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score)\n\n    return _roc_auc(df)\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.root_mean_squared_error","title":"<code>root_mean_squared_error(y_true, y_score)</code>","text":"<p>Computes Root Mean Squared Error (RMSE) as</p> \\[ \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (yt_i - ys_i)^2} \\] <p>where \\( yt \\) is <code>y_true</code> and \\( ys \\) is <code>y_score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>float</code> <p>Root Mean Squared Error (RMSE)</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def root_mean_squared_error(y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    r\"\"\"Computes Root Mean Squared Error (RMSE) as\n\n    \\[ \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (yt_i - ys_i)^2} \\]\n\n    where \\( yt \\) is `y_true` and \\( ys \\) is `y_score`.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    float\n        Root Mean Squared Error (RMSE)\n    \"\"\"\n    return _root_mean_squared_error(_regression_to_df(y_true, y_score))\n</code></pre>"}]}