{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"rapidstats:","text":"<p> Documentation </p>"},{"location":"index.html#what-is-it","title":"What is it?","text":"<p>rapidstats is a minimal library that implements fast statistical routines in Rust and Polars. Currently, its core purpose is to provide the Bootstrap class. Unlike scipy.stats.bootstrap, the Bootstrap class contains specialized functions (for e.g. confusion matrix, ROC-AUC, and so on) implemented in Rust as well as a generic interface for any Python callable. This makes it significantly faster than passing in a callable on the Python side. For example, bootstrapping confusion matrix statistics can be up to 40x faster.</p> <p>This library is in an alpha state. Although all functions are tested against existing libraries, use at your own risk. The API is subject to change very frequently.</p>"},{"location":"index.html#usage","title":"Usage:","text":""},{"location":"index.html#dependencies","title":"Dependencies","text":"<p>rapidstats has a minimal set of dependencies. It only depends on Polars and tqdm (for progress bars). You may install pyarrow (<code>pip install rapidstats[pyarrow]</code>) to allow functions to take NumPy arrays, Pandas objects, and other objects that may be converted through Arrow.</p>"},{"location":"index.html#installing","title":"Installing","text":"<p>The easiest way is to install rapidstats is from PyPI using pip:</p> <pre><code>pip install rapidstats\n</code></pre>"},{"location":"index.html#notes","title":"Notes","text":"<p>The purpose of rapidstats is not to replace scipy or scikit-learn, which would be a massive undertaking. At the moment, only functions that are not easily achievable using existing libraries or functions that are significantly more performant (e.g. the core loop is implemented in Rust) will be added.</p>"},{"location":"bootstrap.html","title":"Bootstrap","text":""},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap","title":"<code>Bootstrap</code>","text":"<p>Computes a two-sided bootstrap confidence interval of a statistic. Note that \\( \\alpha \\) is then defined as \\( \\frac{1 - \\text{confidence}}{2} \\). Regardless of method, the result will be a three-tuple of (lower, mean, upper). The process is as follows:</p> <ul> <li>Resample 100% of the data with replacement for <code>iterations</code></li> <li>Compute the statistic on each resample</li> </ul> <p>If the method is <code>standard</code>,</p> <ul> <li>Compute the mean \\( \\hat{\\theta} \\) of the bootstrap statistics</li> <li>Compute the standard error of the bootstrap statistics. Note that the standard error of any statistic is defined as the standard deviation of its sampling distribution.</li> <li> <p>Compute the Z-score</p> \\[ z_{\\alpha} = \\phi^{-1}(\\alpha) \\] <p>where \\( \\phi^{-1} \\) is the quantile, inverse CDF, or percent-point function</p> </li> </ul> <p>Then the \"Standard\" or \"First-Order Normal Approximation\" interval is</p> \\[ \\hat{\\theta} \\pm z_{\\alpha} \\times \\hat{\\sigma} \\] <p>If the method is <code>percentile</code>, we stop here and compute the interval of the bootstrap distribution that is symmetric about the median and contains <code>confidence</code> of the bootstrap statistics. Then the \"Percentile\" interval is</p> \\[     [\\text{percentile}(\\hat{\\theta}^{*}, \\alpha),     \\text{percentile}(\\hat{\\theta}^{*}, 1 - \\alpha)] \\] <p>where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics.</p> <p>If the method is <code>basic</code>,</p> <ul> <li>Compute the statistic on the original data</li> <li>Compute the \"Percentile\" interval</li> </ul> <p>Then the \"Basic\" or \"Reverse Percentile\" interval is</p> \\[     [2\\hat{\\theta} - PCI_u,     2\\hat{\\theta} - PCI_l,] \\] <p>where \\( \\hat{\\theta} \\) is the statistic on the original data, \\( PCI_u \\) is the upper bound of the \"Percentile\" interval, and \\( PCI_l \\) is the lower bound of the \"Percentile\" interval.</p> <p>If the method is <code>BCa</code>,</p> <ul> <li>Compute the statistic on the original data \\( \\hat{\\theta} \\)</li> <li>Compute the statistic on the data with the \\( i^{th} \\) row deleted (jacknife)</li> <li> <p>Compute the bias correction factor as</p> \\[     \\hat{z_0} = \\phi^{-1}(         \\frac{\\sum_{i=1}^B \\hat{\\theta_i}^{*} \\le \\hat{\\theta}         + \\sum_{i=1}^B \\hat{\\theta_i}^{*} \\leq \\hat{\\theta}}{2 * B}     ) \\] <p>where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics and \\( B \\) is the length of that vector.</p> </li> <li> <p>Compute the acceleration factor as</p> \\[     \\hat{a} = \\frac{1}{6} \\frac{         \\sum_{i=1}^{N} (\\hat{\\theta_{(.)}} - \\hat{\\theta_i})^3     }{         \\sum_{i=1}^{N} [(\\hat{\\theta_{(.)}} - \\hat{\\theta_i})^2]^{1.5}     } \\] <p>where \\( \\hat{\\theta_{(.)}} \\) is the mean of the jacknife statistics and \\( \\hat{\\theta_i} \\) is the \\( i^{th} \\) element of the jacknife vector.</p> </li> <li> <p>Compute the lower and upper percentiles as</p> \\[     \\alpha_l = \\phi(         \\hat{z_0} + \\frac{\\hat{z_0} + z_{\\alpha}}{1 - \\hat{a}(\\hat{z} + z_{\\alpha})}     ) \\] <p>and</p> \\[     \\alpha_u = \\phi(         \\hat{z_0} + \\frac{             \\hat{z_0} + z_{1 - \\alpha}         }{             1 - \\hat{a}(\\hat{z} + z_{1-\\alpha})         }     ) \\] </li> </ul> <p>Then the \"BCa\" or \"Bias-Corrected and Accelerated\" interval is</p> \\[     [\\text{percentile}(\\hat{\\theta}^{*}, \\alpha_l),     \\text{percentile}(\\hat{\\theta}^{*}, \\alpha_u)] \\] <p>where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>How many times to resample the data, by default 1_000</p> <code>1000</code> <code>confidence</code> <code>float</code> <p>The confidence level, by default 0.95</p> <code>0.95</code> <code>method</code> <code>Literal['standard', 'percentile', 'basic', 'BCa']</code> <p>Whether to return the Percentile, Basic / Reverse Percentile, or Bias Corrected and Accelerated Interval, by default \"percentile\"</p> <code>'percentile'</code> <code>seed</code> <code>Optional[int]</code> <p>Seed that controls resampling. Set this to any integer to make results reproducible, by default None</p> <code>None</code> <code>n_jobs</code> <code>Optional[int]</code> <p>How many threads to run with. None means let the executor decide, and 1 means run sequentially, by default None</p> <code>None</code> <code>chunksize</code> <code>Optional[int]</code> <p>The chunksize for each thread. None means let the executor decide, by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the method is not one of <code>standard</code>, <code>percentile</code>, <code>basic</code>, or <code>BCa</code></p> <p>Examples:</p> <p><pre><code>import rapidstats\nci = rapidstats.Bootstrap(seed=208).mean([1, 2, 3])\n</code></pre> (1.0, 1.9783333333333328, 3.0)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>class Bootstrap:\n    r\"\"\"Computes a two-sided bootstrap confidence interval of a statistic. Note that\n    \\( \\alpha \\) is then defined as \\( \\frac{1 - \\text{confidence}}{2} \\). Regardless\n    of method, the result will be a three-tuple of (lower, mean, upper). The process is\n    as follows:\n\n    - Resample 100% of the data with replacement for `iterations`\n    - Compute the statistic on each resample\n\n    If the method is `standard`,\n\n    - Compute the mean \\( \\hat{\\theta} \\) of the bootstrap statistics\n    - Compute the standard error of the bootstrap statistics. Note that the standard\n    error of any statistic is defined as the standard deviation of its sampling\n    distribution.\n    - Compute the Z-score\n\n        \\[ z_{\\alpha} = \\phi^{-1}(\\alpha) \\]\n\n        where \\( \\phi^{-1} \\) is the quantile, inverse CDF, or percent-point function\n\n    Then the \"Standard\" or \"First-Order Normal Approximation\" interval is\n\n    \\[ \\hat{\\theta} \\pm z_{\\alpha} \\times \\hat{\\sigma} \\]\n\n    If the method is `percentile`, we stop here and compute the interval of the\n    bootstrap distribution that is symmetric about the median and contains\n    `confidence` of the bootstrap statistics. Then the \"Percentile\" interval is\n\n    \\[\n        [\\text{percentile}(\\hat{\\theta}^{*}, \\alpha),\n        \\text{percentile}(\\hat{\\theta}^{*}, 1 - \\alpha)]\n    \\]\n\n    where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics.\n\n    If the method is `basic`,\n\n    - Compute the statistic on the original data\n    - Compute the \"Percentile\" interval\n\n    Then the \"Basic\" or \"Reverse Percentile\" interval is\n\n    \\[\n        [2\\hat{\\theta} - PCI_u,\n        2\\hat{\\theta} - PCI_l,]\n    \\]\n\n    where \\( \\hat{\\theta} \\) is the statistic on the original data, \\( PCI_u \\) is the\n    upper bound of the \"Percentile\" interval, and \\( PCI_l \\) is the lower bound of the\n    \"Percentile\" interval.\n\n    If the method is `BCa`,\n\n    - Compute the statistic on the original data \\( \\hat{\\theta} \\)\n    - Compute the statistic on the data with the \\( i^{th} \\) row deleted (jacknife)\n    - Compute the bias correction factor as\n\n        \\[\n            \\hat{z_0} = \\phi^{-1}(\n                \\frac{\\sum_{i=1}^B \\hat{\\theta_i}^{*} \\le \\hat{\\theta}\n                + \\sum_{i=1}^B \\hat{\\theta_i}^{*} \\leq \\hat{\\theta}}{2 * B}\n            )\n        \\]\n\n        where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics and \\( B \\)\n        is the length of that vector.\n\n    - Compute the acceleration factor as\n\n        \\[\n            \\hat{a} = \\frac{1}{6} \\frac{\n                \\sum_{i=1}^{N} (\\hat{\\theta_{(.)}} - \\hat{\\theta_i})^3\n            }{\n                \\sum_{i=1}^{N} [(\\hat{\\theta_{(.)}} - \\hat{\\theta_i})^2]^{1.5}\n            }\n        \\]\n\n        where \\( \\hat{\\theta_{(.)}} \\) is the mean of the jacknife statistics and\n        \\( \\hat{\\theta_i} \\) is the \\( i^{th} \\) element of the jacknife vector.\n\n    - Compute the lower and upper percentiles as\n\n        \\[\n            \\alpha_l = \\phi(\n                \\hat{z_0} + \\frac{\\hat{z_0} + z_{\\alpha}}{1 - \\hat{a}(\\hat{z} + z_{\\alpha})}\n            )\n        \\]\n\n        and\n\n        \\[\n            \\alpha_u = \\phi(\n                \\hat{z_0} + \\frac{\n                    \\hat{z_0} + z_{1 - \\alpha}\n                }{\n                    1 - \\hat{a}(\\hat{z} + z_{1-\\alpha})\n                }\n            )\n        \\]\n\n    Then the \"BCa\" or \"Bias-Corrected and Accelerated\" interval is\n\n    \\[\n        [\\text{percentile}(\\hat{\\theta}^{*}, \\alpha_l),\n        \\text{percentile}(\\hat{\\theta}^{*}, \\alpha_u)]\n    \\]\n\n    where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics.\n\n    Parameters\n    ----------\n    iterations : int, optional\n        How many times to resample the data, by default 1_000\n    confidence : float, optional\n        The confidence level, by default 0.95\n    method : Literal[\"standard\", \"percentile\", \"basic\", \"BCa\"], optional\n        Whether to return the Percentile, Basic / Reverse Percentile, or\n        Bias Corrected and Accelerated Interval, by default \"percentile\"\n    seed : Optional[int], optional\n        Seed that controls resampling. Set this to any integer to make results\n        reproducible, by default None\n    n_jobs: Optional[int], optional\n        How many threads to run with. None means let the executor decide, and 1 means\n        run sequentially, by default None\n    chunksize: Optional[int], optional\n        The chunksize for each thread. None means let the executor decide, by default\n        None\n\n    Raises\n    ------\n    ValueError\n        If the method is not one of `standard`, `percentile`, `basic`, or `BCa`\n\n    Examples\n    --------\n    ``` py\n    import rapidstats\n    ci = rapidstats.Bootstrap(seed=208).mean([1, 2, 3])\n    ```\n    (1.0, 1.9783333333333328, 3.0)\n    \"\"\"\n\n    def __init__(\n        self,\n        iterations: int = 1_000,\n        confidence: float = 0.95,\n        method: Literal[\"standard\", \"percentile\", \"basic\", \"BCa\"] = \"percentile\",\n        seed: Optional[int] = None,\n        n_jobs: Optional[int] = None,\n        chunksize: Optional[int] = None,\n    ) -&gt; None:\n        if method not in (\"standard\", \"percentile\", \"basic\", \"BCa\"):\n            raise ValueError(\n                f\"Invalid confidence interval method `{method}`, only `standard`, `percentile`, `basic`, and `BCa` are supported\",\n            )\n\n        self.iterations = iterations\n        self.confidence = confidence\n        self.seed = seed\n        self.alpha = (1 - confidence) / 2\n        self.method = method\n        self.n_jobs = n_jobs\n        self.chunksize = chunksize\n\n        self._params = {\n            \"iterations\": self.iterations,\n            \"alpha\": self.alpha,\n            \"method\": self.method,\n            \"seed\": self.seed,\n            \"n_jobs\": self.n_jobs,\n            \"chunksize\": self.chunksize,\n        }\n\n    def run(\n        self, df: pl.DataFrame, stat_func: StatFunc, **kwargs\n    ) -&gt; ConfidenceInterval:\n        \"\"\"Run bootstrap for an arbitrary function that accepts a Polars DataFrame and\n        returns a scalar real number.\n\n        Parameters\n        ----------\n        df : pl.DataFrame\n            The data to pass to `stat_func`\n        stat_func : StatFunc\n            A callable that takes a Polars DataFrame as its first argument and returns\n            a scalar real number.\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, higher)\n        \"\"\"\n        default = {\"executor\": \"threads\", \"preserve_order\": False}\n        for k, v in default.items():\n            if k not in kwargs:\n                kwargs[k] = v\n\n        func = functools.partial(_bs_func, df=df, stat_func=stat_func)\n\n        if self.seed is None:\n            iterable = (None for _ in range(self.iterations))\n        else:\n            iterable = (self.seed + i for i in range(self.iterations))\n\n        bootstrap_stats = [\n            x for x in _run_concurrent(func, iterable, **kwargs) if not math.isnan(x)\n        ]\n\n        if len(bootstrap_stats) == 0:\n            return (math.nan, math.nan, math.nan)\n\n        if self.method == \"standard\":\n            return _standard_interval(bootstrap_stats, self.alpha)\n        elif self.method == \"percentile\":\n            return _percentile_interval(bootstrap_stats, self.alpha)\n        elif self.method == \"basic\":\n            original_stat = stat_func(df)\n            return _basic_interval(original_stat, bootstrap_stats, self.alpha)\n        elif self.method == \"BCa\":\n            original_stat = stat_func(df)\n            jacknife_stats = _jacknife(df, stat_func)\n\n            return _bca_interval(\n                original_stat, bootstrap_stats, jacknife_stats, self.alpha\n            )\n        else:\n            # We shouldn't hit this since we check method in __init__, but it makes the\n            # type-checker happy\n            raise ValueError(\"Invalid method\")\n\n    def confusion_matrix(\n        self,\n        y_true: ArrayLike,\n        y_pred: ArrayLike,\n    ) -&gt; BootstrappedConfusionMatrix:\n        \"\"\"Bootstrap confusion matrix. See [rapidstats.confusion_matrix][] for\n        more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_pred : ArrayLike\n            Predicted target\n\n        Returns\n        -------\n        BootstrappedConfusionMatrix\n            A dataclass of confusion matrix metrics as (lower, mean, upper). See\n            [rapidstats._bootstrap.BootstrappedConfusionMatrix][] for more details.\n        \"\"\"\n        df = _y_true_y_pred_to_df(y_true, y_pred)\n\n        return BootstrappedConfusionMatrix(\n            *_bootstrap_confusion_matrix(df, **self._params)\n        )\n\n    def confusion_matrix_at_thresholds(\n        self,\n        y_true: ArrayLike,\n        y_score: ArrayLike,\n        thresholds: Optional[list[float]] = None,\n        metrics: Iterable[ConfusionMatrixMetric] = DefaultConfusionMatrixMetrics,\n        strategy: LoopStrategy = \"auto\",\n    ) -&gt; pl.DataFrame:\n        \"\"\"Bootstrap confusion matrix at thresholds. See\n        [rapidstats.confusion_matrix_at_thresholds][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n        thresholds : Optional[list[float]], optional\n            The thresholds to compute `y_pred` at, i.e. y_score &gt;= t. If None,\n            uses every score present in `y_score`, by default None\n        metrics : Iterable[ConfusionMatrixMetric], optional\n            The metrics to compute, by default DefaultConfusionMatrixMetrics\n        strategy : LoopStrategy, optional\n            Computation method, by default \"auto\"\n\n        Returns\n        -------\n        pl.DataFrame\n            A DataFrame of `threshold`, `metric`, `lower`, `mean`, and `upper`\n\n        Raises\n        ------\n        NotImplementedError\n            When `strategy` is `cum_sum` and `method` is `BCa`\n        \"\"\"\n        df = _y_true_y_score_to_df(y_true, y_score).rename({\"y_score\": \"threshold\"})\n        final_cols = [\"threshold\", \"metric\", \"lower\", \"mean\", \"upper\"]\n\n        strategy = _set_loop_strategy(thresholds, strategy)\n\n        if strategy == \"loop\":\n            cms: list[pl.DataFrame] = []\n            for t in tqdm(set(thresholds or y_score)):\n                cm = (\n                    self.confusion_matrix(df[\"y_true\"], df[\"threshold\"].ge(t))\n                    .to_polars()\n                    .with_columns(pl.lit(t).alias(\"threshold\"))\n                )\n                cms.append(cm)\n\n            return pl.concat(cms, how=\"vertical\").with_columns(\n                pl.col(\"lower\", \"mean\", \"upper\").fill_nan(None)\n            )\n        elif strategy == \"cum_sum\":\n            if thresholds is None:\n                thresholds = df[\"threshold\"].unique()\n\n            def _cm_inner(pf: PolarsFrame) -&gt; pl.LazyFrame:\n                return (\n                    pf.lazy()\n                    .pipe(_base_confusion_matrix_at_thresholds)\n                    .pipe(_full_confusion_matrix_from_base)\n                    .unique(\"threshold\")\n                    .pipe(_map_to_thresholds, thresholds)\n                    .drop(\"_threshold_actual\")\n                )\n\n            def _cm(i: int) -&gt; pl.LazyFrame:\n                sample_df = df.sample(fraction=1, with_replacement=True, seed=i)\n\n                return _cm_inner(sample_df)\n\n            cms: list[pl.LazyFrame] = _run_concurrent(\n                _cm,\n                (\n                    (self.seed + i for i in range(self.iterations))\n                    if self.seed is not None\n                    else (None for _ in range(self.iterations))\n                ),\n            )\n\n            lf = (\n                pl.concat(cms, how=\"vertical\")\n                .select(\"threshold\", *metrics)\n                .unpivot(index=\"threshold\")\n                .rename({\"variable\": \"metric\"})\n                .group_by(\"threshold\", \"metric\")\n            )\n\n            if self.method == \"standard\":\n                return (\n                    _standard_interval_polars(lf, self.alpha)\n                    .select(final_cols)\n                    .collect()\n                )\n            elif self.method == \"percentile\":\n                return (\n                    _percentile_interval_polars(lf, self.alpha)\n                    .select(final_cols)\n                    .collect()\n                )\n            elif self.method == \"basic\":\n                original = (\n                    _cm_inner(df)\n                    .select(\"threshold\", *metrics)\n                    .pipe(_map_to_thresholds, thresholds)\n                    .unpivot(index=\"threshold\")\n                    .rename({\"variable\": \"metric\", \"value\": \"original\"})\n                )\n\n                return (\n                    _percentile_interval_polars(lf, self.alpha)\n                    .join(\n                        original,\n                        on=[\"threshold\", \"metric\"],\n                        how=\"left\",\n                        validate=\"1:1\",\n                    )\n                    .pipe(_basic_interval_polars)\n                    .select(final_cols)\n                    .collect()\n                )\n            elif self.method == \"BCa\":\n                raise NotImplementedError(\n                    \"BCa is not yet implemented for strategy `cum_sum`.\"\n                )\n\n    def roc_auc(\n        self,\n        y_true: ArrayLike,\n        y_score: ArrayLike,\n    ) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap ROC-AUC. See [rapidstats.roc_auc][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, upper)\n        \"\"\"\n        df = _y_true_y_score_to_df(y_true, y_score).with_columns(\n            pl.col(\"y_true\").cast(pl.Float64)\n        )\n\n        return _bootstrap_roc_auc(df, **self._params)\n\n    def max_ks(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap Max-KS. See [rapidstats.max_ks][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, upper)\n        \"\"\"\n        df = _y_true_y_score_to_df(y_true, y_score)\n\n        return _bootstrap_max_ks(df, **self._params)\n\n    def brier_loss(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap Brier loss. See [rapidstats.brier_loss][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, upper)\n        \"\"\"\n        df = _y_true_y_score_to_df(y_true, y_score)\n\n        return _bootstrap_brier_loss(df, **self._params)\n\n    def mean(self, y: ArrayLike) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap mean.\n\n        Parameters\n        ----------\n        y : ArrayLike\n            A 1D-array\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, upper)\n        \"\"\"\n        df = pl.DataFrame({\"y\": y})\n\n        return _bootstrap_mean(df, **self._params)\n\n    def adverse_impact_ratio(\n        self, y_pred: ArrayLike, protected: ArrayLike, control: ArrayLike\n    ) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap AIR. See [rapidstats.adverse_impact_ratio][] for more details.\n\n        Parameters\n        ----------\n        y_pred : ArrayLike\n            Predicted target\n        protected : ArrayLike\n            An array of booleans identifying the protected class\n        control : ArrayLike\n            An array of booleans identifying the control class\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, upper)\n        \"\"\"\n        df = pl.DataFrame(\n            {\"y_pred\": y_pred, \"protected\": protected, \"control\": control}\n        ).cast(pl.Boolean)\n\n        return _bootstrap_adverse_impact_ratio(df, **self._params)\n\n    def adverse_impact_ratio_at_thresholds(\n        self,\n        y_score: ArrayLike,\n        protected: ArrayLike,\n        control: ArrayLike,\n        thresholds: Optional[list[float]] = None,\n        strategy: LoopStrategy = \"auto\",\n    ) -&gt; pl.DataFrame:\n        \"\"\"Bootstrap AIR at thresholds. See\n        [rapidstats.adverse_impact_ratio_at_thresholds][] for more details.\n\n        Parameters\n        ----------\n        y_score : ArrayLike\n            Predicted scores\n        protected : ArrayLike\n            An array of booleans identifying the protected class\n        control : ArrayLike\n            An array of booleans identifying the control class\n        thresholds : Optional[list[float]], optional\n            The thresholds to compute `is_predicted_negative` at, i.e. y_score &lt; t.\n            If None, uses every score present in `y_score`, by default None\n        strategy : LoopStrategy, optional\n            Computation method, by default \"auto\"\n\n        Returns\n        -------\n        pl.DataFrame\n            A DataFrame of `threshold`, `lower`, `mean`, and `upper`\n\n        Raises\n        ------\n        NotImplementedError\n            When `strategy` is `cum_sum` and `method` is `BCa`\n        \"\"\"\n        df = pl.DataFrame(\n            {\"y_score\": y_score, \"protected\": protected, \"control\": control}\n        ).with_columns(pl.col(\"protected\", \"control\").cast(pl.Boolean))\n\n        strategy = _set_loop_strategy(thresholds, strategy)\n\n        if strategy == \"loop\":\n            airs: list[dict[str, float]] = []\n            for t in tqdm(set(thresholds or y_score)):\n                l, m, u = self.adverse_impact_ratio(\n                    df[\"y_score\"].lt(t), df[\"protected\"], df[\"control\"]\n                )\n                airs.append({\"threshold\": t, \"lower\": l, \"mean\": m, \"upper\": u})\n\n            return pl.DataFrame(airs).fill_nan(None).pipe(_fill_infinite, None)\n\n        elif strategy == \"cum_sum\":\n            if thresholds is None:\n                thresholds = df[\"y_score\"]\n\n            def _air(i: int) -&gt; pl.LazyFrame:\n                sample_df = df.sample(fraction=1, with_replacement=True, seed=i)\n\n                return _air_at_thresholds_core(sample_df, thresholds)\n\n            airs: list[pl.LazyFrame] = _run_concurrent(\n                _air,\n                (\n                    (self.seed + i for i in range(self.iterations))\n                    if self.seed is not None\n                    else (None for _ in range(self.iterations))\n                ),\n            )\n            lf = (\n                pl.concat(airs, how=\"vertical\")\n                .rename({\"air\": \"value\"})\n                .with_columns(\n                    _expr_fill_infinite(pl.col(\"value\").fill_nan(None)).alias(\"value\")\n                )\n                .group_by(\"threshold\")\n            )\n\n            final_cols = [\"threshold\", \"lower\", \"mean\", \"upper\"]\n\n            if self.method == \"standard\":\n                return (\n                    _standard_interval_polars(lf, self.alpha)\n                    .select(final_cols)\n                    .collect()\n                )\n            elif self.method == \"percentile\":\n                return (\n                    _percentile_interval_polars(lf, self.alpha)\n                    .select(final_cols)\n                    .collect()\n                )\n            elif self.method == \"basic\":\n                original = (\n                    _air_at_thresholds_core(df)\n                    .rename({\"air\": \"original\"})\n                    .unique(\"threshold\")\n                )\n\n                return (\n                    _percentile_interval_polars(lf, self.alpha)\n                    .join(original, on=\"threshold\", how=\"left\", validate=\"1:1\")\n                    .pipe(_basic_interval_polars)\n                    .select(final_cols)\n                    .collect()\n                )\n            elif self.method == \"BCa\":\n                raise NotImplementedError(\n                    \"BCa not yet implemented for strategy `cum_sum`.\"\n                )\n\n    def mean_squared_error(\n        self, y_true: ArrayLike, y_score: ArrayLike\n    ) -&gt; ConfidenceInterval:\n        r\"\"\"Bootstrap MSE. See [rapidstats.mean_squared_error][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, upper)\n        \"\"\"\n        return _bootstrap_mean_squared_error(\n            _regression_to_df(y_true, y_score), **self._params\n        )\n\n    def root_mean_squared_error(\n        self, y_true: ArrayLike, y_score: ArrayLike\n    ) -&gt; ConfidenceInterval:\n        r\"\"\"Bootstrap RMSE. See [rapidstats.root_mean_squared_error][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, mean, upper)\n        \"\"\"\n        return _bootstrap_root_mean_squared_error(\n            _regression_to_df(y_true, y_score), **self._params\n        )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.adverse_impact_ratio","title":"<code>adverse_impact_ratio(y_pred, protected, control)</code>","text":"<p>Bootstrap AIR. See rapidstats.adverse_impact_ratio for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>ArrayLike</code> <p>Predicted target</p> required <code>protected</code> <code>ArrayLike</code> <p>An array of booleans identifying the protected class</p> required <code>control</code> <code>ArrayLike</code> <p>An array of booleans identifying the control class</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, upper)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def adverse_impact_ratio(\n    self, y_pred: ArrayLike, protected: ArrayLike, control: ArrayLike\n) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap AIR. See [rapidstats.adverse_impact_ratio][] for more details.\n\n    Parameters\n    ----------\n    y_pred : ArrayLike\n        Predicted target\n    protected : ArrayLike\n        An array of booleans identifying the protected class\n    control : ArrayLike\n        An array of booleans identifying the control class\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, upper)\n    \"\"\"\n    df = pl.DataFrame(\n        {\"y_pred\": y_pred, \"protected\": protected, \"control\": control}\n    ).cast(pl.Boolean)\n\n    return _bootstrap_adverse_impact_ratio(df, **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.adverse_impact_ratio_at_thresholds","title":"<code>adverse_impact_ratio_at_thresholds(y_score, protected, control, thresholds=None, strategy='auto')</code>","text":"<p>Bootstrap AIR at thresholds. See rapidstats.adverse_impact_ratio_at_thresholds for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <code>protected</code> <code>ArrayLike</code> <p>An array of booleans identifying the protected class</p> required <code>control</code> <code>ArrayLike</code> <p>An array of booleans identifying the control class</p> required <code>thresholds</code> <code>Optional[list[float]]</code> <p>The thresholds to compute <code>is_predicted_negative</code> at, i.e. y_score &lt; t. If None, uses every score present in <code>y_score</code>, by default None</p> <code>None</code> <code>strategy</code> <code>LoopStrategy</code> <p>Computation method, by default \"auto\"</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame of <code>threshold</code>, <code>lower</code>, <code>mean</code>, and <code>upper</code></p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>When <code>strategy</code> is <code>cum_sum</code> and <code>method</code> is <code>BCa</code></p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def adverse_impact_ratio_at_thresholds(\n    self,\n    y_score: ArrayLike,\n    protected: ArrayLike,\n    control: ArrayLike,\n    thresholds: Optional[list[float]] = None,\n    strategy: LoopStrategy = \"auto\",\n) -&gt; pl.DataFrame:\n    \"\"\"Bootstrap AIR at thresholds. See\n    [rapidstats.adverse_impact_ratio_at_thresholds][] for more details.\n\n    Parameters\n    ----------\n    y_score : ArrayLike\n        Predicted scores\n    protected : ArrayLike\n        An array of booleans identifying the protected class\n    control : ArrayLike\n        An array of booleans identifying the control class\n    thresholds : Optional[list[float]], optional\n        The thresholds to compute `is_predicted_negative` at, i.e. y_score &lt; t.\n        If None, uses every score present in `y_score`, by default None\n    strategy : LoopStrategy, optional\n        Computation method, by default \"auto\"\n\n    Returns\n    -------\n    pl.DataFrame\n        A DataFrame of `threshold`, `lower`, `mean`, and `upper`\n\n    Raises\n    ------\n    NotImplementedError\n        When `strategy` is `cum_sum` and `method` is `BCa`\n    \"\"\"\n    df = pl.DataFrame(\n        {\"y_score\": y_score, \"protected\": protected, \"control\": control}\n    ).with_columns(pl.col(\"protected\", \"control\").cast(pl.Boolean))\n\n    strategy = _set_loop_strategy(thresholds, strategy)\n\n    if strategy == \"loop\":\n        airs: list[dict[str, float]] = []\n        for t in tqdm(set(thresholds or y_score)):\n            l, m, u = self.adverse_impact_ratio(\n                df[\"y_score\"].lt(t), df[\"protected\"], df[\"control\"]\n            )\n            airs.append({\"threshold\": t, \"lower\": l, \"mean\": m, \"upper\": u})\n\n        return pl.DataFrame(airs).fill_nan(None).pipe(_fill_infinite, None)\n\n    elif strategy == \"cum_sum\":\n        if thresholds is None:\n            thresholds = df[\"y_score\"]\n\n        def _air(i: int) -&gt; pl.LazyFrame:\n            sample_df = df.sample(fraction=1, with_replacement=True, seed=i)\n\n            return _air_at_thresholds_core(sample_df, thresholds)\n\n        airs: list[pl.LazyFrame] = _run_concurrent(\n            _air,\n            (\n                (self.seed + i for i in range(self.iterations))\n                if self.seed is not None\n                else (None for _ in range(self.iterations))\n            ),\n        )\n        lf = (\n            pl.concat(airs, how=\"vertical\")\n            .rename({\"air\": \"value\"})\n            .with_columns(\n                _expr_fill_infinite(pl.col(\"value\").fill_nan(None)).alias(\"value\")\n            )\n            .group_by(\"threshold\")\n        )\n\n        final_cols = [\"threshold\", \"lower\", \"mean\", \"upper\"]\n\n        if self.method == \"standard\":\n            return (\n                _standard_interval_polars(lf, self.alpha)\n                .select(final_cols)\n                .collect()\n            )\n        elif self.method == \"percentile\":\n            return (\n                _percentile_interval_polars(lf, self.alpha)\n                .select(final_cols)\n                .collect()\n            )\n        elif self.method == \"basic\":\n            original = (\n                _air_at_thresholds_core(df)\n                .rename({\"air\": \"original\"})\n                .unique(\"threshold\")\n            )\n\n            return (\n                _percentile_interval_polars(lf, self.alpha)\n                .join(original, on=\"threshold\", how=\"left\", validate=\"1:1\")\n                .pipe(_basic_interval_polars)\n                .select(final_cols)\n                .collect()\n            )\n        elif self.method == \"BCa\":\n            raise NotImplementedError(\n                \"BCa not yet implemented for strategy `cum_sum`.\"\n            )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.brier_loss","title":"<code>brier_loss(y_true, y_score)</code>","text":"<p>Bootstrap Brier loss. See rapidstats.brier_loss for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, upper)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def brier_loss(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap Brier loss. See [rapidstats.brier_loss][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, upper)\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score)\n\n    return _bootstrap_brier_loss(df, **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.confusion_matrix","title":"<code>confusion_matrix(y_true, y_pred)</code>","text":"<p>Bootstrap confusion matrix. See rapidstats.confusion_matrix for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_pred</code> <code>ArrayLike</code> <p>Predicted target</p> required <p>Returns:</p> Type Description <code>BootstrappedConfusionMatrix</code> <p>A dataclass of confusion matrix metrics as (lower, mean, upper). See rapidstats._bootstrap.BootstrappedConfusionMatrix for more details.</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def confusion_matrix(\n    self,\n    y_true: ArrayLike,\n    y_pred: ArrayLike,\n) -&gt; BootstrappedConfusionMatrix:\n    \"\"\"Bootstrap confusion matrix. See [rapidstats.confusion_matrix][] for\n    more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_pred : ArrayLike\n        Predicted target\n\n    Returns\n    -------\n    BootstrappedConfusionMatrix\n        A dataclass of confusion matrix metrics as (lower, mean, upper). See\n        [rapidstats._bootstrap.BootstrappedConfusionMatrix][] for more details.\n    \"\"\"\n    df = _y_true_y_pred_to_df(y_true, y_pred)\n\n    return BootstrappedConfusionMatrix(\n        *_bootstrap_confusion_matrix(df, **self._params)\n    )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.confusion_matrix_at_thresholds","title":"<code>confusion_matrix_at_thresholds(y_true, y_score, thresholds=None, metrics=DefaultConfusionMatrixMetrics, strategy='auto')</code>","text":"<p>Bootstrap confusion matrix at thresholds. See rapidstats.confusion_matrix_at_thresholds for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <code>thresholds</code> <code>Optional[list[float]]</code> <p>The thresholds to compute <code>y_pred</code> at, i.e. y_score &gt;= t. If None, uses every score present in <code>y_score</code>, by default None</p> <code>None</code> <code>metrics</code> <code>Iterable[ConfusionMatrixMetric]</code> <p>The metrics to compute, by default DefaultConfusionMatrixMetrics</p> <code>DefaultConfusionMatrixMetrics</code> <code>strategy</code> <code>LoopStrategy</code> <p>Computation method, by default \"auto\"</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame of <code>threshold</code>, <code>metric</code>, <code>lower</code>, <code>mean</code>, and <code>upper</code></p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>When <code>strategy</code> is <code>cum_sum</code> and <code>method</code> is <code>BCa</code></p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def confusion_matrix_at_thresholds(\n    self,\n    y_true: ArrayLike,\n    y_score: ArrayLike,\n    thresholds: Optional[list[float]] = None,\n    metrics: Iterable[ConfusionMatrixMetric] = DefaultConfusionMatrixMetrics,\n    strategy: LoopStrategy = \"auto\",\n) -&gt; pl.DataFrame:\n    \"\"\"Bootstrap confusion matrix at thresholds. See\n    [rapidstats.confusion_matrix_at_thresholds][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n    thresholds : Optional[list[float]], optional\n        The thresholds to compute `y_pred` at, i.e. y_score &gt;= t. If None,\n        uses every score present in `y_score`, by default None\n    metrics : Iterable[ConfusionMatrixMetric], optional\n        The metrics to compute, by default DefaultConfusionMatrixMetrics\n    strategy : LoopStrategy, optional\n        Computation method, by default \"auto\"\n\n    Returns\n    -------\n    pl.DataFrame\n        A DataFrame of `threshold`, `metric`, `lower`, `mean`, and `upper`\n\n    Raises\n    ------\n    NotImplementedError\n        When `strategy` is `cum_sum` and `method` is `BCa`\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score).rename({\"y_score\": \"threshold\"})\n    final_cols = [\"threshold\", \"metric\", \"lower\", \"mean\", \"upper\"]\n\n    strategy = _set_loop_strategy(thresholds, strategy)\n\n    if strategy == \"loop\":\n        cms: list[pl.DataFrame] = []\n        for t in tqdm(set(thresholds or y_score)):\n            cm = (\n                self.confusion_matrix(df[\"y_true\"], df[\"threshold\"].ge(t))\n                .to_polars()\n                .with_columns(pl.lit(t).alias(\"threshold\"))\n            )\n            cms.append(cm)\n\n        return pl.concat(cms, how=\"vertical\").with_columns(\n            pl.col(\"lower\", \"mean\", \"upper\").fill_nan(None)\n        )\n    elif strategy == \"cum_sum\":\n        if thresholds is None:\n            thresholds = df[\"threshold\"].unique()\n\n        def _cm_inner(pf: PolarsFrame) -&gt; pl.LazyFrame:\n            return (\n                pf.lazy()\n                .pipe(_base_confusion_matrix_at_thresholds)\n                .pipe(_full_confusion_matrix_from_base)\n                .unique(\"threshold\")\n                .pipe(_map_to_thresholds, thresholds)\n                .drop(\"_threshold_actual\")\n            )\n\n        def _cm(i: int) -&gt; pl.LazyFrame:\n            sample_df = df.sample(fraction=1, with_replacement=True, seed=i)\n\n            return _cm_inner(sample_df)\n\n        cms: list[pl.LazyFrame] = _run_concurrent(\n            _cm,\n            (\n                (self.seed + i for i in range(self.iterations))\n                if self.seed is not None\n                else (None for _ in range(self.iterations))\n            ),\n        )\n\n        lf = (\n            pl.concat(cms, how=\"vertical\")\n            .select(\"threshold\", *metrics)\n            .unpivot(index=\"threshold\")\n            .rename({\"variable\": \"metric\"})\n            .group_by(\"threshold\", \"metric\")\n        )\n\n        if self.method == \"standard\":\n            return (\n                _standard_interval_polars(lf, self.alpha)\n                .select(final_cols)\n                .collect()\n            )\n        elif self.method == \"percentile\":\n            return (\n                _percentile_interval_polars(lf, self.alpha)\n                .select(final_cols)\n                .collect()\n            )\n        elif self.method == \"basic\":\n            original = (\n                _cm_inner(df)\n                .select(\"threshold\", *metrics)\n                .pipe(_map_to_thresholds, thresholds)\n                .unpivot(index=\"threshold\")\n                .rename({\"variable\": \"metric\", \"value\": \"original\"})\n            )\n\n            return (\n                _percentile_interval_polars(lf, self.alpha)\n                .join(\n                    original,\n                    on=[\"threshold\", \"metric\"],\n                    how=\"left\",\n                    validate=\"1:1\",\n                )\n                .pipe(_basic_interval_polars)\n                .select(final_cols)\n                .collect()\n            )\n        elif self.method == \"BCa\":\n            raise NotImplementedError(\n                \"BCa is not yet implemented for strategy `cum_sum`.\"\n            )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.max_ks","title":"<code>max_ks(y_true, y_score)</code>","text":"<p>Bootstrap Max-KS. See rapidstats.max_ks for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, upper)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def max_ks(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap Max-KS. See [rapidstats.max_ks][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, upper)\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score)\n\n    return _bootstrap_max_ks(df, **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.mean","title":"<code>mean(y)</code>","text":"<p>Bootstrap mean.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ArrayLike</code> <p>A 1D-array</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, upper)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def mean(self, y: ArrayLike) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap mean.\n\n    Parameters\n    ----------\n    y : ArrayLike\n        A 1D-array\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, upper)\n    \"\"\"\n    df = pl.DataFrame({\"y\": y})\n\n    return _bootstrap_mean(df, **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.mean_squared_error","title":"<code>mean_squared_error(y_true, y_score)</code>","text":"<p>Bootstrap MSE. See rapidstats.mean_squared_error for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, upper)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def mean_squared_error(\n    self, y_true: ArrayLike, y_score: ArrayLike\n) -&gt; ConfidenceInterval:\n    r\"\"\"Bootstrap MSE. See [rapidstats.mean_squared_error][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, upper)\n    \"\"\"\n    return _bootstrap_mean_squared_error(\n        _regression_to_df(y_true, y_score), **self._params\n    )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.roc_auc","title":"<code>roc_auc(y_true, y_score)</code>","text":"<p>Bootstrap ROC-AUC. See rapidstats.roc_auc for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, upper)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def roc_auc(\n    self,\n    y_true: ArrayLike,\n    y_score: ArrayLike,\n) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap ROC-AUC. See [rapidstats.roc_auc][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, upper)\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score).with_columns(\n        pl.col(\"y_true\").cast(pl.Float64)\n    )\n\n    return _bootstrap_roc_auc(df, **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.root_mean_squared_error","title":"<code>root_mean_squared_error(y_true, y_score)</code>","text":"<p>Bootstrap RMSE. See rapidstats.root_mean_squared_error for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, upper)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def root_mean_squared_error(\n    self, y_true: ArrayLike, y_score: ArrayLike\n) -&gt; ConfidenceInterval:\n    r\"\"\"Bootstrap RMSE. See [rapidstats.root_mean_squared_error][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, upper)\n    \"\"\"\n    return _bootstrap_root_mean_squared_error(\n        _regression_to_df(y_true, y_score), **self._params\n    )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.run","title":"<code>run(df, stat_func, **kwargs)</code>","text":"<p>Run bootstrap for an arbitrary function that accepts a Polars DataFrame and returns a scalar real number.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The data to pass to <code>stat_func</code></p> required <code>stat_func</code> <code>StatFunc</code> <p>A callable that takes a Polars DataFrame as its first argument and returns a scalar real number.</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, mean, higher)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def run(\n    self, df: pl.DataFrame, stat_func: StatFunc, **kwargs\n) -&gt; ConfidenceInterval:\n    \"\"\"Run bootstrap for an arbitrary function that accepts a Polars DataFrame and\n    returns a scalar real number.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        The data to pass to `stat_func`\n    stat_func : StatFunc\n        A callable that takes a Polars DataFrame as its first argument and returns\n        a scalar real number.\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, mean, higher)\n    \"\"\"\n    default = {\"executor\": \"threads\", \"preserve_order\": False}\n    for k, v in default.items():\n        if k not in kwargs:\n            kwargs[k] = v\n\n    func = functools.partial(_bs_func, df=df, stat_func=stat_func)\n\n    if self.seed is None:\n        iterable = (None for _ in range(self.iterations))\n    else:\n        iterable = (self.seed + i for i in range(self.iterations))\n\n    bootstrap_stats = [\n        x for x in _run_concurrent(func, iterable, **kwargs) if not math.isnan(x)\n    ]\n\n    if len(bootstrap_stats) == 0:\n        return (math.nan, math.nan, math.nan)\n\n    if self.method == \"standard\":\n        return _standard_interval(bootstrap_stats, self.alpha)\n    elif self.method == \"percentile\":\n        return _percentile_interval(bootstrap_stats, self.alpha)\n    elif self.method == \"basic\":\n        original_stat = stat_func(df)\n        return _basic_interval(original_stat, bootstrap_stats, self.alpha)\n    elif self.method == \"BCa\":\n        original_stat = stat_func(df)\n        jacknife_stats = _jacknife(df, stat_func)\n\n        return _bca_interval(\n            original_stat, bootstrap_stats, jacknife_stats, self.alpha\n        )\n    else:\n        # We shouldn't hit this since we check method in __init__, but it makes the\n        # type-checker happy\n        raise ValueError(\"Invalid method\")\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.BootstrappedConfusionMatrix","title":"<code>BootstrappedConfusionMatrix</code>  <code>dataclass</code>","text":"<p>Result object returned by <code>rapidstats.Bootstrap().confusion_matrix</code>.</p> <p>See rapidstats._metrics.ConfusionMatrix for a detailed breakdown of the attributes stored in this class. However, instead of storing the statistic, it stores the bootstrapped confidence interval as (lower, mean, upper).</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>@dataclasses.dataclass\nclass BootstrappedConfusionMatrix:\n    \"\"\"Result object returned by `rapidstats.Bootstrap().confusion_matrix`.\n\n    See [rapidstats._metrics.ConfusionMatrix][] for a detailed breakdown of the attributes stored in\n    this class. However, instead of storing the statistic, it stores the bootstrapped\n    confidence interval as (lower, mean, upper).\n    \"\"\"\n\n    tn: ConfidenceInterval\n    fp: ConfidenceInterval\n    fn: ConfidenceInterval\n    tp: ConfidenceInterval\n    tpr: ConfidenceInterval\n    fpr: ConfidenceInterval\n    fnr: ConfidenceInterval\n    tnr: ConfidenceInterval\n    prevalence: ConfidenceInterval\n    prevalence_threshold: ConfidenceInterval\n    informedness: ConfidenceInterval\n    precision: ConfidenceInterval\n    false_omission_rate: ConfidenceInterval\n    plr: ConfidenceInterval\n    nlr: ConfidenceInterval\n    acc: ConfidenceInterval\n    balanced_accuracy: ConfidenceInterval\n    f1: ConfidenceInterval\n    folkes_mallows_index: ConfidenceInterval\n    mcc: ConfidenceInterval\n    threat_score: ConfidenceInterval\n    markedness: ConfidenceInterval\n    fdr: ConfidenceInterval\n    npv: ConfidenceInterval\n    dor: ConfidenceInterval\n    ppr: ConfidenceInterval\n    pnr: ConfidenceInterval\n\n    def to_polars(self) -&gt; pl.DataFrame:\n        \"\"\"Transform the dataclass to a long Polars DataFrame with columns\n        `metric`, `lower`, `mean`, and `upper`.\n\n        Returns\n        -------\n        pl.DataFrame\n            A DataFrame with columns `metric`, `lower`, `mean`, and `upper`\n        \"\"\"\n        dct = self.__dict__\n        lower = []\n        mean = []\n        upper = []\n        for l, m, u in dct.values():  # noqa: E741\n            lower.append(l)\n            mean.append(m)\n            upper.append(u)\n\n        return pl.DataFrame(\n            {\n                \"metric\": dct.keys(),\n                \"lower\": lower,\n                \"mean\": mean,\n                \"upper\": upper,\n            }\n        )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.BootstrappedConfusionMatrix.to_polars","title":"<code>to_polars()</code>","text":"<p>Transform the dataclass to a long Polars DataFrame with columns <code>metric</code>, <code>lower</code>, <code>mean</code>, and <code>upper</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with columns <code>metric</code>, <code>lower</code>, <code>mean</code>, and <code>upper</code></p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def to_polars(self) -&gt; pl.DataFrame:\n    \"\"\"Transform the dataclass to a long Polars DataFrame with columns\n    `metric`, `lower`, `mean`, and `upper`.\n\n    Returns\n    -------\n    pl.DataFrame\n        A DataFrame with columns `metric`, `lower`, `mean`, and `upper`\n    \"\"\"\n    dct = self.__dict__\n    lower = []\n    mean = []\n    upper = []\n    for l, m, u in dct.values():  # noqa: E741\n        lower.append(l)\n        mean.append(m)\n        upper.append(u)\n\n    return pl.DataFrame(\n        {\n            \"metric\": dct.keys(),\n            \"lower\": lower,\n            \"mean\": mean,\n            \"upper\": upper,\n        }\n    )\n</code></pre>"},{"location":"correlation.html","title":"Correlation","text":""},{"location":"correlation.html#rapidstats._corr.correlation_matrix","title":"<code>correlation_matrix(data, l1=None, l2=None, method='pearson')</code>","text":"<p>Warning</p> <p>If you know that your data has no nulls, you should use <code>np.corrcoef</code> instead. While this function will return the correct result and is reasonably fast, computing the null-aware correlation matrix will always be slower than assuming that there are no nulls.</p> <p>Compute the null-aware correlation matrix between two lists of columns. If both lists are None, then the correlation matrix is over all columns in the input DataFrame. If <code>l1</code> is not None, and is a list of 2-tuples, <code>l1</code> is interpreted as the combinations of columns to compute the correlation for.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[LazyFrame, DataFrame, ConvertibleToPolars]</code> <p>The input DataFrame. It must be either a Polars Frame or something convertible to a Polars Frame.</p> required <code>l1</code> <code>Union[list[str], list[tuple[str, str]]]</code> <p>A list of columns to appear as the columns of the correlation matrix, by default None</p> <code>None</code> <code>l2</code> <code>list[str]</code> <p>A list of columns to appear as the rows of the correlation matrix, by default None</p> <code>None</code> <code>method</code> <code>CorrelationMethod</code> <p>How to calculate the correlation, by default \"pearson\"</p> <code>'pearson'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A correlation matrix with <code>l1</code> as the columns and <code>l2</code> as the rows</p> Source code in <code>python/rapidstats/_corr.py</code> <pre><code>def correlation_matrix(\n    data: Union[pl.LazyFrame, pl.DataFrame, ConvertibleToPolars],\n    l1: Optional[Union[list[str], list[tuple[str, str]]]] = None,\n    l2: Optional[list[str]] = None,\n    method: CorrelationMethod = \"pearson\",\n) -&gt; pl.DataFrame:\n    \"\"\"\n    !!! warning\n\n        If you know that your data has no nulls, you should use `np.corrcoef` instead.\n        While this function will return the correct result and is reasonably fast,\n        computing the null-aware correlation matrix will always be slower than assuming\n        that there are no nulls.\n\n    Compute the null-aware correlation matrix between two lists of columns. If both\n    lists are None, then the correlation matrix is over all columns in the input\n    DataFrame. If `l1` is not None, and is a list of 2-tuples, `l1` is interpreted\n    as the combinations of columns to compute the correlation for.\n\n    Parameters\n    ----------\n    data : Union[pl.LazyFrame, pl.DataFrame, ConvertibleToPolars]\n        The input DataFrame. It must be either a Polars Frame or something convertible\n        to a Polars Frame.\n    l1 : Union[list[str], list[tuple[str, str]]], optional\n        A list of columns to appear as the columns of the correlation matrix,\n        by default None\n    l2 : list[str], optional\n        A list of columns to appear as the rows of the correlation matrix,\n        by default None\n    method : CorrelationMethod, optional\n        How to calculate the correlation, by default \"pearson\"\n\n    Returns\n    -------\n    pl.DataFrame\n        A correlation matrix with `l1` as the columns and `l2` as the rows\n    \"\"\"\n    # pl.corr works with nulls but NOT NaNs\n    pf = _to_polars(data).select(cs.numeric() | cs.boolean()).fill_nan(None)\n\n    if l1 is None and l2 is None:\n        original = pf.columns\n        new_columns = [f\"{i}\" for i, _ in enumerate(original)]\n        combinations = itertools.combinations(new_columns, r=2)\n        l1 = original[:-1]\n        l2 = original[1:]\n    elif l1 is not None and l2 is None:\n        # In this case the user should pass in the combinations directly as a list of\n        # 2-tuples.\n        original = set()\n        for a, b in l1:\n            original.add(a)\n            original.add(b)\n        original = list(original)\n        mapper = {name: f\"{i}\" for i, name in enumerate(original)}\n        combinations = [(mapper[a], mapper[b]) for a, b in l1]\n        new_columns = list(mapper.values())\n\n        l1 = original\n        l2 = original\n    else:\n        assert l1 is not None\n        assert l2 is not None\n        valid_cols = set(pf.columns)\n        l1 = [c for c in l1 if c in valid_cols]\n        l2 = [c for c in l2 if c in valid_cols]\n\n        new_l1 = [f\"l{i}\" for i, _ in enumerate(l1)]\n        new_l2 = [f\"r{i}\" for i, _ in enumerate(l2)]\n        new_columns = new_l1 + new_l2\n        combinations = _pairs(new_l1, new_l2)\n        original = l1 + l2\n\n    old_to_new_mapper = {old: new for old, new in zip(original, new_columns)}\n    new_to_old_mapper = {new: old for new, old in zip(new_columns, original)}\n\n    corr_mat = (\n        pf.lazy()\n        .select(original)\n        .rename(old_to_new_mapper)\n        .select(_corr_expr(c1, c2, method=method) for c1, c2 in combinations)\n        .unpivot()\n        .with_columns(pl.col(\"variable\").str.split(\"_\"))\n        .with_columns(\n            pl.col(\"variable\").list.get(0).alias(\"c1\"),\n            pl.col(\"variable\").list.get(1).alias(\"c2\"),\n        )\n        .drop(\"variable\")\n        .collect()\n        .pivot(index=\"c2\", on=\"c1\", values=\"value\")\n    )\n\n    new_row_names = corr_mat[\"c2\"]\n    corr_mat = corr_mat.drop(\"c2\")\n\n    # Set the column names\n    valid_old_names = [new_to_old_mapper[c] for c in corr_mat.columns]\n    corr_mat.columns = valid_old_names\n\n    # Set the row names\n    valid_old_row_names = [new_to_old_mapper[c] for c in new_row_names]\n    corr_mat = corr_mat.with_columns(pl.Series(\"\", valid_old_row_names)).select(\n        \"\", *valid_old_names\n    )\n\n    return corr_mat\n</code></pre>"},{"location":"distributions.html","title":"Distributions","text":""},{"location":"distributions.html#rapidstats._distributions.norm","title":"<code>norm</code>","text":"<p>Functions for working with a normal continuous random variable.</p> Source code in <code>python/rapidstats/_distributions.py</code> <pre><code>class norm:\n    \"\"\"Functions for working with a normal continuous random variable.\"\"\"\n\n    @staticmethod\n    def ppf(q: float) -&gt; float:\n        r\"\"\"The percent point function. Also called the quantile, percentile, inverse\n        CDF, or inverse distribution function. Computes the value of a random variable\n        such that its probability is \\( \\leq q \\). If `q` is 0, it returns negative\n        infinity, if `q` is 1, it returns infinity. Any number outside of [0, 1] will\n        result in NaN.\n\n        Parameters\n        ----------\n        q : float\n            Probability value\n\n        Returns\n        -------\n        float\n            Likelihood a random variable is realized in the range at or below `q` for\n            the normal distribution.\n        \"\"\"\n        return _norm_ppf(q)\n\n    @staticmethod\n    def cdf(x: float) -&gt; float:\n        r\"\"\"The cumulative distribution function.\n\n        Parameters\n        ----------\n        x : float\n\n        Returns\n        -------\n        float\n            The probability a random variable will take a value \\( \\leq x \\)\n        \"\"\"\n        return _norm_cdf(x)\n</code></pre>"},{"location":"distributions.html#rapidstats._distributions.norm.cdf","title":"<code>cdf(x)</code>  <code>staticmethod</code>","text":"<p>The cumulative distribution function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> required <p>Returns:</p> Type Description <code>float</code> <p>The probability a random variable will take a value \\( \\leq x \\)</p> Source code in <code>python/rapidstats/_distributions.py</code> <pre><code>@staticmethod\ndef cdf(x: float) -&gt; float:\n    r\"\"\"The cumulative distribution function.\n\n    Parameters\n    ----------\n    x : float\n\n    Returns\n    -------\n    float\n        The probability a random variable will take a value \\( \\leq x \\)\n    \"\"\"\n    return _norm_cdf(x)\n</code></pre>"},{"location":"distributions.html#rapidstats._distributions.norm.ppf","title":"<code>ppf(q)</code>  <code>staticmethod</code>","text":"<p>The percent point function. Also called the quantile, percentile, inverse CDF, or inverse distribution function. Computes the value of a random variable such that its probability is \\( \\leq q \\). If <code>q</code> is 0, it returns negative infinity, if <code>q</code> is 1, it returns infinity. Any number outside of [0, 1] will result in NaN.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>float</code> <p>Probability value</p> required <p>Returns:</p> Type Description <code>float</code> <p>Likelihood a random variable is realized in the range at or below <code>q</code> for the normal distribution.</p> Source code in <code>python/rapidstats/_distributions.py</code> <pre><code>@staticmethod\ndef ppf(q: float) -&gt; float:\n    r\"\"\"The percent point function. Also called the quantile, percentile, inverse\n    CDF, or inverse distribution function. Computes the value of a random variable\n    such that its probability is \\( \\leq q \\). If `q` is 0, it returns negative\n    infinity, if `q` is 1, it returns infinity. Any number outside of [0, 1] will\n    result in NaN.\n\n    Parameters\n    ----------\n    q : float\n        Probability value\n\n    Returns\n    -------\n    float\n        Likelihood a random variable is realized in the range at or below `q` for\n        the normal distribution.\n    \"\"\"\n    return _norm_ppf(q)\n</code></pre>"},{"location":"metrics.html","title":"Metrics","text":""},{"location":"metrics.html#rapidstats._metrics.ConfusionMatrix","title":"<code>ConfusionMatrix</code>  <code>dataclass</code>","text":"<p>Result object returned by <code>rapidstats.confusion_matrix</code></p> <p>Attributes:</p> Name Type Description <code>tn</code> <code>float</code> <p>\u2191Count of True Negatives; y_true == False and y_pred == False</p> <code>fp</code> <code>float</code> <p>\u2193Count of False Positives; y_true == False and y_pred == True</p> <code>fn</code> <code>float</code> <p>\u2193Count of False Negatives; y_true == True and y_pred == False</p> <code>tp</code> <code>float</code> <p>\u2191Count of True Positives; y_true == True, y_pred == True</p> <code>tpr</code> <code>float</code> <p>\u2191True Positive Rate, Recall, Sensitivity; Probability that an actual positive will be predicted positive; \\( \\frac{TP}{TP + FN} \\)</p> <code>fpr</code> <code>float</code> <p>\u2193False Positive Rate, Type I Error; Probability that an actual negative will be predicted positive; \\( \\frac{FP}{FP + TN} \\)</p> <code>fnr</code> <code>float</code> <p>\u2193False Negative Rate, Type II Error; Probability an actual positive will be predicted negative; \\( \\frac{FN}{TP + FN} \\)</p> <code>tnr</code> <code>float</code> <p>\u2191True Negative Rate, Specificity; Probability an actual negative will be predicted negative; \\( \\frac{TN}{FP + TN} \\)</p> <code>prevalence</code> <code>float</code> <p>Prevalence; Proportion of positive classes; \\( \\frac{TP + FN}{TN + FP + FN + TP} \\)</p> <code>prevalence_threshold</code> <code>float</code> <p>Prevalence Threshold; \\( \\frac{\\sqrt{TPR \\times FPR} - FPR}{TPR - FPR} \\)</p> <code>informedness</code> <code>float</code> <p>\u2191Informedness, Youden's J; \\( TPR + TNR - 1 \\)</p> <code>precision</code> <code>float</code> <p>\u2191Precision, Positive Predicted Value (PPV); Probability a predicted positive was actually correct; \\( \\frac{TP}{TP + FP} \\)</p> <code>false_omission_rate</code> <code>float</code> <p>\u2193False Omission Rate (FOR); Proportion of predicted negatives that were wrong \\( \\frac{FN}{FN + TN} \\)</p> <code>plr</code> <code>float</code> <p>\u2191Positive Likelihood Ratio, LR+; \\( \\frac{TPR}{FPR} \\)</p> <code>nlr</code> <code>float</code> <p>Negative Likelihood Ratio, LR-; \\( \\frac{FNR}{TNR} \\)</p> <code>acc</code> <code>float</code> <p>\u2191Accuracy (ACC); Probability of a correct prediction; \\( \\frac{TP + TN}{TN + FP + FN + TP} \\)</p> <code>balanced_accuracy</code> <code>float</code> <p>\u2191Balanced Accuracy (BA); \\( \\frac{TP + TN}{2} \\)</p> <code>f1</code> <code>float</code> <p>\u2191F1; Harmonic mean of Precision and Recall; \\( \\frac{2 \\times PPV \\times TPR}{PPV + TPR} \\)</p> <code>folkes_mallows_index</code> <code>float</code> <p>\u2191Folkes Mallows Index (FM); \\( \\sqrt{PPV \\times TPR} \\)</p> <code>mcc</code> <code>float</code> <p>\u2191Matthew Correlation Coefficient (MCC), Yule Phi Coefficient; \\( \\sqrt{TPR \\times TNR \\times PPV \\times NPV} - \\sqrt{FNR \\times FPR \\times FOR \\times FDR} \\)</p> <code>threat_score</code> <code>float</code> <p>\u2191Threat Score (TS), Critical Success Index (CSI), Jaccard Index; \\( \\frac{TP}{TP + FN + FP} \\)</p> <code>markedness</code> <code>float</code> <p>Markedness (MP), deltaP; \\( PPV + NPV - 1 \\)</p> <code>fdr</code> <code>float</code> <p>\u2193False Discovery Rate, Proportion of predicted positives that are wrong; \\( \\frac{FP}{TP + FP} \\)</p> <code>\u2191npv</code> <code>float</code> <p>Negative Predictive Value; Proportion of predicted negatives that are correct; \\( \\frac{TN}{FN + TN} \\)</p> <code>dor</code> <code>float</code> <p>Diagnostic Odds Ratio; \\( \\frac{LR+}{LR-} \\)</p> <code>ppr</code> <code>float</code> <p>Predicted Positive Ratio; Proportion that are predicted positive; ( \\frac{TP + FP}{TN + FP + FN + TP})</p> <code>pnr</code> <code>float</code> <p>Predicted Negative Ratio; Proportion that are predicted negative; ( \\frac{TN + FN}{TN + FP + FN + TP})</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>@dataclasses.dataclass\nclass ConfusionMatrix:\n    r\"\"\"Result object returned by `rapidstats.confusion_matrix`\n\n    Attributes\n    ----------\n    tn : float\n        \u2191Count of True Negatives; y_true == False and y_pred == False\n    fp : float\n        \u2193Count of False Positives; y_true == False and y_pred == True\n    fn : float\n        \u2193Count of False Negatives; y_true == True and y_pred == False\n    tp : float\n        \u2191Count of True Positives; y_true == True, y_pred == True\n    tpr : float\n        \u2191True Positive Rate, Recall, Sensitivity; Probability that an actual positive\n        will be predicted positive; \\( \\frac{TP}{TP + FN} \\)\n    fpr : float\n        \u2193False Positive Rate, Type I Error; Probability that an actual negative will\n        be predicted positive; \\( \\frac{FP}{FP + TN} \\)\n    fnr : float\n        \u2193False Negative Rate, Type II Error; Probability an actual positive will be\n        predicted negative; \\( \\frac{FN}{TP + FN} \\)\n    tnr : float\n        \u2191True Negative Rate, Specificity; Probability an actual negative will be\n        predicted negative; \\( \\frac{TN}{FP + TN} \\)\n    prevalence : float\n        Prevalence; Proportion of positive classes; \\( \\frac{TP + FN}{TN + FP + FN + TP} \\)\n    prevalence_threshold : float\n        Prevalence Threshold; \\( \\frac{\\sqrt{TPR \\times FPR} - FPR}{TPR - FPR} \\)\n    informedness : float\n        \u2191Informedness, Youden's J; \\( TPR + TNR - 1 \\)\n    precision : float\n        \u2191Precision, Positive Predicted Value (PPV); Probability a predicted positive was\n        actually correct; \\( \\frac{TP}{TP + FP} \\)\n    false_omission_rate : float\n        \u2193False Omission Rate (FOR); Proportion of predicted negatives that were wrong\n        \\( \\frac{FN}{FN + TN} \\)\n    plr : float\n        \u2191Positive Likelihood Ratio, LR+; \\( \\frac{TPR}{FPR} \\)\n    nlr : float\n        Negative Likelihood Ratio, LR-; \\( \\frac{FNR}{TNR} \\)\n    acc : float\n        \u2191Accuracy (ACC); Probability of a correct prediction; \\( \\frac{TP + TN}{TN + FP + FN + TP} \\)\n    balanced_accuracy : float\n        \u2191Balanced Accuracy (BA); \\( \\frac{TP + TN}{2} \\)\n    f1 : float\n        \u2191F1; Harmonic mean of Precision and Recall; \\( \\frac{2 \\times PPV \\times TPR}{PPV + TPR} \\)\n    folkes_mallows_index : float\n        \u2191Folkes Mallows Index (FM); \\( \\sqrt{PPV \\times TPR} \\)\n    mcc : float\n        \u2191Matthew Correlation Coefficient (MCC), Yule Phi Coefficient; \\( \\sqrt{TPR \\times TNR \\times PPV \\times NPV} - \\sqrt{FNR \\times FPR \\times FOR \\times FDR} \\)\n    threat_score : float\n        \u2191Threat Score (TS), Critical Success Index (CSI), Jaccard Index; \\( \\frac{TP}{TP + FN + FP} \\)\n    markedness : float\n        Markedness (MP), deltaP; \\( PPV + NPV - 1 \\)\n    fdr : float\n        \u2193False Discovery Rate, Proportion of predicted positives that are wrong; \\( \\frac{FP}{TP + FP} \\)\n    \u2191npv : float\n        Negative Predictive Value; Proportion of predicted negatives that are correct; \\( \\frac{TN}{FN + TN} \\)\n    dor : float\n        Diagnostic Odds Ratio; \\( \\frac{LR+}{LR-} \\)\n    ppr : float\n        Predicted Positive Ratio; Proportion that are predicted positive; \\( \\frac{TP + FP}{TN + FP + FN + TP})\n    pnr : float\n        Predicted Negative Ratio; Proportion that are predicted negative; \\( \\frac{TN + FN}{TN + FP + FN + TP})\n    \"\"\"\n\n    tn: float\n    fp: float\n    fn: float\n    tp: float\n    tpr: float\n    fpr: float\n    fnr: float\n    tnr: float\n    prevalence: float\n    prevalence_threshold: float\n    informedness: float\n    precision: float\n    false_omission_rate: float\n    plr: float\n    nlr: float\n    acc: float\n    balanced_accuracy: float\n    f1: float\n    folkes_mallows_index: float\n    mcc: float\n    threat_score: float\n    markedness: float\n    fdr: float\n    npv: float\n    dor: float\n    ppr: float\n    pnr: float\n\n    def to_polars(self) -&gt; pl.DataFrame:\n        \"\"\"Convert the dataclass to a long Polars DataFrame with columns `metric` and\n        `value`.\n\n        Returns\n        -------\n        pl.DataFrame\n            DataFrame with columns `metric` and `value`\n        \"\"\"\n        dct = self.__dict__\n\n        return pl.DataFrame({\"metric\": dct.keys(), \"value\": dct.values()})\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.ConfusionMatrix.to_polars","title":"<code>to_polars()</code>","text":"<p>Convert the dataclass to a long Polars DataFrame with columns <code>metric</code> and <code>value</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns <code>metric</code> and <code>value</code></p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def to_polars(self) -&gt; pl.DataFrame:\n    \"\"\"Convert the dataclass to a long Polars DataFrame with columns `metric` and\n    `value`.\n\n    Returns\n    -------\n    pl.DataFrame\n        DataFrame with columns `metric` and `value`\n    \"\"\"\n    dct = self.__dict__\n\n    return pl.DataFrame({\"metric\": dct.keys(), \"value\": dct.values()})\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.adverse_impact_ratio","title":"<code>adverse_impact_ratio(y_pred, protected, control)</code>","text":"<p>Computes the Adverse Impact Ratio (AIR), which is the ratio of negative predictions for the protected class and the control class. The ideal ratio is 1. For example, in an underwriting context, this means that the model is equally as likely to approve protected applicants as it is unprotected applicants, given that the model score is probability of bad.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>ArrayLike</code> <p>Predicted negative</p> required <code>protected</code> <code>ArrayLike</code> <p>An array of booleans identifying the protected class</p> required <code>control</code> <code>ArrayLike</code> <p>An array of booleans identifying the control class</p> required <p>Returns:</p> Type Description <code>float</code> <p>Adverse Impact Ratio (AIR)</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def adverse_impact_ratio(\n    y_pred: ArrayLike,\n    protected: ArrayLike,\n    control: ArrayLike,\n) -&gt; float:\n    \"\"\"Computes the Adverse Impact Ratio (AIR), which is the ratio of negative\n    predictions for the protected class and the control class. The ideal ratio is 1.\n    For example, in an underwriting context, this means that the model is equally as\n    likely to approve protected applicants as it is unprotected applicants, given that\n    the model score is probability of bad.\n\n    Parameters\n    ----------\n    y_pred : ArrayLike\n        Predicted negative\n    protected : ArrayLike\n        An array of booleans identifying the protected class\n    control : ArrayLike\n        An array of booleans identifying the control class\n\n    Returns\n    -------\n    float\n        Adverse Impact Ratio (AIR)\n    \"\"\"\n    return _adverse_impact_ratio(\n        pl.DataFrame(\n            {\"y_pred\": y_pred, \"protected\": protected, \"control\": control}\n        ).cast(pl.Boolean)\n    )\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.adverse_impact_ratio_at_thresholds","title":"<code>adverse_impact_ratio_at_thresholds(y_score, protected, control, thresholds=None, strategy='auto')</code>","text":"<p>Computes the Adverse Impact Ratio (AIR) at each threshold of <code>y_score</code>. See rapidstats.adverse_impact_ratio for more details. When the <code>strategy</code> is <code>cum_sum</code>, computes</p> <pre><code>for t in y_score:\n    is_predicted_negative = y_score &lt; t\n    adverse_impact_ratio(is_predicted_negative, protected, control)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <code>protected</code> <code>ArrayLike</code> <p>An array of booleans identifying the protected class</p> required <code>control</code> <code>ArrayLike</code> <p>An array of booleans identifying the control class</p> required <code>thresholds</code> <code>Optional[list[float]]</code> <p>The thresholds to compute <code>is_predicted_negative</code> at, i.e. y_score &lt; t. If None, uses every score present in <code>y_score</code>, by default None</p> <code>None</code> <code>strategy</code> <code>LoopStrategy</code> <p>Computation method, by default \"auto\"</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame of <code>threshold</code> and <code>air</code></p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def adverse_impact_ratio_at_thresholds(\n    y_score: ArrayLike,\n    protected: ArrayLike,\n    control: ArrayLike,\n    thresholds: Optional[list[float]] = None,\n    strategy: LoopStrategy = \"auto\",\n) -&gt; pl.DataFrame:\n    \"\"\"Computes the Adverse Impact Ratio (AIR) at each threshold of `y_score`. See\n    [rapidstats.adverse_impact_ratio][] for more details. When the `strategy` is\n    `cum_sum`, computes\n\n\n    ``` py\n    for t in y_score:\n        is_predicted_negative = y_score &lt; t\n        adverse_impact_ratio(is_predicted_negative, protected, control)\n    ```\n\n    Parameters\n    ----------\n    y_score : ArrayLike\n        Predicted scores\n    protected : ArrayLike\n        An array of booleans identifying the protected class\n    control : ArrayLike\n        An array of booleans identifying the control class\n    thresholds : Optional[list[float]], optional\n        The thresholds to compute `is_predicted_negative` at, i.e. y_score &lt; t. If None,\n        uses every score present in `y_score`, by default None\n    strategy : LoopStrategy, optional\n        Computation method, by default \"auto\"\n\n    Returns\n    -------\n    pl.DataFrame\n        A DataFrame of `threshold` and `air`\n    \"\"\"\n    df = pl.DataFrame(\n        {\"y_score\": y_score, \"protected\": protected, \"control\": control}\n    ).with_columns(pl.col(\"protected\", \"control\").cast(pl.Boolean))\n\n    strategy = _set_loop_strategy(thresholds, strategy)\n\n    if strategy == \"loop\":\n\n        def _air(t):\n            return {\n                \"threshold\": t,\n                \"air\": _adverse_impact_ratio(\n                    df.select(\n                        pl.col(\"y_score\").lt(t).alias(\"y_pred\"), \"protected\", \"control\"\n                    )\n                ),\n            }\n\n        airs = _run_concurrent(_air, set(thresholds or y_score))\n\n        res = pl.LazyFrame(airs)\n    elif strategy == \"cum_sum\":\n        res = _air_at_thresholds_core(df, thresholds)\n\n    return res.pipe(_fill_infinite, None).fill_nan(None).collect()\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.brier_loss","title":"<code>brier_loss(y_true, y_score)</code>","text":"<p>Computes the Brier loss (smaller is better). The Brier loss measures the mean squared difference between the predicted scores and the ground truth target. Calculated as:</p> \\[ \\frac{1}{N} \\sum_{i=1}^N (yt_i - ys_i)^2 \\] <p>where \\( yt \\) is <code>y_true</code> and \\( ys \\) is <code>y_score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>float</code> <p>Brier loss</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def brier_loss(y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    r\"\"\"Computes the Brier loss (smaller is better). The Brier loss measures the mean\n    squared difference between the predicted scores and the ground truth target.\n    Calculated as:\n\n    \\[ \\frac{1}{N} \\sum_{i=1}^N (yt_i - ys_i)^2 \\]\n\n    where \\( yt \\) is `y_true` and \\( ys \\) is `y_score`.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    float\n        Brier loss\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score)\n\n    return _brier_loss(df)\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.confusion_matrix","title":"<code>confusion_matrix(y_true, y_pred)</code>","text":"<p>Computes confusion matrix metrics (TP, FP, TN, FN, TPR, F1, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_pred</code> <code>ArrayLike</code> <p>Predicted target</p> required <p>Returns:</p> Type Description <code>ConfusionMatrix</code> <p>Dataclass of confusion matrix metrics</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def confusion_matrix(y_true: ArrayLike, y_pred: ArrayLike) -&gt; ConfusionMatrix:\n    \"\"\"Computes confusion matrix metrics (TP, FP, TN, FN, TPR, F1, etc.).\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_pred : ArrayLike\n        Predicted target\n\n    Returns\n    -------\n    ConfusionMatrix\n        Dataclass of confusion matrix metrics\n    \"\"\"\n    df = _y_true_y_pred_to_df(y_true, y_pred)\n\n    return ConfusionMatrix(*_confusion_matrix(df))\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.confusion_matrix_at_thresholds","title":"<code>confusion_matrix_at_thresholds(y_true, y_score, thresholds=None, metrics=DefaultConfusionMatrixMetrics, strategy='auto')</code>","text":"<p>Computes the confusion matrix at each threshold. When the <code>strategy</code> is \"cum_sum\", computes</p> <pre><code>for t in y_score:\n    y_pred = y_score &gt;= t\n    confusion_matrix(y_true, y_pred)\n</code></pre> <p>using fast DataFrame operations.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <code>thresholds</code> <code>Optional[list[float]]</code> <p>The thresholds to compute <code>y_pred</code> at, i.e. y_score &gt;= t. If None, uses every score present in <code>y_score</code>, by default None</p> <code>None</code> <code>metrics</code> <code>Iterable[ConfusionMatrixMetric]</code> <p>The metrics to compute, by default DefaultConfusionMatrixMetrics</p> <code>DefaultConfusionMatrixMetrics</code> <code>strategy</code> <code>LoopStrategy</code> <p>Computation method, by default \"auto\"</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Polars DataFrame of <code>threshold</code>, <code>metric</code>, and <code>value</code></p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def confusion_matrix_at_thresholds(\n    y_true: ArrayLike,\n    y_score: ArrayLike,\n    thresholds: Optional[list[float]] = None,\n    metrics: Iterable[ConfusionMatrixMetric] = DefaultConfusionMatrixMetrics,\n    strategy: LoopStrategy = \"auto\",\n) -&gt; pl.DataFrame:\n    \"\"\"Computes the confusion matrix at each threshold. When the `strategy` is\n    \"cum_sum\", computes\n\n    ``` py\n    for t in y_score:\n        y_pred = y_score &gt;= t\n        confusion_matrix(y_true, y_pred)\n    ```\n\n    using fast DataFrame operations.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n    thresholds : Optional[list[float]], optional\n        The thresholds to compute `y_pred` at, i.e. y_score &gt;= t. If None,\n        uses every score present in `y_score`, by default None\n    metrics : Iterable[ConfusionMatrixMetric], optional\n        The metrics to compute, by default DefaultConfusionMatrixMetrics\n    strategy : LoopStrategy, optional\n        Computation method, by default \"auto\"\n\n    Returns\n    -------\n    pl.DataFrame\n        A Polars DataFrame of `threshold`, `metric`, and `value`\n    \"\"\"\n    strategy = _set_loop_strategy(thresholds, strategy)\n\n    if strategy == \"loop\":\n        df = _y_true_y_score_to_df(y_true, y_score)\n\n        def _cm(t):\n            return (\n                confusion_matrix(df[\"y_true\"], df[\"y_score\"].ge(t))\n                .to_polars()\n                .with_columns(pl.lit(t).alias(\"threshold\"))\n            )\n\n        cms: list[pl.DataFrame] = _run_concurrent(_cm, set(thresholds or y_score))\n\n        return pl.concat(cms, how=\"vertical\").fill_nan(None)\n    elif strategy == \"cum_sum\":\n        return (\n            pl.LazyFrame({\"y_true\": y_true, \"threshold\": y_score})\n            .with_columns(pl.col(\"y_true\").cast(pl.Boolean))\n            .drop_nulls()\n            .pipe(_base_confusion_matrix_at_thresholds)\n            .pipe(_full_confusion_matrix_from_base)\n            .select(\"threshold\", *metrics)\n            .unique(\"threshold\")\n            .pipe(_map_to_thresholds, thresholds)\n            .drop(\"_threshold_actual\", strict=False)\n            .unpivot(index=\"threshold\")\n            .rename({\"variable\": \"metric\"})\n            .collect()\n        )\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.max_ks","title":"<code>max_ks(y_true, y_score)</code>","text":"<p>Performs the two-sample Kolmogorov-Smirnov test on the predicted scores of the ground truth positive and ground truth negative classes. The KS test measures the highest distance between two CDFs, so the Max-KS metric measures how well the model separates two classes. In pseucode:</p> <pre><code>df = Frame(y_true, y_score)\nclass0 = df.filter(~y_true)[\"y_score\"]\nclass1 = df.filter(y_true)[\"y_score\"]\n\nks(class0, class1)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>float</code> <p>Max-KS</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def max_ks(y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    \"\"\"Performs the two-sample Kolmogorov-Smirnov test on the predicted scores of the\n    ground truth positive and ground truth negative classes. The KS test measures the\n    highest distance between two CDFs, so the Max-KS metric measures how well the model\n    separates two classes. In pseucode:\n\n    ``` py\n    df = Frame(y_true, y_score)\n    class0 = df.filter(~y_true)[\"y_score\"]\n    class1 = df.filter(y_true)[\"y_score\"]\n\n    ks(class0, class1)\n    ```\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    float\n        Max-KS\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score)\n\n    return _max_ks(df)\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.mean","title":"<code>mean(y)</code>","text":"<p>Computes the mean of the input array.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ArrayLike</code> <p>A 1D-array of numbers</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def mean(y: ArrayLike) -&gt; float:\n    \"\"\"Computes the mean of the input array.\n\n    Parameters\n    ----------\n    y : ArrayLike\n        A 1D-array of numbers\n\n    Returns\n    -------\n    float\n        Mean\n    \"\"\"\n    return _mean(pl.DataFrame({\"y\": y}))\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.mean_squared_error","title":"<code>mean_squared_error(y_true, y_score)</code>","text":"<p>Computes Mean Squared Error (MSE) as</p> \\[ \\frac{1}{N} \\sum_{i=1}^{N} (yt_i - ys_i)^2 \\] <p>where \\( yt \\) is <code>y_true</code> and \\( ys \\) is <code>y_score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean Squared Error (MSE)</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def mean_squared_error(y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    r\"\"\"Computes Mean Squared Error (MSE) as\n\n    \\[ \\frac{1}{N} \\sum_{i=1}^{N} (yt_i - ys_i)^2 \\]\n\n    where \\( yt \\) is `y_true` and \\( ys \\) is `y_score`.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    float\n        Mean Squared Error (MSE)\n    \"\"\"\n    return _mean_squared_error(_regression_to_df(y_true, y_score))\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.predicted_positive_ratio_at_thresholds","title":"<code>predicted_positive_ratio_at_thresholds(y_score, thresholds=None, strategy='auto')</code>","text":"<p>Computes the Predicted Positive Ratio (PPR) at each threshold, where the PPR is the ratio of predicted positive to the total, and a positive is defined as <code>y_score</code> &gt;= threshold.</p> <p>Parameters:</p> Name Type Description Default <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <code>thresholds</code> <code>Optional[list[float]]</code> <p>The thresholds to compute <code>y_pred</code> at, i.e. y_score &gt;= t. If None, uses every score present in <code>y_score</code>, by default None</p> <code>None</code> <code>strategy</code> <code>LoopStrategy</code> <p>Computation method, by default \"auto\"</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame of <code>threshold</code> and <code>ppr</code></p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def predicted_positive_ratio_at_thresholds(\n    y_score: ArrayLike,\n    thresholds: Optional[list[float]] = None,\n    strategy: LoopStrategy = \"auto\",\n) -&gt; pl.DataFrame:\n    \"\"\"Computes the Predicted Positive Ratio (PPR) at each threshold, where the PPR is\n    the ratio of predicted positive to the total, and a positive is defined as\n    `y_score` &gt;= threshold.\n\n    Parameters\n    ----------\n    y_score : ArrayLike\n        Predicted scores\n    thresholds : Optional[list[float]], optional\n        The thresholds to compute `y_pred` at, i.e. y_score &gt;= t. If None,\n        uses every score present in `y_score`, by default None\n    strategy : LoopStrategy, optional\n        Computation method, by default \"auto\"\n\n    Returns\n    -------\n    pl.DataFrame\n        A DataFrame of `threshold` and `ppr`\n    \"\"\"\n    strategy = _set_loop_strategy(y_score, strategy)\n\n    if strategy == \"loop\":\n        s = pl.Series(y_score).drop_nulls()\n\n        def _ppr(t: float) -&gt; float:\n            return {\"threshold\": t, \"ppr\": s.ge(t).mean()}\n\n        return pl.DataFrame(_run_concurrent(_ppr, set(thresholds or y_score)))\n    elif strategy == \"cum_sum\":\n        return (\n            pl.LazyFrame({\"y_score\": y_score})\n            .drop_nulls()\n            .sort(\"y_score\", descending=True)\n            .with_row_index(\"cumulative_predicted_positive\", offset=1)\n            .with_columns(\n                pl.col(\"cumulative_predicted_positive\").truediv(pl.len()).alias(\"ppr\")\n            )\n            .rename({\"y_score\": \"threshold\"})\n            .select(\"threshold\", \"ppr\")\n            .unique(\"threshold\")\n            .pipe(_map_to_thresholds, thresholds)\n            .drop(\"_threshold_actual\", strict=False)\n            .collect()\n        )\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.roc_auc","title":"<code>roc_auc(y_true, y_score)</code>","text":"<p>Computes Area Under the Receiver Operating Characteristic Curve.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>float</code> <p>ROC-AUC</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def roc_auc(y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    \"\"\"Computes Area Under the Receiver Operating Characteristic Curve.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    float\n        ROC-AUC\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score).with_columns(\n        pl.col(\"y_true\").cast(pl.Float64)\n    )\n\n    return _roc_auc(df)\n</code></pre>"},{"location":"metrics.html#rapidstats._metrics.root_mean_squared_error","title":"<code>root_mean_squared_error(y_true, y_score)</code>","text":"<p>Computes Root Mean Squared Error (RMSE) as</p> \\[ \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (yt_i - ys_i)^2} \\] <p>where \\( yt \\) is <code>y_true</code> and \\( ys \\) is <code>y_score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>float</code> <p>Root Mean Squared Error (RMSE)</p> Source code in <code>python/rapidstats/_metrics.py</code> <pre><code>def root_mean_squared_error(y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    r\"\"\"Computes Root Mean Squared Error (RMSE) as\n\n    \\[ \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (yt_i - ys_i)^2} \\]\n\n    where \\( yt \\) is `y_true` and \\( ys \\) is `y_score`.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    float\n        Root Mean Squared Error (RMSE)\n    \"\"\"\n    return _root_mean_squared_error(_regression_to_df(y_true, y_score))\n</code></pre>"}]}