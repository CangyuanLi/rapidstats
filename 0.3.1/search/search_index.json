{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"rapidstats:","text":"<p> Documentation </p>"},{"location":"index.html#what-is-it","title":"What is it?","text":"<p>rapidstats is a minimal library that implements fast statistical routines in Rust and Polars. While similar in spirit, it does not aim to be a complete re-implementation of libraries like scikit-learn or scipy. Only functions that can be significantly faster (e.g. a bootstrap class that offers optimized Rust kernels for metrics such as ROC-AUC) or significantly more ergonomic (e.g. dataframe-first encoders and scalers) are added.</p> <p>This library is in an alpha state. Although all functions are tested against existing libraries, use at your own risk. The API is subject to change very frequently.</p>"},{"location":"index.html#usage","title":"Usage:","text":""},{"location":"index.html#dependencies","title":"Dependencies","text":"<p>rapidstats has a minimal set of dependencies. It only depends on polars, narwhals (for dataframe compatibility), and tqdm (for progress bars). You may install pyarrow (<code>pip install rapidstats[pyarrow]</code>) to allow functions to take numpy arrays, pandas objects, and other objects that may be converted through Arrow.</p>"},{"location":"index.html#installing","title":"Installing","text":"<p>The easiest way is to install rapidstats is from PyPI using pip:</p> <pre><code>pip install rapidstats\n</code></pre>"},{"location":"index.html#performance","title":"Performance","text":"<p>rapidstats is very fast. For example, say you wanted the confusion matrix metrics for a 50,000 row dataset. You aren't sure what exact threshold you want yet, so you decide to compute the metrics for multiple thresholds, let's say 500. With sklearn, this takes 40 seconds. With rapidstats, this takes just .2 seconds, a 198x speedup! Furthermore, rapidstats can use a cumuluative sum algorithm that computes the metrics at all possible thresholds, not just these particular 500. So finding the metrics for 500 or 50,000 metrics takes the exact same amount of time. In addition, even just looping the rapidstats version is a 58x speedup, since rapidstats applies several optimizations, such as computing the basic confusion matrix (TP, FP, FN, TN) using a nice bincount trick and avoiding re-computing this basic matrix for each different metric.</p> <p></p> <p>Similarly, calculating the bootstrapped (100 iterations) ROC-AUC of a 25,000 sample dataset takes only .15 seconds, compared to .83 seconds for the equivalent sklearn + scipy operation, a speedup of 5.3x.</p> <p></p>"},{"location":"bin.html","title":"Binning","text":"<p>Functions:</p> Name Description <code>doane</code> <p>Doane's rule defines the bin count as</p> <code>freedman_diaconis</code> <p>The Freedman-Diaconis rule defines the bin width as</p> <code>rice</code> <p>Rice's rule defines the bin count as</p> <code>scott</code> <p>Scott's rule defines the bin width as</p> <code>sqrt</code> <p>The square root rule defines the bin count as</p> <code>sturges</code> <p>Sturges' rule defines the bin count as</p>"},{"location":"bin.html#rapidstats.bin.doane","title":"<code>doane(x)</code>","text":"<p>Doane's rule defines the bin count as</p> \\[     k = 1 + \\log_{2}(n) + \\log_{2}\\left(1 + \\frac{|g_{1}|}{\\sigma_{g_{1}}}\\right) \\] <p>where</p> \\[     \\sigma_{g_{1}} = \\sqrt{\\frac{6(n-2)}{(n+1)(n+3)}} \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> required <p>Returns:</p> Type Description <code>int</code> <p>Bin count</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If \\(n &lt; 2\\)</p> Source code in <code>python/rapidstats/bin.py</code> <pre><code>def doane(x: ArrayLike) -&gt; int:\n    r\"\"\"Doane's rule defines the bin count as\n\n    \\[\n        k = 1 + \\log_{2}(n) + \\log_{2}\\left(1 + \\frac{|g_{1}|}{\\sigma_{g_{1}}}\\right)\n    \\]\n\n    where\n\n    \\[\n        \\sigma_{g_{1}} = \\sqrt{\\frac{6(n-2)}{(n+1)(n+3)}}\n    \\]\n\n    Parameters\n    ----------\n    x : ArrayLike\n\n    Returns\n    -------\n    int\n        Bin count\n\n    Raises\n    ------\n    ValueError\n        If $n &lt; 2$\n    \"\"\"\n    x = pl.Series(x)\n    x_len = x.len()\n\n    if x_len &lt;= 2:\n        raise ValueError(\"Doane's rule requires at least 3 observations\")\n\n    g1 = abs(x.skew())\n    sg1 = math.sqrt(6.0 * (x_len - 2) / ((x_len + 1.0) * (x_len + 3)))\n\n    return int(1 + math.log2(x_len) + math.log2(1 + (g1 / sg1)))\n</code></pre>"},{"location":"bin.html#rapidstats.bin.freedman_diaconis","title":"<code>freedman_diaconis(x)</code>","text":"<p>The Freedman-Diaconis rule defines the bin width as</p> \\[     h = 2\\frac{IQR(x)}{\\sqrt[3]{n}} \\] <p>where \\(x\\) is the input array and \\(n\\) is the length of \\(x\\).</p> <p>The bin width is converted to a bin count via</p> \\[     k = \\lceil \\frac{\\max{x} - \\min{x}}{h} \\rceil \\] <p>If \\(h\\) is 0, compute the generalized IQR using successively larger intervals (e.g. .01 and .99 instead of .25 and .75) to determine \\(h\\). As a last ditch effort, use 3.5 times the standard deviation as \\(h\\).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> required <p>Returns:</p> Type Description <code>int</code> <p>Bin count</p> Source code in <code>python/rapidstats/bin.py</code> <pre><code>def freedman_diaconis(x: ArrayLike) -&gt; int:\n    r\"\"\"The Freedman-Diaconis rule defines the bin width as\n\n    \\[\n        h = 2\\frac{IQR(x)}{\\sqrt[3]{n}}\n    \\]\n\n    where $x$ is the input array and $n$ is the length of $x$.\n\n    The bin width is converted to a bin count via\n\n    \\[\n        k = \\lceil \\frac{\\max{x} - \\min{x}}{h} \\rceil\n    \\]\n\n    If $h$ is 0, compute the generalized IQR using successively larger intervals (e.g.\n    .01 and .99 instead of .25 and .75) to determine $h$. As a last ditch effort, use\n    3.5 times the standard deviation as $h$.\n\n    Parameters\n    ----------\n    x : ArrayLike\n\n    Returns\n    -------\n    int\n        Bin count\n    \"\"\"\n    x = pl.Series(x)\n\n    iqr = x.quantile(0.75, interpolation=\"linear\") - x.quantile(\n        0.25, interpolation=\"linear\"\n    )\n    h = 2 * iqr\n\n    # It's possible that the IQR is 0. In that case, we try and compute the generalized\n    # IQR using successively wider quantiles. Taken from R's hist.default function and\n    # this answer: https://stats.stackexchange.com/questions/455237/what-to-do-when-iqr-returns-0-in-freedman-diaconis-rule\n    alpha = 1 / 4\n    alpha_min = 1 / 512\n\n    while h == 0 and alpha &gt;= alpha_min:\n        alpha /= 2\n\n        h = (\n            x.quantile(alpha, interpolation=\"linear\")\n            - x.quantile(1 - alpha, interpolation=\"linear\")\n        ) / (1 - 2 * alpha)\n\n    # As a last ditch, use 3.5 times the standard deviation\n    if h == 0:\n        h = 3.5 * x.std()\n\n    bin_width = h * x.len() ** (-1.0 / 3.0)\n\n    return _bin_width_to_count(x, bin_width)\n</code></pre>"},{"location":"bin.html#rapidstats.bin.rice","title":"<code>rice(x)</code>","text":"<p>Rice's rule defines the bin count as</p> \\[     k = \\lceil 2n^{\\frac{1}{3}} \\rceil \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> required <p>Returns:</p> Type Description <code>int</code> <p>Bin count</p> Source code in <code>python/rapidstats/bin.py</code> <pre><code>def rice(x: ArrayLike) -&gt; int:\n    r\"\"\"Rice's rule defines the bin count as\n\n    \\[\n        k = \\lceil 2n^{\\frac{1}{3}} \\rceil\n    \\]\n\n    Parameters\n    ----------\n    x : ArrayLike\n\n    Returns\n    -------\n    int\n        Bin count\n    \"\"\"\n    return math.ceil(2 * (len(x) ** (1 / 3)))\n</code></pre>"},{"location":"bin.html#rapidstats.bin.scott","title":"<code>scott(x)</code>","text":"<p>Scott's rule defines the bin width as</p> \\[     h = 3.49\\sigma n^{-\\frac{1}{3}} \\] <p>The bin count is given by</p> \\[     k = \\lceil \\frac{\\max{x} - \\min{x}}{h} \\rceil \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> required <p>Returns:</p> Type Description <code>int</code> <p>Bin count</p> Source code in <code>python/rapidstats/bin.py</code> <pre><code>def scott(x: ArrayLike) -&gt; int:\n    r\"\"\"Scott's rule defines the bin width as\n\n    \\[\n        h = 3.49\\sigma n^{-\\frac{1}{3}}\n    \\]\n\n    The bin count is given by\n\n    \\[\n        k = \\lceil \\frac{\\max{x} - \\min{x}}{h} \\rceil\n    \\]\n\n    Parameters\n    ----------\n    x : ArrayLike\n\n    Returns\n    -------\n    int\n        Bin count\n    \"\"\"\n    x = pl.Series(x)\n\n    bin_width = (3.49 * x.std()) / (len(x) ** (1 / 3))\n\n    return _bin_width_to_count(x, bin_width)\n</code></pre>"},{"location":"bin.html#rapidstats.bin.sqrt","title":"<code>sqrt(x)</code>","text":"<p>The square root rule defines the bin count as</p> \\[     k = \\lceil\\sqrt{n}\\rceil \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> required <p>Returns:</p> Type Description <code>int</code> <p>Bin count</p> Source code in <code>python/rapidstats/bin.py</code> <pre><code>def sqrt(x: ArrayLike) -&gt; int:\n    r\"\"\"The square root rule defines the bin count as\n\n    \\[\n        k = \\lceil\\sqrt{n}\\rceil\n    \\]\n\n    Parameters\n    ----------\n    x : ArrayLike\n\n    Returns\n    -------\n    int\n        Bin count\n    \"\"\"\n    return math.ceil(math.sqrt(len(x)))\n</code></pre>"},{"location":"bin.html#rapidstats.bin.sturges","title":"<code>sturges(x)</code>","text":"<p>Sturges' rule defines the bin count as</p> \\[     k = \\lceil 1 + \\log_{2}(n) \\rceil \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> required <p>Returns:</p> Type Description <code>int</code> <p>Bin count</p> Source code in <code>python/rapidstats/bin.py</code> <pre><code>def sturges(x: ArrayLike) -&gt; int:\n    r\"\"\"Sturges' rule defines the bin count as\n\n    \\[\n        k = \\lceil 1 + \\log_{2}(n) \\rceil\n    \\]\n\n    Parameters\n    ----------\n    x : ArrayLike\n\n    Returns\n    -------\n    int\n        Bin count\n    \"\"\"\n    return math.ceil(math.log2(len(x))) + 1\n</code></pre>"},{"location":"bootstrap.html","title":"Bootstrap","text":"<p>Classes:</p> Name Description <code>Bootstrap</code> <p>Computes a two-sided bootstrap confidence interval of a statistic. Note that</p> <code>BootstrappedConfusionMatrix</code> <p>Result object returned by <code>rapidstats.Bootstrap().confusion_matrix</code>.</p>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap","title":"<code>Bootstrap</code>","text":"<p>Computes a two-sided bootstrap confidence interval of a statistic. Note that \\( \\alpha \\) is then defined as \\( \\frac{1 - \\text{confidence}}{2} \\). Regardless of method, the result will be a three-tuple of (lower, point, upper), where point is the point estimate. The process is as follows:</p> <ul> <li>Resample 100% of the data with replacement for <code>iterations</code></li> <li>Compute the statistic on each resample</li> </ul> <p>If the method is <code>standard</code>,</p> <ul> <li>Compute the statistic on the original data \\( \\hat{\\theta} \\)</li> <li>Compute the standard error of the bootstrap statistics. Note that the standard error of any statistic is defined as the standard deviation of its sampling distribution.</li> <li> <p>Compute the Z-score</p> \\[ z_{\\alpha} = \\phi^{-1}(\\alpha) \\] <p>where \\( \\phi^{-1} \\) is the quantile, inverse CDF, or percent-point function</p> </li> </ul> <p>Then the \"Standard\" or \"First-Order Normal Approximation\" interval is</p> \\[ \\hat{\\theta} \\pm z_{\\alpha} \\times \\hat{\\sigma} \\] <p>If the method is <code>percentile</code>, we stop here and compute the interval of the bootstrap distribution that is symmetric about the median and contains <code>confidence</code> of the bootstrap statistics. Then the \"Percentile\" interval is</p> \\[     [\\text{percentile}(\\hat{\\theta}^{*}, \\alpha),     \\text{percentile}(\\hat{\\theta}^{*}, 1 - \\alpha)] \\] <p>where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics.</p> <p>If the method is <code>basic</code>,</p> <ul> <li>Compute the statistic on the original data</li> <li>Compute the \"Percentile\" interval</li> </ul> <p>Then the \"Basic\" or \"Reverse Percentile\" interval is</p> \\[     [2\\hat{\\theta} - PCI_u,     2\\hat{\\theta} - PCI_l,] \\] <p>where \\( \\hat{\\theta} \\) is the statistic on the original data, \\( PCI_u \\) is the upper bound of the \"Percentile\" interval, and \\( PCI_l \\) is the lower bound of the \"Percentile\" interval.</p> <p>If the method is <code>BCa</code>,</p> <ul> <li>Compute the statistic on the original data \\( \\hat{\\theta} \\)</li> <li>Compute the statistic on the data with the \\( i^{th} \\) row deleted (jacknife)</li> <li> <p>Compute the bias correction factor as</p> \\[     \\hat{z_0} = \\phi^{-1}(         \\frac{\\sum_{i=1}^B \\hat{\\theta_i}^{*} \\le \\hat{\\theta}         + \\sum_{i=1}^B \\hat{\\theta_i}^{*} \\leq \\hat{\\theta}}{2 * B}     ) \\] <p>where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics and \\( B \\) is the length of that vector.</p> </li> <li> <p>Compute the acceleration factor as</p> \\[     \\hat{a} = \\frac{1}{6} \\frac{         \\sum_{i=1}^{N} (\\hat{\\theta_{(.)}} - \\hat{\\theta_i})^3     }{         \\sum_{i=1}^{N} [(\\hat{\\theta_{(.)}} - \\hat{\\theta_i})^2]^{1.5}     } \\] <p>where \\( \\hat{\\theta_{(.)}} \\) is the mean of the jacknife statistics and \\( \\hat{\\theta_i} \\) is the \\( i^{th} \\) element of the jacknife vector.</p> </li> <li> <p>Compute the lower and upper percentiles as</p> \\[     \\alpha_l = \\phi(         \\hat{z_0} + \\frac{\\hat{z_0} + z_{\\alpha}}{1 - \\hat{a}(\\hat{z} + z_{\\alpha})}     ) \\] <p>and</p> \\[     \\alpha_u = \\phi(         \\hat{z_0} + \\frac{             \\hat{z_0} + z_{1 - \\alpha}         }{             1 - \\hat{a}(\\hat{z} + z_{1-\\alpha})         }     ) \\] </li> </ul> <p>Then the \"BCa\" or \"Bias-Corrected and Accelerated\" interval is</p> \\[     [\\text{percentile}(\\hat{\\theta}^{*}, \\alpha_l),     \\text{percentile}(\\hat{\\theta}^{*}, \\alpha_u)] \\] <p>where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>How many times to resample the data, by default 1_000</p> <code>1000</code> <code>confidence</code> <code>float</code> <p>The confidence level, by default 0.95</p> <code>0.95</code> <code>method</code> <code>Literal['standard', 'percentile', 'basic', 'BCa']</code> <p>Whether to return the Percentile, Basic / Reverse Percentile, or Bias Corrected and Accelerated Interval, by default \"percentile\"</p> <code>'percentile'</code> <code>sampling_method</code> <code>Literal['poisson', 'multinomial']</code> <p>How to sample. If \"multinomial\", sample with replacement. If \"poisson\", simulate number of draws via a Poisson(1) distribution. Note that \"poisson\" is usually much more performant, especially since order is preserved, which allows certain functions to avoid sorting every iteration. However, \"poisson\" is still in a beta stage, by default \"multinomial\"</p> <code>'multinomial'</code> <code>seed</code> <code>Optional[int]</code> <p>Seed that controls resampling. Set this to any integer to make results reproducible, by default None</p> <code>None</code> <code>n_jobs</code> <code>Optional[int]</code> <p>How many threads to run with. None means let the executor decide, and 1 means run sequentially, by default None</p> <code>None</code> <code>chunksize</code> <code>Optional[int]</code> <p>The chunksize for each thread. None means let the executor decide, by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the method is not one of <code>standard</code>, <code>percentile</code>, <code>basic</code>, or <code>BCa</code></p> <code>ValueError</code> <p>If the sampling method is not one of <code>poisson</code> or <code>multinomial</code></p> <p>Examples:</p> <p><pre><code>import rapidstats as rs\nci = rs.Bootstrap(seed=208).mean([1, 1, 2, 3])\n</code></pre> (1.0, 1.75, 2.5)</p> <p>Methods:</p> Name Description <code>adverse_impact_ratio</code> <p>Bootstrap AIR. See rapidstats.metrics.adverse_impact_ratio for more details.</p> <code>adverse_impact_ratio_at_thresholds</code> <p>Bootstrap AIR at thresholds. See</p> <code>average_precision</code> <p>Bootstrap average precision. See rapidstats.metrics.average_precision for more</p> <code>brier_loss</code> <p>Bootstrap Brier loss. See rapidstats.metrics.brier_loss for more details.</p> <code>confusion_matrix</code> <p>Bootstrap confusion matrix. See rapidstats.metrics.confusion_matrix for</p> <code>confusion_matrix_at_thresholds</code> <p>Bootstrap confusion matrix at thresholds. See</p> <code>max_ks</code> <p>Bootstrap Max-KS. See rapidstats.metrics.max_ks for more details.</p> <code>mean</code> <p>Bootstrap mean.</p> <code>mean_squared_error</code> <p>Bootstrap MSE. See rapidstats.metrics.mean_squared_error for more details.</p> <code>r2</code> <p>Bootstrap R2. See rapidstats.metrics.r2 for more details.</p> <code>roc_auc</code> <p>Bootstrap ROC-AUC. See rapidstats.metrics.roc_auc for more details.</p> <code>root_mean_squared_error</code> <p>Bootstrap RMSE. See rapidstats.metrics.root_mean_squared_error for more details.</p> <code>run</code> <p>Run bootstrap for an arbitrary function that accepts a Polars DataFrame and</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>class Bootstrap:\n    r\"\"\"Computes a two-sided bootstrap confidence interval of a statistic. Note that\n    \\( \\alpha \\) is then defined as \\( \\frac{1 - \\text{confidence}}{2} \\). Regardless\n    of method, the result will be a three-tuple of (lower, point, upper), where point is\n    the point estimate. The process is as follows:\n\n    - Resample 100% of the data with replacement for `iterations`\n    - Compute the statistic on each resample\n\n    If the method is `standard`,\n\n    - Compute the statistic on the original data \\( \\hat{\\theta} \\)\n    - Compute the standard error of the bootstrap statistics. Note that the standard\n    error of any statistic is defined as the standard deviation of its sampling\n    distribution.\n    - Compute the Z-score\n\n        \\[ z_{\\alpha} = \\phi^{-1}(\\alpha) \\]\n\n        where \\( \\phi^{-1} \\) is the quantile, inverse CDF, or percent-point function\n\n    Then the \"Standard\" or \"First-Order Normal Approximation\" interval is\n\n    \\[ \\hat{\\theta} \\pm z_{\\alpha} \\times \\hat{\\sigma} \\]\n\n    If the method is `percentile`, we stop here and compute the interval of the\n    bootstrap distribution that is symmetric about the median and contains\n    `confidence` of the bootstrap statistics. Then the \"Percentile\" interval is\n\n    \\[\n        [\\text{percentile}(\\hat{\\theta}^{*}, \\alpha),\n        \\text{percentile}(\\hat{\\theta}^{*}, 1 - \\alpha)]\n    \\]\n\n    where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics.\n\n    If the method is `basic`,\n\n    - Compute the statistic on the original data\n    - Compute the \"Percentile\" interval\n\n    Then the \"Basic\" or \"Reverse Percentile\" interval is\n\n    \\[\n        [2\\hat{\\theta} - PCI_u,\n        2\\hat{\\theta} - PCI_l,]\n    \\]\n\n    where \\( \\hat{\\theta} \\) is the statistic on the original data, \\( PCI_u \\) is the\n    upper bound of the \"Percentile\" interval, and \\( PCI_l \\) is the lower bound of the\n    \"Percentile\" interval.\n\n    If the method is `BCa`,\n\n    - Compute the statistic on the original data \\( \\hat{\\theta} \\)\n    - Compute the statistic on the data with the \\( i^{th} \\) row deleted (jacknife)\n    - Compute the bias correction factor as\n\n        \\[\n            \\hat{z_0} = \\phi^{-1}(\n                \\frac{\\sum_{i=1}^B \\hat{\\theta_i}^{*} \\le \\hat{\\theta}\n                + \\sum_{i=1}^B \\hat{\\theta_i}^{*} \\leq \\hat{\\theta}}{2 * B}\n            )\n        \\]\n\n        where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics and \\( B \\)\n        is the length of that vector.\n\n    - Compute the acceleration factor as\n\n        \\[\n            \\hat{a} = \\frac{1}{6} \\frac{\n                \\sum_{i=1}^{N} (\\hat{\\theta_{(.)}} - \\hat{\\theta_i})^3\n            }{\n                \\sum_{i=1}^{N} [(\\hat{\\theta_{(.)}} - \\hat{\\theta_i})^2]^{1.5}\n            }\n        \\]\n\n        where \\( \\hat{\\theta_{(.)}} \\) is the mean of the jacknife statistics and\n        \\( \\hat{\\theta_i} \\) is the \\( i^{th} \\) element of the jacknife vector.\n\n    - Compute the lower and upper percentiles as\n\n        \\[\n            \\alpha_l = \\phi(\n                \\hat{z_0} + \\frac{\\hat{z_0} + z_{\\alpha}}{1 - \\hat{a}(\\hat{z} + z_{\\alpha})}\n            )\n        \\]\n\n        and\n\n        \\[\n            \\alpha_u = \\phi(\n                \\hat{z_0} + \\frac{\n                    \\hat{z_0} + z_{1 - \\alpha}\n                }{\n                    1 - \\hat{a}(\\hat{z} + z_{1-\\alpha})\n                }\n            )\n        \\]\n\n    Then the \"BCa\" or \"Bias-Corrected and Accelerated\" interval is\n\n    \\[\n        [\\text{percentile}(\\hat{\\theta}^{*}, \\alpha_l),\n        \\text{percentile}(\\hat{\\theta}^{*}, \\alpha_u)]\n    \\]\n\n    where \\( \\hat{\\theta}^{*} \\) is the vector of bootstrap statistics.\n\n    Parameters\n    ----------\n    iterations : int, optional\n        How many times to resample the data, by default 1_000\n    confidence : float, optional\n        The confidence level, by default 0.95\n    method : Literal[\"standard\", \"percentile\", \"basic\", \"BCa\"], optional\n        Whether to return the Percentile, Basic / Reverse Percentile, or\n        Bias Corrected and Accelerated Interval, by default \"percentile\"\n    sampling_method: Literal[\"poisson\", \"multinomial\"], optional\n        How to sample. If \"multinomial\", sample with replacement. If \"poisson\", simulate\n        number of draws via a Poisson(1) distribution. Note that \"poisson\" is usually\n        much more performant, especially since order is preserved, which allows certain\n        functions to avoid sorting every iteration. However, \"poisson\" is still in a\n        beta stage, by default \"multinomial\"\n    seed : Optional[int], optional\n        Seed that controls resampling. Set this to any integer to make results\n        reproducible, by default None\n    n_jobs: Optional[int], optional\n        How many threads to run with. None means let the executor decide, and 1 means\n        run sequentially, by default None\n    chunksize: Optional[int], optional\n        The chunksize for each thread. None means let the executor decide, by default\n        None\n\n    Raises\n    ------\n    ValueError\n        If the method is not one of `standard`, `percentile`, `basic`, or `BCa`\n    ValueError\n        If the sampling method is not one of `poisson` or `multinomial`\n\n    Examples\n    --------\n    ``` py\n    import rapidstats as rs\n    ci = rs.Bootstrap(seed=208).mean([1, 1, 2, 3])\n    ```\n    (1.0, 1.75, 2.5)\n    \"\"\"\n\n    def __init__(\n        self,\n        iterations: int = 1_000,\n        confidence: float = 0.95,\n        method: Literal[\"standard\", \"percentile\", \"basic\", \"BCa\"] = \"percentile\",\n        sampling_method: Literal[\"poisson\", \"multinomial\"] = \"multinomial\",\n        seed: Optional[int] = None,\n        n_jobs: Optional[int] = None,\n        chunksize: Optional[int] = None,\n    ) -&gt; None:\n        if method not in (\"standard\", \"percentile\", \"basic\", \"BCa\"):\n            raise ValueError(\n                f\"Invalid confidence interval method `{method}`, only `standard`, `percentile`, `basic`, and `BCa` are supported\",\n            )\n\n        if sampling_method not in (\"poisson\", \"multinomial\"):\n            raise ValueError(\n                f\"Invalid sampling method `{sampling_method}`, only `poisson` and `multinomial` are supported\"\n            )\n\n        self.iterations = iterations\n        self.confidence = confidence\n        self.seed = seed\n        self.alpha = (1 - confidence) / 2\n        self.method = method\n        self.sampling_method = sampling_method\n        self.n_jobs = n_jobs\n        self.chunksize = chunksize\n\n        self._params = {\n            \"iterations\": self.iterations,\n            \"alpha\": self.alpha,\n            \"method\": self.method,\n            \"seed\": self.seed,\n            \"n_jobs\": self.n_jobs,\n            \"chunksize\": self.chunksize,\n            \"poisson\": self.sampling_method == \"poisson\",\n        }\n\n    def run(\n        self, df: pl.DataFrame, stat_func: StatFunc, **kwargs\n    ) -&gt; ConfidenceInterval:\n        \"\"\"Run bootstrap for an arbitrary function that accepts a Polars DataFrame and\n        returns a scalar real number.\n\n        Parameters\n        ----------\n        df : pl.DataFrame\n            The data to pass to `stat_func`\n        stat_func : StatFunc\n            A callable that takes a Polars DataFrame as its first argument and returns\n            a scalar real number.\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, point, upper)\n\n        Added in version 0.1.0\n        ----------------------\n        \"\"\"\n        default = {\"executor\": \"threads\", \"preserve_order\": False}\n        for k, v in default.items():\n            if k not in kwargs:\n                kwargs[k] = v\n\n        if self._params[\"poisson\"]:\n            func = functools.partial(\n                _poisson_bs_func, df=df, df_height=df.height, stat_func=stat_func\n            )\n        else:\n            func = functools.partial(_bs_func, df=df, stat_func=stat_func)\n\n        if self.seed is None:\n            iterable = (None for _ in range(self.iterations))\n        else:\n            iterable = (self.seed + i for i in range(self.iterations))\n\n        bootstrap_stats = [\n            x for x in _run_concurrent(func, iterable, **kwargs) if not math.isnan(x)\n        ]\n\n        original_stat = stat_func(df)\n\n        if len(bootstrap_stats) == 0:\n            return (math.nan, math.nan, math.nan)\n\n        if self.method == \"standard\":\n            return _standard_interval(original_stat, bootstrap_stats, self.alpha)\n        elif self.method == \"percentile\":\n            return _percentile_interval(original_stat, bootstrap_stats, self.alpha)\n        elif self.method == \"basic\":\n            return _basic_interval(original_stat, bootstrap_stats, self.alpha)\n        elif self.method == \"BCa\":\n            jacknife_stats = [x for x in _jacknife(df, stat_func) if not math.isnan(x)]\n\n            return _bca_interval(\n                original_stat, bootstrap_stats, jacknife_stats, self.alpha\n            )\n        else:\n            # We shouldn't hit this since we check method in __init__, but it makes the\n            # type-checker happy\n            raise ValueError(\"Invalid method\")\n\n    def confusion_matrix(\n        self,\n        y_true: ArrayLike,\n        y_pred: ArrayLike,\n        beta: float = 1.0,\n        sample_weight: Optional[ArrayLike] = None,\n    ) -&gt; BootstrappedConfusionMatrix:\n        r\"\"\"Bootstrap confusion matrix. See [rapidstats.metrics.confusion_matrix][] for\n        more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_pred : ArrayLike\n            Predicted target\n        beta : float, optional\n            \\( \\beta \\) to use in \\( F_\\beta \\), by default 1\n        sample_weight: Optional[ArrayLike], optional\n            Sample weights, set to 1 if None\n\n            !!! Version\n                Added 0.2.0\n\n        Returns\n        -------\n        BootstrappedConfusionMatrix\n            A dataclass of confusion matrix metrics as (lower, point, upper). See\n            [rapidstats._bootstrap.BootstrappedConfusionMatrix][] for more details.\n\n        Added in version 0.1.0\n        ----------------------\n        \"\"\"\n        df = _y_true_y_pred_to_df(y_true, y_pred, sample_weight).with_columns(\n            pl.col(\"y_true\").cast(pl.UInt8)\n        )\n\n        return BootstrappedConfusionMatrix(\n            *_bootstrap_confusion_matrix(df, beta, **self._params)\n        )\n\n    def confusion_matrix_at_thresholds(\n        self,\n        y_true: ArrayLike,\n        y_score: ArrayLike,\n        thresholds: Optional[list[float]] = None,\n        metrics: Iterable[ConfusionMatrixMetric] = DefaultConfusionMatrixMetrics,\n        strategy: LoopStrategy = \"auto\",\n        beta: float = 1.0,\n        sample_weight: Optional[ArrayLike] = None,\n    ) -&gt; pl.DataFrame:\n        r\"\"\"Bootstrap confusion matrix at thresholds. See\n        [rapidstats.metrics.confusion_matrix_at_thresholds][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n        thresholds : Optional[list[float]], optional\n            The thresholds to compute `y_pred` at, i.e. y_score &gt;= t. If None,\n            uses every score present in `y_score`, by default None\n        metrics : Iterable[ConfusionMatrixMetric], optional\n            The metrics to compute, by default DefaultConfusionMatrixMetrics\n        strategy : LoopStrategy, optional\n            Computation method, by default \"auto\"\n        beta : float, optional\n            \\( \\beta \\) to use in \\( F_\\beta \\), by default 1\n        sample_weight: Optional[ArrayLike], optional\n            Sample weights, set to 1 if None\n\n            !!! Version\n                Added 0.2.0\n\n        Returns\n        -------\n        pl.DataFrame\n            A DataFrame of `threshold`, `metric`, `lower`, `mean`, and `upper`\n\n        Raises\n        ------\n        NotImplementedError\n            When `strategy` is `cum_sum` and `method` is `BCa`\n\n        Added in version 0.1.0\n        ----------------------\n        \"\"\"\n        df = (\n            _y_true_y_score_to_df(y_true, y_score, sample_weight)\n            .rename({\"y_score\": \"threshold\"})\n            .sort(\"threshold\", descending=True)\n        )\n        final_cols = [\"threshold\", \"metric\", \"lower\", \"point\", \"upper\"]\n\n        strategy = _set_loop_strategy(thresholds, strategy)\n\n        if strategy == \"loop\":\n            cms: list[pl.DataFrame] = []\n            for t in tqdm(set(thresholds or y_score)):\n                cm = (\n                    self.confusion_matrix(\n                        df[\"y_true\"],\n                        df[\"threshold\"].ge(t),\n                        beta=beta,\n                        sample_weight=df[\"sample_weight\"],\n                    )\n                    .to_polars()\n                    .with_columns(pl.lit(t).alias(\"threshold\"))\n                )\n                cms.append(cm)\n\n            return pl.concat(cms, how=\"vertical\").with_columns(\n                pl.col(\"lower\", \"point\", \"upper\").fill_nan(None)\n            )\n        elif strategy == \"cum_sum\":\n            if thresholds is None:\n                thresholds = df[\"threshold\"].unique()\n\n            if self._params[\"poisson\"]:\n                _matrix_func = _base_confusion_matrix_at_thresholds_sorted\n                _sample_func = functools.partial(_poisson_sample, df_height=df.height)\n                df = df.lazy()\n            else:\n                _matrix_func = _base_confusion_matrix_at_thresholds\n                _sample_func = _multinomial_sample\n\n            def _cm_inner(pf: PolarsFrame) -&gt; pl.LazyFrame:\n                return (\n                    pf.lazy()\n                    .pipe(_matrix_func)\n                    .pipe(_full_confusion_matrix_from_base, beta=beta)\n                    .unique(\"threshold\")\n                    .pipe(_map_to_thresholds, thresholds)\n                    .drop(\"_threshold_actual\")\n                )\n\n            def _cm(i: int) -&gt; pl.LazyFrame:\n                sample_df = _sample_func(df, seed=i)\n\n                return _cm_inner(sample_df)\n\n            cms: list[pl.LazyFrame] = _run_concurrent(\n                _cm,\n                (\n                    (self.seed + i for i in range(self.iterations))\n                    if self.seed is not None\n                    else (None for _ in range(self.iterations))\n                ),\n            )\n\n            def _process_results(lf: pl.LazyFrame) -&gt; pl.LazyFrame:\n                return (\n                    lf.select(\"threshold\", *metrics)\n                    .unpivot(index=\"threshold\")\n                    .rename({\"variable\": \"metric\"})\n                )\n\n            bootstrap_lf = pl.concat(cms, how=\"vertical\").pipe(_process_results)\n\n            lf = bootstrap_lf.group_by(\"threshold\", \"metric\")\n\n            original = (\n                _cm_inner(df)\n                .select(\"threshold\", *metrics)\n                .pipe(_map_to_thresholds, thresholds)\n                .unpivot(index=\"threshold\")\n                .rename({\"variable\": \"metric\", \"value\": \"point\"})\n            )\n\n            if self.method == \"standard\":\n                return (\n                    _standard_interval_polars(lf, self.alpha)\n                    .join(\n                        original,\n                        on=[\"threshold\", \"metric\"],\n                        how=\"left\",\n                        validate=\"1:1\",\n                    )\n                    .select(final_cols)\n                    .collect()\n                )\n            elif self.method == \"percentile\":\n                return (\n                    _percentile_interval_polars(lf, self.alpha)\n                    .join(\n                        original,\n                        on=[\"threshold\", \"metric\"],\n                        how=\"left\",\n                        validate=\"1:1\",\n                    )\n                    .select(final_cols)\n                    .collect()\n                )\n            elif self.method == \"basic\":\n                return (\n                    _percentile_interval_polars(lf, self.alpha)\n                    .join(\n                        original,\n                        on=[\"threshold\", \"metric\"],\n                        how=\"left\",\n                        validate=\"1:1\",\n                    )\n                    .pipe(_basic_interval_polars)\n                    .select(final_cols)\n                    .collect()\n                )\n            elif self.method == \"BCa\":\n                raise NotImplementedError(\n                    \"Method `BCa` not implemented for strategy `cum_sum` due to https://github.com/pola-rs/polars/issues/20951\"\n                )\n                original_lf = (\n                    _cm_inner(df)\n                    .select(\"threshold\", *metrics)\n                    .pipe(_map_to_thresholds, thresholds)\n                    .unpivot(index=\"threshold\")\n                    .rename({\"variable\": \"metric\", \"value\": \"original_value\"})\n                )\n                jacknife_lf = pl.concat(_jacknife(df, _cm_inner), how=\"vertical\").pipe(\n                    _process_results\n                )\n\n                return (\n                    _bca_interval_polars(\n                        original_lf,\n                        bootstrap_lf=bootstrap_lf,\n                        jacknife_lf=jacknife_lf,\n                        alpha=self.alpha,\n                        by=[\"threshold\", \"metric\"],\n                    )\n                    .select(final_cols)\n                    .collect()\n                )\n            else:\n                raise ValueError()\n\n    def roc_auc(\n        self,\n        y_true: ArrayLike,\n        y_score: ArrayLike,\n        sample_weight: Optional[ArrayLike] = None,\n    ) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap ROC-AUC. See [rapidstats.metrics.roc_auc][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n        sample_weight: Optional[ArrayLike], optional\n            Sample weights, set to 1 if None\n\n            !!! Version\n                Added 0.2.0\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, point, upper)\n\n        Changelog\n        ---------\n        - Added in version 0.1.0\n        - Returns point estimate instead of mean starting version 0.3.0\n        \"\"\"\n        df = _y_true_y_score_to_df(y_true, y_score, sample_weight).with_columns(\n            pl.col(\"y_true\").cast(pl.Float64)\n        )\n\n        if self._params[\"poisson\"]:\n            df = df.sort(\"y_score\")\n            _f = _bootstrap_roc_auc_sorted\n        else:\n            _f = _bootstrap_roc_auc\n\n        return _f(df, **self._params)\n\n    def average_precision(\n        self,\n        y_true: ArrayLike,\n        y_score: ArrayLike,\n        sample_weight: Optional[ArrayLike] = None,\n    ) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap average precision. See [rapidstats.metrics.average_precision][] for more\n        details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n        sample_weight: Optional[ArrayLike], optional\n            Sample weights, set to 1 if None\n\n            !!! Version\n                Added 0.2.0\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, point, upper)\n\n        Changelog\n        ----------------------\n        - Added in version 0.1.0\n        - Returns point estimate instead of mean starting version 0.3.0\n        \"\"\"\n        df = (\n            _y_true_y_score_to_df(y_true, y_score, sample_weight)\n            .rename({\"y_score\": \"threshold\"})\n            .drop_nulls()\n        )\n\n        def _cm_inner(pf: PolarsFrame) -&gt; pl.LazyFrame:\n            return (\n                pf.lazy()\n                .pipe(_base_confusion_matrix_at_thresholds)\n                .pipe(_full_confusion_matrix_from_base)\n                .select(\"threshold\", \"precision\", \"tpr\")\n            )\n\n        def _cm(i: int) -&gt; pl.LazyFrame:\n            sample_df = df.sample(fraction=1, with_replacement=True, seed=i)\n\n            return _cm_inner(sample_df)\n\n        cms: list[pl.LazyFrame] = _run_concurrent(\n            _cm,\n            (\n                (self.seed + i for i in range(self.iterations))\n                if self.seed is not None\n                else (None for _ in range(self.iterations))\n            ),\n        )\n\n        cms = [\n            cm.with_columns(pl.lit(i).alias(\"iteration\")) for i, cm in enumerate(cms)\n        ]\n\n        bootstrap_stats = (\n            pl.concat(cms, how=\"vertical\")\n            .sort(\"threshold\")\n            .group_by(\"iteration\", maintain_order=True)\n            .agg(\n                _ap_from_pr_curve(pl.col(\"precision\"), pl.col(\"tpr\")).alias(\n                    \"average_precision\"\n                )\n            )\n            .collect()[\"average_precision\"]\n            .to_list()\n        )\n\n        original_stat = _ap(y_true, y_score)\n\n        if self.method == \"standard\":\n            return _standard_interval(original_stat, bootstrap_stats, self.alpha)\n        elif self.method == \"percentile\":\n            return _percentile_interval(original_stat, bootstrap_stats, self.alpha)\n        elif self.method == \"basic\":\n            return _basic_interval(original_stat, bootstrap_stats, self.alpha)\n        elif self.method == \"BCa\":\n\n            def _cm_jacknife(i):\n                j_df = df.filter(pl.col(\"index\").ne(i))\n\n                return _cm_inner(j_df).with_columns(pl.lit(i).alias(\"iteration\"))\n\n            df = df.with_row_index(\"index\")\n            cms = _run_concurrent(_cm_jacknife, range(df.height))\n            jacknife_stats = (\n                pl.concat(cms, how=\"vertical\")\n                .sort(\"threshold\")\n                .group_by(\"iteration\", maintain_order=True)\n                .agg(\n                    _ap_from_pr_curve(pl.col(\"precision\"), pl.col(\"tpr\")).alias(\n                        \"average_precision\"\n                    )\n                )\n                .collect()[\"average_precision\"]\n                .to_list()\n            )\n\n            return _bca_interval(\n                original_stat, bootstrap_stats, jacknife_stats, self.alpha\n            )\n\n    def max_ks(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap Max-KS. See [rapidstats.metrics.max_ks][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, point, upper)\n\n        Changelog\n        ----------------------\n        - Added in version 0.1.0\n        - Returns point estimate instead of mean starting version 0.3.0\n        \"\"\"\n        df = _y_true_y_score_to_df(y_true, y_score)\n\n        return _bootstrap_max_ks(df, **self._params)\n\n    def brier_loss(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap Brier loss. See [rapidstats.metrics.brier_loss][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, point, upper)\n        \"\"\"\n        df = _y_true_y_score_to_df(y_true, y_score)\n\n        return _bootstrap_brier_loss(df, **self._params)\n\n    def mean(self, y: ArrayLike) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap mean.\n\n        Parameters\n        ----------\n        y : ArrayLike\n            A 1D-array\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, point, upper)\n\n        Added in version 0.1.0\n        ----------------------\n        \"\"\"\n        df = pl.DataFrame({\"y\": y})\n\n        return _bootstrap_mean(df, **self._params)\n\n    def adverse_impact_ratio(\n        self,\n        y_pred: ArrayLike,\n        protected: ArrayLike,\n        control: ArrayLike,\n        sample_weight: Optional[ArrayLike] = None,\n    ) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap AIR. See [rapidstats.metrics.adverse_impact_ratio][] for more details.\n\n        Parameters\n        ----------\n        y_pred : ArrayLike\n            Predicted target\n        protected : ArrayLike\n            An array of booleans identifying the protected class\n        control : ArrayLike\n            An array of booleans identifying the control class\n        sample_weight: Optional[ArrayLike], optional\n            Sample weights, set to 1 if None\n\n            !!! Version\n                Added 0.2.0\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, point, upper)\n\n        Changelog\n        ----------------------\n        - Added in version 0.1.0\n        \"\"\"\n        df = (\n            pl.DataFrame(\n                {\n                    \"y_pred\": y_pred,\n                    \"protected\": protected,\n                    \"control\": control,\n                    \"sample_weight\": 1.0 if sample_weight is None else sample_weight,\n                }\n            )\n            .with_columns(pl.col(\"y_pred\", \"protected\", \"control\").cast(pl.Boolean))\n            .with_columns(pl.col(\"y_pred\").cast(pl.Float64))\n        )\n\n        return _bootstrap_adverse_impact_ratio(df, **self._params)\n\n    def adverse_impact_ratio_at_thresholds(\n        self,\n        y_score: ArrayLike,\n        protected: ArrayLike,\n        control: ArrayLike,\n        sample_weight: Optional[ArrayLike] = None,\n        thresholds: Optional[list[float]] = None,\n        strategy: LoopStrategy = \"auto\",\n    ) -&gt; pl.DataFrame:\n        \"\"\"Bootstrap AIR at thresholds. See\n        [rapidstats.metrics.adverse_impact_ratio_at_thresholds][] for more details.\n\n        Parameters\n        ----------\n        y_score : ArrayLike\n            Predicted scores\n        protected : ArrayLike\n            An array of booleans identifying the protected class\n        control : ArrayLike\n            An array of booleans identifying the control class\n        sample_weight: Optional[ArrayLike], optional\n            Sample weights, set to 1 if None\n\n            !!! Version\n                Added 0.2.0\n        thresholds : Optional[list[float]], optional\n            The thresholds to compute `is_predicted_negative` at, i.e. y_score &lt; t.\n            If None, uses every score present in `y_score`, by default None\n        strategy : LoopStrategy, optional\n            Computation method, by default \"auto\"\n\n        Returns\n        -------\n        pl.DataFrame\n            A DataFrame of `threshold`, `lower`, `mean`, and `upper`\n\n        Raises\n        ------\n        NotImplementedError\n            When `strategy` is `cum_sum` and `method` is `BCa`\n        \"\"\"\n        has_sample_weight = sample_weight is not None\n        df = pl.DataFrame(\n            {\"y_score\": y_score, \"protected\": protected, \"control\": control}\n        ).with_columns(\n            pl.col(\"protected\", \"control\").cast(pl.Boolean),\n            pl.col(\"y_score\").cast(pl.Float64),\n        )\n\n        if has_sample_weight:\n            df = df.with_columns(\n                pl.Series(\"sample_weight\", sample_weight).cast(pl.Float64)\n            )\n\n        strategy = _set_loop_strategy(thresholds, strategy)\n\n        if strategy == \"loop\":\n            airs: list[dict[str, float]] = []\n            for t in tqdm(set(thresholds or y_score)):\n                lower, point, upper = self.adverse_impact_ratio(\n                    df[\"y_score\"].lt(t),\n                    df[\"protected\"],\n                    df[\"control\"],\n                    sample_weight=sample_weight,\n                )\n                airs.append(\n                    {\"threshold\": t, \"lower\": lower, \"point\": point, \"upper\": upper}\n                )\n\n            return pl.DataFrame(airs).fill_nan(None).pipe(_fill_infinite, None)\n\n        elif strategy == \"cum_sum\":\n            if thresholds is None:\n                thresholds = df[\"y_score\"]\n\n            if self._params[\"poisson\"]:\n                _air_func = _air_at_thresholds_core_sorted\n                _sample_func = functools.partial(_poisson_sample, df_height=df.height)\n                df = df.lazy()\n            else:\n                _air_func = _air_at_thresholds_core\n                _sample_func = _multinomial_sample\n\n            def _air(i: int) -&gt; pl.LazyFrame:\n                sample_df = _sample_func(df, seed=i)\n\n                return _air_func(sample_df, thresholds, has_sample_weight)\n\n            airs: list[pl.LazyFrame] = _run_concurrent(\n                _air,\n                (\n                    (self.seed + i for i in range(self.iterations))\n                    if self.seed is not None\n                    else (None for _ in range(self.iterations))\n                ),\n            )\n            bootstrap_lf = (\n                pl.concat(airs, how=\"vertical\")\n                .rename({\"air\": \"value\"})\n                .with_columns(\n                    _expr_fill_infinite(pl.col(\"value\").fill_nan(None)).alias(\"value\")\n                )\n            )\n\n            lf = bootstrap_lf.group_by(\"threshold\")\n\n            final_cols = [\"threshold\", \"lower\", \"point\", \"upper\"]\n\n            original = (\n                _air_at_thresholds_core(df, thresholds, has_sample_weight)\n                .rename({\"air\": \"point\"})\n                .unique(\"threshold\")\n            )\n\n            if self.method == \"standard\":\n                return (\n                    _standard_interval_polars(lf, self.alpha)\n                    .join(original, on=\"threshold\", how=\"left\", validate=\"1:1\")\n                    .select(final_cols)\n                    .collect()\n                )\n            elif self.method == \"percentile\":\n                return (\n                    _percentile_interval_polars(lf, self.alpha)\n                    .join(original, on=\"threshold\", how=\"left\", validate=\"1:1\")\n                    .select(final_cols)\n                    .collect()\n                )\n            elif self.method == \"basic\":\n                return (\n                    _percentile_interval_polars(lf, self.alpha)\n                    .join(original, on=\"threshold\", how=\"left\", validate=\"1:1\")\n                    .pipe(_basic_interval_polars)\n                    .select(final_cols)\n                    .collect()\n                )\n            elif self.method == \"BCa\":\n                raise NotImplementedError(\n                    \"Method `BCa` not implemented for strategy `cum_sum` due to https://github.com/pola-rs/polars/issues/20951\"\n                )\n                original_lf = (\n                    _air_at_thresholds_core(df, thresholds, has_sample_weight)\n                    .rename({\"air\": \"original_value\"})\n                    .unique(\"threshold\")\n                )\n\n                tmp = functools.partial(\n                    _air_at_thresholds_core,\n                    thresholds=thresholds,\n                    has_sample_weight=has_sample_weight,\n                )\n                jacknife_lf = (\n                    pl.concat(_jacknife(df, tmp), how=\"vertical\")\n                    .rename({\"air\": \"value\"})\n                    .unique(\"threshold\")\n                )\n\n                return (\n                    _bca_interval_polars(\n                        original_lf,\n                        bootstrap_lf=bootstrap_lf.rename({\"air\": \"value\"}),\n                        jacknife_lf=jacknife_lf,\n                        alpha=self.alpha,\n                        by=[\"threshold\"],\n                    )\n                    .select(final_cols)\n                    .collect()\n                )\n\n    def mean_squared_error(\n        self, y_true: ArrayLike, y_score: ArrayLike\n    ) -&gt; ConfidenceInterval:\n        r\"\"\"Bootstrap MSE. See [rapidstats.metrics.mean_squared_error][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, point, upper)\n\n        Added in version 0.1.0\n        ----------------------\n        \"\"\"\n        return _bootstrap_mean_squared_error(\n            _regression_to_df(y_true, y_score), **self._params\n        )\n\n    def root_mean_squared_error(\n        self, y_true: ArrayLike, y_score: ArrayLike\n    ) -&gt; ConfidenceInterval:\n        r\"\"\"Bootstrap RMSE. See [rapidstats.metrics.root_mean_squared_error][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, point, upper)\n\n        Added in version 0.1.0\n        ----------------------\n        \"\"\"\n        return _bootstrap_root_mean_squared_error(\n            _regression_to_df(y_true, y_score), **self._params\n        )\n\n    def r2(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n        \"\"\"Bootstrap R2. See [rapidstats.metrics.r2][] for more details.\n\n        Parameters\n        ----------\n        y_true : ArrayLike\n            Ground truth target\n        y_score : ArrayLike\n            Predicted scores\n\n        Returns\n        -------\n        ConfidenceInterval\n            A tuple of (lower, point, upper)\n\n        Added in version 0.1.0\n        ----------------------\n        \"\"\"\n        return _bootstrap_r2(_regression_to_df(y_true, y_score), **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.adverse_impact_ratio","title":"<code>adverse_impact_ratio(y_pred, protected, control, sample_weight=None)</code>","text":"<p>Bootstrap AIR. See rapidstats.metrics.adverse_impact_ratio for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>ArrayLike</code> <p>Predicted target</p> required <code>protected</code> <code>ArrayLike</code> <p>An array of booleans identifying the protected class</p> required <code>control</code> <code>ArrayLike</code> <p>An array of booleans identifying the control class</p> required <code>sample_weight</code> <code>Optional[ArrayLike]</code> <p>Sample weights, set to 1 if None</p> <p>Version</p> <p>Added 0.2.0</p> <code>None</code> <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, point, upper)</p> Changelog <ul> <li>Added in version 0.1.0</li> </ul> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def adverse_impact_ratio(\n    self,\n    y_pred: ArrayLike,\n    protected: ArrayLike,\n    control: ArrayLike,\n    sample_weight: Optional[ArrayLike] = None,\n) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap AIR. See [rapidstats.metrics.adverse_impact_ratio][] for more details.\n\n    Parameters\n    ----------\n    y_pred : ArrayLike\n        Predicted target\n    protected : ArrayLike\n        An array of booleans identifying the protected class\n    control : ArrayLike\n        An array of booleans identifying the control class\n    sample_weight: Optional[ArrayLike], optional\n        Sample weights, set to 1 if None\n\n        !!! Version\n            Added 0.2.0\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, point, upper)\n\n    Changelog\n    ----------------------\n    - Added in version 0.1.0\n    \"\"\"\n    df = (\n        pl.DataFrame(\n            {\n                \"y_pred\": y_pred,\n                \"protected\": protected,\n                \"control\": control,\n                \"sample_weight\": 1.0 if sample_weight is None else sample_weight,\n            }\n        )\n        .with_columns(pl.col(\"y_pred\", \"protected\", \"control\").cast(pl.Boolean))\n        .with_columns(pl.col(\"y_pred\").cast(pl.Float64))\n    )\n\n    return _bootstrap_adverse_impact_ratio(df, **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.adverse_impact_ratio_at_thresholds","title":"<code>adverse_impact_ratio_at_thresholds(y_score, protected, control, sample_weight=None, thresholds=None, strategy='auto')</code>","text":"<p>Bootstrap AIR at thresholds. See rapidstats.metrics.adverse_impact_ratio_at_thresholds for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <code>protected</code> <code>ArrayLike</code> <p>An array of booleans identifying the protected class</p> required <code>control</code> <code>ArrayLike</code> <p>An array of booleans identifying the control class</p> required <code>sample_weight</code> <code>Optional[ArrayLike]</code> <p>Sample weights, set to 1 if None</p> <p>Version</p> <p>Added 0.2.0</p> <code>None</code> <code>thresholds</code> <code>Optional[list[float]]</code> <p>The thresholds to compute <code>is_predicted_negative</code> at, i.e. y_score &lt; t. If None, uses every score present in <code>y_score</code>, by default None</p> <code>None</code> <code>strategy</code> <code>LoopStrategy</code> <p>Computation method, by default \"auto\"</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame of <code>threshold</code>, <code>lower</code>, <code>mean</code>, and <code>upper</code></p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>When <code>strategy</code> is <code>cum_sum</code> and <code>method</code> is <code>BCa</code></p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def adverse_impact_ratio_at_thresholds(\n    self,\n    y_score: ArrayLike,\n    protected: ArrayLike,\n    control: ArrayLike,\n    sample_weight: Optional[ArrayLike] = None,\n    thresholds: Optional[list[float]] = None,\n    strategy: LoopStrategy = \"auto\",\n) -&gt; pl.DataFrame:\n    \"\"\"Bootstrap AIR at thresholds. See\n    [rapidstats.metrics.adverse_impact_ratio_at_thresholds][] for more details.\n\n    Parameters\n    ----------\n    y_score : ArrayLike\n        Predicted scores\n    protected : ArrayLike\n        An array of booleans identifying the protected class\n    control : ArrayLike\n        An array of booleans identifying the control class\n    sample_weight: Optional[ArrayLike], optional\n        Sample weights, set to 1 if None\n\n        !!! Version\n            Added 0.2.0\n    thresholds : Optional[list[float]], optional\n        The thresholds to compute `is_predicted_negative` at, i.e. y_score &lt; t.\n        If None, uses every score present in `y_score`, by default None\n    strategy : LoopStrategy, optional\n        Computation method, by default \"auto\"\n\n    Returns\n    -------\n    pl.DataFrame\n        A DataFrame of `threshold`, `lower`, `mean`, and `upper`\n\n    Raises\n    ------\n    NotImplementedError\n        When `strategy` is `cum_sum` and `method` is `BCa`\n    \"\"\"\n    has_sample_weight = sample_weight is not None\n    df = pl.DataFrame(\n        {\"y_score\": y_score, \"protected\": protected, \"control\": control}\n    ).with_columns(\n        pl.col(\"protected\", \"control\").cast(pl.Boolean),\n        pl.col(\"y_score\").cast(pl.Float64),\n    )\n\n    if has_sample_weight:\n        df = df.with_columns(\n            pl.Series(\"sample_weight\", sample_weight).cast(pl.Float64)\n        )\n\n    strategy = _set_loop_strategy(thresholds, strategy)\n\n    if strategy == \"loop\":\n        airs: list[dict[str, float]] = []\n        for t in tqdm(set(thresholds or y_score)):\n            lower, point, upper = self.adverse_impact_ratio(\n                df[\"y_score\"].lt(t),\n                df[\"protected\"],\n                df[\"control\"],\n                sample_weight=sample_weight,\n            )\n            airs.append(\n                {\"threshold\": t, \"lower\": lower, \"point\": point, \"upper\": upper}\n            )\n\n        return pl.DataFrame(airs).fill_nan(None).pipe(_fill_infinite, None)\n\n    elif strategy == \"cum_sum\":\n        if thresholds is None:\n            thresholds = df[\"y_score\"]\n\n        if self._params[\"poisson\"]:\n            _air_func = _air_at_thresholds_core_sorted\n            _sample_func = functools.partial(_poisson_sample, df_height=df.height)\n            df = df.lazy()\n        else:\n            _air_func = _air_at_thresholds_core\n            _sample_func = _multinomial_sample\n\n        def _air(i: int) -&gt; pl.LazyFrame:\n            sample_df = _sample_func(df, seed=i)\n\n            return _air_func(sample_df, thresholds, has_sample_weight)\n\n        airs: list[pl.LazyFrame] = _run_concurrent(\n            _air,\n            (\n                (self.seed + i for i in range(self.iterations))\n                if self.seed is not None\n                else (None for _ in range(self.iterations))\n            ),\n        )\n        bootstrap_lf = (\n            pl.concat(airs, how=\"vertical\")\n            .rename({\"air\": \"value\"})\n            .with_columns(\n                _expr_fill_infinite(pl.col(\"value\").fill_nan(None)).alias(\"value\")\n            )\n        )\n\n        lf = bootstrap_lf.group_by(\"threshold\")\n\n        final_cols = [\"threshold\", \"lower\", \"point\", \"upper\"]\n\n        original = (\n            _air_at_thresholds_core(df, thresholds, has_sample_weight)\n            .rename({\"air\": \"point\"})\n            .unique(\"threshold\")\n        )\n\n        if self.method == \"standard\":\n            return (\n                _standard_interval_polars(lf, self.alpha)\n                .join(original, on=\"threshold\", how=\"left\", validate=\"1:1\")\n                .select(final_cols)\n                .collect()\n            )\n        elif self.method == \"percentile\":\n            return (\n                _percentile_interval_polars(lf, self.alpha)\n                .join(original, on=\"threshold\", how=\"left\", validate=\"1:1\")\n                .select(final_cols)\n                .collect()\n            )\n        elif self.method == \"basic\":\n            return (\n                _percentile_interval_polars(lf, self.alpha)\n                .join(original, on=\"threshold\", how=\"left\", validate=\"1:1\")\n                .pipe(_basic_interval_polars)\n                .select(final_cols)\n                .collect()\n            )\n        elif self.method == \"BCa\":\n            raise NotImplementedError(\n                \"Method `BCa` not implemented for strategy `cum_sum` due to https://github.com/pola-rs/polars/issues/20951\"\n            )\n            original_lf = (\n                _air_at_thresholds_core(df, thresholds, has_sample_weight)\n                .rename({\"air\": \"original_value\"})\n                .unique(\"threshold\")\n            )\n\n            tmp = functools.partial(\n                _air_at_thresholds_core,\n                thresholds=thresholds,\n                has_sample_weight=has_sample_weight,\n            )\n            jacknife_lf = (\n                pl.concat(_jacknife(df, tmp), how=\"vertical\")\n                .rename({\"air\": \"value\"})\n                .unique(\"threshold\")\n            )\n\n            return (\n                _bca_interval_polars(\n                    original_lf,\n                    bootstrap_lf=bootstrap_lf.rename({\"air\": \"value\"}),\n                    jacknife_lf=jacknife_lf,\n                    alpha=self.alpha,\n                    by=[\"threshold\"],\n                )\n                .select(final_cols)\n                .collect()\n            )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.average_precision","title":"<code>average_precision(y_true, y_score, sample_weight=None)</code>","text":"<p>Bootstrap average precision. See rapidstats.metrics.average_precision for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <code>sample_weight</code> <code>Optional[ArrayLike]</code> <p>Sample weights, set to 1 if None</p> <p>Version</p> <p>Added 0.2.0</p> <code>None</code> <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, point, upper)</p> Changelog <ul> <li>Added in version 0.1.0</li> <li>Returns point estimate instead of mean starting version 0.3.0</li> </ul> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def average_precision(\n    self,\n    y_true: ArrayLike,\n    y_score: ArrayLike,\n    sample_weight: Optional[ArrayLike] = None,\n) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap average precision. See [rapidstats.metrics.average_precision][] for more\n    details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n    sample_weight: Optional[ArrayLike], optional\n        Sample weights, set to 1 if None\n\n        !!! Version\n            Added 0.2.0\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, point, upper)\n\n    Changelog\n    ----------------------\n    - Added in version 0.1.0\n    - Returns point estimate instead of mean starting version 0.3.0\n    \"\"\"\n    df = (\n        _y_true_y_score_to_df(y_true, y_score, sample_weight)\n        .rename({\"y_score\": \"threshold\"})\n        .drop_nulls()\n    )\n\n    def _cm_inner(pf: PolarsFrame) -&gt; pl.LazyFrame:\n        return (\n            pf.lazy()\n            .pipe(_base_confusion_matrix_at_thresholds)\n            .pipe(_full_confusion_matrix_from_base)\n            .select(\"threshold\", \"precision\", \"tpr\")\n        )\n\n    def _cm(i: int) -&gt; pl.LazyFrame:\n        sample_df = df.sample(fraction=1, with_replacement=True, seed=i)\n\n        return _cm_inner(sample_df)\n\n    cms: list[pl.LazyFrame] = _run_concurrent(\n        _cm,\n        (\n            (self.seed + i for i in range(self.iterations))\n            if self.seed is not None\n            else (None for _ in range(self.iterations))\n        ),\n    )\n\n    cms = [\n        cm.with_columns(pl.lit(i).alias(\"iteration\")) for i, cm in enumerate(cms)\n    ]\n\n    bootstrap_stats = (\n        pl.concat(cms, how=\"vertical\")\n        .sort(\"threshold\")\n        .group_by(\"iteration\", maintain_order=True)\n        .agg(\n            _ap_from_pr_curve(pl.col(\"precision\"), pl.col(\"tpr\")).alias(\n                \"average_precision\"\n            )\n        )\n        .collect()[\"average_precision\"]\n        .to_list()\n    )\n\n    original_stat = _ap(y_true, y_score)\n\n    if self.method == \"standard\":\n        return _standard_interval(original_stat, bootstrap_stats, self.alpha)\n    elif self.method == \"percentile\":\n        return _percentile_interval(original_stat, bootstrap_stats, self.alpha)\n    elif self.method == \"basic\":\n        return _basic_interval(original_stat, bootstrap_stats, self.alpha)\n    elif self.method == \"BCa\":\n\n        def _cm_jacknife(i):\n            j_df = df.filter(pl.col(\"index\").ne(i))\n\n            return _cm_inner(j_df).with_columns(pl.lit(i).alias(\"iteration\"))\n\n        df = df.with_row_index(\"index\")\n        cms = _run_concurrent(_cm_jacknife, range(df.height))\n        jacknife_stats = (\n            pl.concat(cms, how=\"vertical\")\n            .sort(\"threshold\")\n            .group_by(\"iteration\", maintain_order=True)\n            .agg(\n                _ap_from_pr_curve(pl.col(\"precision\"), pl.col(\"tpr\")).alias(\n                    \"average_precision\"\n                )\n            )\n            .collect()[\"average_precision\"]\n            .to_list()\n        )\n\n        return _bca_interval(\n            original_stat, bootstrap_stats, jacknife_stats, self.alpha\n        )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.brier_loss","title":"<code>brier_loss(y_true, y_score)</code>","text":"<p>Bootstrap Brier loss. See rapidstats.metrics.brier_loss for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, point, upper)</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def brier_loss(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap Brier loss. See [rapidstats.metrics.brier_loss][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, point, upper)\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score)\n\n    return _bootstrap_brier_loss(df, **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.confusion_matrix","title":"<code>confusion_matrix(y_true, y_pred, beta=1.0, sample_weight=None)</code>","text":"<p>Bootstrap confusion matrix. See rapidstats.metrics.confusion_matrix for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_pred</code> <code>ArrayLike</code> <p>Predicted target</p> required <code>beta</code> <code>float</code> <p>\\( \\beta \\) to use in \\( F_\\beta \\), by default 1</p> <code>1.0</code> <code>sample_weight</code> <code>Optional[ArrayLike]</code> <p>Sample weights, set to 1 if None</p> <p>Version</p> <p>Added 0.2.0</p> <code>None</code> <p>Returns:</p> Type Description <code>BootstrappedConfusionMatrix</code> <p>A dataclass of confusion matrix metrics as (lower, point, upper). See rapidstats._bootstrap.BootstrappedConfusionMatrix for more details.</p> Added in version 0.1.0 Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def confusion_matrix(\n    self,\n    y_true: ArrayLike,\n    y_pred: ArrayLike,\n    beta: float = 1.0,\n    sample_weight: Optional[ArrayLike] = None,\n) -&gt; BootstrappedConfusionMatrix:\n    r\"\"\"Bootstrap confusion matrix. See [rapidstats.metrics.confusion_matrix][] for\n    more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_pred : ArrayLike\n        Predicted target\n    beta : float, optional\n        \\( \\beta \\) to use in \\( F_\\beta \\), by default 1\n    sample_weight: Optional[ArrayLike], optional\n        Sample weights, set to 1 if None\n\n        !!! Version\n            Added 0.2.0\n\n    Returns\n    -------\n    BootstrappedConfusionMatrix\n        A dataclass of confusion matrix metrics as (lower, point, upper). See\n        [rapidstats._bootstrap.BootstrappedConfusionMatrix][] for more details.\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    df = _y_true_y_pred_to_df(y_true, y_pred, sample_weight).with_columns(\n        pl.col(\"y_true\").cast(pl.UInt8)\n    )\n\n    return BootstrappedConfusionMatrix(\n        *_bootstrap_confusion_matrix(df, beta, **self._params)\n    )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.confusion_matrix_at_thresholds","title":"<code>confusion_matrix_at_thresholds(y_true, y_score, thresholds=None, metrics=DefaultConfusionMatrixMetrics, strategy='auto', beta=1.0, sample_weight=None)</code>","text":"<p>Bootstrap confusion matrix at thresholds. See rapidstats.metrics.confusion_matrix_at_thresholds for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <code>thresholds</code> <code>Optional[list[float]]</code> <p>The thresholds to compute <code>y_pred</code> at, i.e. y_score &gt;= t. If None, uses every score present in <code>y_score</code>, by default None</p> <code>None</code> <code>metrics</code> <code>Iterable[ConfusionMatrixMetric]</code> <p>The metrics to compute, by default DefaultConfusionMatrixMetrics</p> <code>DefaultConfusionMatrixMetrics</code> <code>strategy</code> <code>LoopStrategy</code> <p>Computation method, by default \"auto\"</p> <code>'auto'</code> <code>beta</code> <code>float</code> <p>\\( \\beta \\) to use in \\( F_\\beta \\), by default 1</p> <code>1.0</code> <code>sample_weight</code> <code>Optional[ArrayLike]</code> <p>Sample weights, set to 1 if None</p> <p>Version</p> <p>Added 0.2.0</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame of <code>threshold</code>, <code>metric</code>, <code>lower</code>, <code>mean</code>, and <code>upper</code></p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>When <code>strategy</code> is <code>cum_sum</code> and <code>method</code> is <code>BCa</code></p> Added in version 0.1.0 Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def confusion_matrix_at_thresholds(\n    self,\n    y_true: ArrayLike,\n    y_score: ArrayLike,\n    thresholds: Optional[list[float]] = None,\n    metrics: Iterable[ConfusionMatrixMetric] = DefaultConfusionMatrixMetrics,\n    strategy: LoopStrategy = \"auto\",\n    beta: float = 1.0,\n    sample_weight: Optional[ArrayLike] = None,\n) -&gt; pl.DataFrame:\n    r\"\"\"Bootstrap confusion matrix at thresholds. See\n    [rapidstats.metrics.confusion_matrix_at_thresholds][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n    thresholds : Optional[list[float]], optional\n        The thresholds to compute `y_pred` at, i.e. y_score &gt;= t. If None,\n        uses every score present in `y_score`, by default None\n    metrics : Iterable[ConfusionMatrixMetric], optional\n        The metrics to compute, by default DefaultConfusionMatrixMetrics\n    strategy : LoopStrategy, optional\n        Computation method, by default \"auto\"\n    beta : float, optional\n        \\( \\beta \\) to use in \\( F_\\beta \\), by default 1\n    sample_weight: Optional[ArrayLike], optional\n        Sample weights, set to 1 if None\n\n        !!! Version\n            Added 0.2.0\n\n    Returns\n    -------\n    pl.DataFrame\n        A DataFrame of `threshold`, `metric`, `lower`, `mean`, and `upper`\n\n    Raises\n    ------\n    NotImplementedError\n        When `strategy` is `cum_sum` and `method` is `BCa`\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    df = (\n        _y_true_y_score_to_df(y_true, y_score, sample_weight)\n        .rename({\"y_score\": \"threshold\"})\n        .sort(\"threshold\", descending=True)\n    )\n    final_cols = [\"threshold\", \"metric\", \"lower\", \"point\", \"upper\"]\n\n    strategy = _set_loop_strategy(thresholds, strategy)\n\n    if strategy == \"loop\":\n        cms: list[pl.DataFrame] = []\n        for t in tqdm(set(thresholds or y_score)):\n            cm = (\n                self.confusion_matrix(\n                    df[\"y_true\"],\n                    df[\"threshold\"].ge(t),\n                    beta=beta,\n                    sample_weight=df[\"sample_weight\"],\n                )\n                .to_polars()\n                .with_columns(pl.lit(t).alias(\"threshold\"))\n            )\n            cms.append(cm)\n\n        return pl.concat(cms, how=\"vertical\").with_columns(\n            pl.col(\"lower\", \"point\", \"upper\").fill_nan(None)\n        )\n    elif strategy == \"cum_sum\":\n        if thresholds is None:\n            thresholds = df[\"threshold\"].unique()\n\n        if self._params[\"poisson\"]:\n            _matrix_func = _base_confusion_matrix_at_thresholds_sorted\n            _sample_func = functools.partial(_poisson_sample, df_height=df.height)\n            df = df.lazy()\n        else:\n            _matrix_func = _base_confusion_matrix_at_thresholds\n            _sample_func = _multinomial_sample\n\n        def _cm_inner(pf: PolarsFrame) -&gt; pl.LazyFrame:\n            return (\n                pf.lazy()\n                .pipe(_matrix_func)\n                .pipe(_full_confusion_matrix_from_base, beta=beta)\n                .unique(\"threshold\")\n                .pipe(_map_to_thresholds, thresholds)\n                .drop(\"_threshold_actual\")\n            )\n\n        def _cm(i: int) -&gt; pl.LazyFrame:\n            sample_df = _sample_func(df, seed=i)\n\n            return _cm_inner(sample_df)\n\n        cms: list[pl.LazyFrame] = _run_concurrent(\n            _cm,\n            (\n                (self.seed + i for i in range(self.iterations))\n                if self.seed is not None\n                else (None for _ in range(self.iterations))\n            ),\n        )\n\n        def _process_results(lf: pl.LazyFrame) -&gt; pl.LazyFrame:\n            return (\n                lf.select(\"threshold\", *metrics)\n                .unpivot(index=\"threshold\")\n                .rename({\"variable\": \"metric\"})\n            )\n\n        bootstrap_lf = pl.concat(cms, how=\"vertical\").pipe(_process_results)\n\n        lf = bootstrap_lf.group_by(\"threshold\", \"metric\")\n\n        original = (\n            _cm_inner(df)\n            .select(\"threshold\", *metrics)\n            .pipe(_map_to_thresholds, thresholds)\n            .unpivot(index=\"threshold\")\n            .rename({\"variable\": \"metric\", \"value\": \"point\"})\n        )\n\n        if self.method == \"standard\":\n            return (\n                _standard_interval_polars(lf, self.alpha)\n                .join(\n                    original,\n                    on=[\"threshold\", \"metric\"],\n                    how=\"left\",\n                    validate=\"1:1\",\n                )\n                .select(final_cols)\n                .collect()\n            )\n        elif self.method == \"percentile\":\n            return (\n                _percentile_interval_polars(lf, self.alpha)\n                .join(\n                    original,\n                    on=[\"threshold\", \"metric\"],\n                    how=\"left\",\n                    validate=\"1:1\",\n                )\n                .select(final_cols)\n                .collect()\n            )\n        elif self.method == \"basic\":\n            return (\n                _percentile_interval_polars(lf, self.alpha)\n                .join(\n                    original,\n                    on=[\"threshold\", \"metric\"],\n                    how=\"left\",\n                    validate=\"1:1\",\n                )\n                .pipe(_basic_interval_polars)\n                .select(final_cols)\n                .collect()\n            )\n        elif self.method == \"BCa\":\n            raise NotImplementedError(\n                \"Method `BCa` not implemented for strategy `cum_sum` due to https://github.com/pola-rs/polars/issues/20951\"\n            )\n            original_lf = (\n                _cm_inner(df)\n                .select(\"threshold\", *metrics)\n                .pipe(_map_to_thresholds, thresholds)\n                .unpivot(index=\"threshold\")\n                .rename({\"variable\": \"metric\", \"value\": \"original_value\"})\n            )\n            jacknife_lf = pl.concat(_jacknife(df, _cm_inner), how=\"vertical\").pipe(\n                _process_results\n            )\n\n            return (\n                _bca_interval_polars(\n                    original_lf,\n                    bootstrap_lf=bootstrap_lf,\n                    jacknife_lf=jacknife_lf,\n                    alpha=self.alpha,\n                    by=[\"threshold\", \"metric\"],\n                )\n                .select(final_cols)\n                .collect()\n            )\n        else:\n            raise ValueError()\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.max_ks","title":"<code>max_ks(y_true, y_score)</code>","text":"<p>Bootstrap Max-KS. See rapidstats.metrics.max_ks for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, point, upper)</p> Changelog <ul> <li>Added in version 0.1.0</li> <li>Returns point estimate instead of mean starting version 0.3.0</li> </ul> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def max_ks(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap Max-KS. See [rapidstats.metrics.max_ks][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, point, upper)\n\n    Changelog\n    ----------------------\n    - Added in version 0.1.0\n    - Returns point estimate instead of mean starting version 0.3.0\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score)\n\n    return _bootstrap_max_ks(df, **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.mean","title":"<code>mean(y)</code>","text":"<p>Bootstrap mean.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ArrayLike</code> <p>A 1D-array</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, point, upper)</p> Added in version 0.1.0 Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def mean(self, y: ArrayLike) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap mean.\n\n    Parameters\n    ----------\n    y : ArrayLike\n        A 1D-array\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, point, upper)\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    df = pl.DataFrame({\"y\": y})\n\n    return _bootstrap_mean(df, **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.mean_squared_error","title":"<code>mean_squared_error(y_true, y_score)</code>","text":"<p>Bootstrap MSE. See rapidstats.metrics.mean_squared_error for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, point, upper)</p> Added in version 0.1.0 Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def mean_squared_error(\n    self, y_true: ArrayLike, y_score: ArrayLike\n) -&gt; ConfidenceInterval:\n    r\"\"\"Bootstrap MSE. See [rapidstats.metrics.mean_squared_error][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, point, upper)\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    return _bootstrap_mean_squared_error(\n        _regression_to_df(y_true, y_score), **self._params\n    )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.r2","title":"<code>r2(y_true, y_score)</code>","text":"<p>Bootstrap R2. See rapidstats.metrics.r2 for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, point, upper)</p> Added in version 0.1.0 Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def r2(self, y_true: ArrayLike, y_score: ArrayLike) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap R2. See [rapidstats.metrics.r2][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, point, upper)\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    return _bootstrap_r2(_regression_to_df(y_true, y_score), **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.roc_auc","title":"<code>roc_auc(y_true, y_score, sample_weight=None)</code>","text":"<p>Bootstrap ROC-AUC. See rapidstats.metrics.roc_auc for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <code>sample_weight</code> <code>Optional[ArrayLike]</code> <p>Sample weights, set to 1 if None</p> <p>Version</p> <p>Added 0.2.0</p> <code>None</code> <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, point, upper)</p> Changelog <ul> <li>Added in version 0.1.0</li> <li>Returns point estimate instead of mean starting version 0.3.0</li> </ul> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def roc_auc(\n    self,\n    y_true: ArrayLike,\n    y_score: ArrayLike,\n    sample_weight: Optional[ArrayLike] = None,\n) -&gt; ConfidenceInterval:\n    \"\"\"Bootstrap ROC-AUC. See [rapidstats.metrics.roc_auc][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n    sample_weight: Optional[ArrayLike], optional\n        Sample weights, set to 1 if None\n\n        !!! Version\n            Added 0.2.0\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, point, upper)\n\n    Changelog\n    ---------\n    - Added in version 0.1.0\n    - Returns point estimate instead of mean starting version 0.3.0\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score, sample_weight).with_columns(\n        pl.col(\"y_true\").cast(pl.Float64)\n    )\n\n    if self._params[\"poisson\"]:\n        df = df.sort(\"y_score\")\n        _f = _bootstrap_roc_auc_sorted\n    else:\n        _f = _bootstrap_roc_auc\n\n    return _f(df, **self._params)\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.root_mean_squared_error","title":"<code>root_mean_squared_error(y_true, y_score)</code>","text":"<p>Bootstrap RMSE. See rapidstats.metrics.root_mean_squared_error for more details.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, point, upper)</p> Added in version 0.1.0 Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def root_mean_squared_error(\n    self, y_true: ArrayLike, y_score: ArrayLike\n) -&gt; ConfidenceInterval:\n    r\"\"\"Bootstrap RMSE. See [rapidstats.metrics.root_mean_squared_error][] for more details.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, point, upper)\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    return _bootstrap_root_mean_squared_error(\n        _regression_to_df(y_true, y_score), **self._params\n    )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.Bootstrap.run","title":"<code>run(df, stat_func, **kwargs)</code>","text":"<p>Run bootstrap for an arbitrary function that accepts a Polars DataFrame and returns a scalar real number.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The data to pass to <code>stat_func</code></p> required <code>stat_func</code> <code>StatFunc</code> <p>A callable that takes a Polars DataFrame as its first argument and returns a scalar real number.</p> required <p>Returns:</p> Type Description <code>ConfidenceInterval</code> <p>A tuple of (lower, point, upper)</p> Added in version 0.1.0 Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def run(\n    self, df: pl.DataFrame, stat_func: StatFunc, **kwargs\n) -&gt; ConfidenceInterval:\n    \"\"\"Run bootstrap for an arbitrary function that accepts a Polars DataFrame and\n    returns a scalar real number.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        The data to pass to `stat_func`\n    stat_func : StatFunc\n        A callable that takes a Polars DataFrame as its first argument and returns\n        a scalar real number.\n\n    Returns\n    -------\n    ConfidenceInterval\n        A tuple of (lower, point, upper)\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    default = {\"executor\": \"threads\", \"preserve_order\": False}\n    for k, v in default.items():\n        if k not in kwargs:\n            kwargs[k] = v\n\n    if self._params[\"poisson\"]:\n        func = functools.partial(\n            _poisson_bs_func, df=df, df_height=df.height, stat_func=stat_func\n        )\n    else:\n        func = functools.partial(_bs_func, df=df, stat_func=stat_func)\n\n    if self.seed is None:\n        iterable = (None for _ in range(self.iterations))\n    else:\n        iterable = (self.seed + i for i in range(self.iterations))\n\n    bootstrap_stats = [\n        x for x in _run_concurrent(func, iterable, **kwargs) if not math.isnan(x)\n    ]\n\n    original_stat = stat_func(df)\n\n    if len(bootstrap_stats) == 0:\n        return (math.nan, math.nan, math.nan)\n\n    if self.method == \"standard\":\n        return _standard_interval(original_stat, bootstrap_stats, self.alpha)\n    elif self.method == \"percentile\":\n        return _percentile_interval(original_stat, bootstrap_stats, self.alpha)\n    elif self.method == \"basic\":\n        return _basic_interval(original_stat, bootstrap_stats, self.alpha)\n    elif self.method == \"BCa\":\n        jacknife_stats = [x for x in _jacknife(df, stat_func) if not math.isnan(x)]\n\n        return _bca_interval(\n            original_stat, bootstrap_stats, jacknife_stats, self.alpha\n        )\n    else:\n        # We shouldn't hit this since we check method in __init__, but it makes the\n        # type-checker happy\n        raise ValueError(\"Invalid method\")\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.BootstrappedConfusionMatrix","title":"<code>BootstrappedConfusionMatrix</code>  <code>dataclass</code>","text":"<p>Result object returned by <code>rapidstats.Bootstrap().confusion_matrix</code>.</p> <p>See rapidstats.metrics.ConfusionMatrix for a detailed breakdown of the attributes stored in this class. However, instead of storing the statistic, it stores the bootstrapped confidence interval as (lower, point, upper).</p> <p>Methods:</p> Name Description <code>to_polars</code> <p>Transform the dataclass to a long Polars DataFrame with columns</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>@dataclasses.dataclass\nclass BootstrappedConfusionMatrix:\n    \"\"\"Result object returned by `rapidstats.Bootstrap().confusion_matrix`.\n\n    See [rapidstats.metrics.ConfusionMatrix][] for a detailed breakdown of the attributes stored in\n    this class. However, instead of storing the statistic, it stores the bootstrapped\n    confidence interval as (lower, point, upper).\n    \"\"\"\n\n    tn: ConfidenceInterval\n    fp: ConfidenceInterval\n    fn: ConfidenceInterval\n    tp: ConfidenceInterval\n    tpr: ConfidenceInterval\n    fpr: ConfidenceInterval\n    fnr: ConfidenceInterval\n    tnr: ConfidenceInterval\n    prevalence: ConfidenceInterval\n    prevalence_threshold: ConfidenceInterval\n    informedness: ConfidenceInterval\n    precision: ConfidenceInterval\n    false_omission_rate: ConfidenceInterval\n    plr: ConfidenceInterval\n    nlr: ConfidenceInterval\n    acc: ConfidenceInterval\n    balanced_accuracy: ConfidenceInterval\n    fbeta: ConfidenceInterval\n    folkes_mallows_index: ConfidenceInterval\n    mcc: ConfidenceInterval\n    threat_score: ConfidenceInterval\n    markedness: ConfidenceInterval\n    fdr: ConfidenceInterval\n    npv: ConfidenceInterval\n    dor: ConfidenceInterval\n    ppr: ConfidenceInterval\n    pnr: ConfidenceInterval\n\n    def to_polars(self) -&gt; pl.DataFrame:\n        \"\"\"Transform the dataclass to a long Polars DataFrame with columns\n        `metric`, `lower`, `point`, and `upper`.\n\n        Returns\n        -------\n        pl.DataFrame\n            A DataFrame with columns `metric`, `lower`, `point`, and `upper`\n\n        Changelog\n        ---------\n        Return point estimate instead of mean starting version 0.3.0\n        \"\"\"\n        dct = self.__dict__\n        lower = []\n        point = []\n        upper = []\n        for l, p, u in dct.values():  # noqa: E741\n            lower.append(l)\n            point.append(p)\n            upper.append(u)\n\n        return pl.DataFrame(\n            {\n                \"metric\": dct.keys(),\n                \"lower\": lower,\n                \"point\": point,\n                \"upper\": upper,\n            }\n        )\n</code></pre>"},{"location":"bootstrap.html#rapidstats._bootstrap.BootstrappedConfusionMatrix.to_polars","title":"<code>to_polars()</code>","text":"<p>Transform the dataclass to a long Polars DataFrame with columns <code>metric</code>, <code>lower</code>, <code>point</code>, and <code>upper</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with columns <code>metric</code>, <code>lower</code>, <code>point</code>, and <code>upper</code></p> Changelog <p>Return point estimate instead of mean starting version 0.3.0</p> Source code in <code>python/rapidstats/_bootstrap.py</code> <pre><code>def to_polars(self) -&gt; pl.DataFrame:\n    \"\"\"Transform the dataclass to a long Polars DataFrame with columns\n    `metric`, `lower`, `point`, and `upper`.\n\n    Returns\n    -------\n    pl.DataFrame\n        A DataFrame with columns `metric`, `lower`, `point`, and `upper`\n\n    Changelog\n    ---------\n    Return point estimate instead of mean starting version 0.3.0\n    \"\"\"\n    dct = self.__dict__\n    lower = []\n    point = []\n    upper = []\n    for l, p, u in dct.values():  # noqa: E741\n        lower.append(l)\n        point.append(p)\n        upper.append(u)\n\n    return pl.DataFrame(\n        {\n            \"metric\": dct.keys(),\n            \"lower\": lower,\n            \"point\": point,\n            \"upper\": upper,\n        }\n    )\n</code></pre>"},{"location":"correlation.html","title":"Correlation","text":"<p>Functions:</p> Name Description <code>correlation_matrix</code> <p>Warning</p>"},{"location":"correlation.html#rapidstats._corr.correlation_matrix","title":"<code>correlation_matrix(data, l1=None, l2=None, method='pearson', index_name='')</code>","text":"<p>Warning</p> <p>If you know that your data has no nulls, you should use <code>np.corrcoef</code> instead. While this function will return the correct result and is reasonably fast, computing the null-aware correlation matrix will always be slower than assuming that there are no nulls.</p> <p>Compute the null-aware correlation matrix between two lists of columns. If both lists are None, then the correlation matrix is over all columns in the input DataFrame. If <code>l1</code> is not None, and is a list of 2-tuples, <code>l1</code> is interpreted as the combinations of columns to compute the correlation for.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>IntoFrameT</code> <p>The input data</p> required <code>l1</code> <code>Union[list[str], list[tuple[str, str]]]</code> <p>A list of columns to appear as the columns of the correlation matrix, by default None</p> <code>None</code> <code>l2</code> <code>list[str]</code> <p>A list of columns to appear as the rows of the correlation matrix, by default None</p> <code>None</code> <code>method</code> <code>CorrelationMethod</code> <p>How to calculate the correlation, by default \"pearson\"</p> <code>'pearson'</code> <code>index_name</code> <code>str</code> <p>The name of the <code>l2</code> column in the final output, by default \"\"</p> <p>!!! Added in version 0.2.0</p> <code>''</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A correlation matrix with <code>l1</code> as the columns and <code>l2</code> as the rows</p> Added in version 0.0.24 Source code in <code>python/rapidstats/_corr.py</code> <pre><code>def correlation_matrix(\n    data: nwt.IntoFrame,\n    l1: Optional[Union[list[str], list[tuple[str, str]]]] = None,\n    l2: Optional[list[str]] = None,\n    method: CorrelationMethod = \"pearson\",\n    index_name: str = \"\",\n) -&gt; pl.DataFrame:\n    \"\"\"\n    !!! warning\n\n        If you know that your data has no nulls, you should use `np.corrcoef` instead.\n        While this function will return the correct result and is reasonably fast,\n        computing the null-aware correlation matrix will always be slower than assuming\n        that there are no nulls.\n\n    Compute the null-aware correlation matrix between two lists of columns. If both\n    lists are None, then the correlation matrix is over all columns in the input\n    DataFrame. If `l1` is not None, and is a list of 2-tuples, `l1` is interpreted\n    as the combinations of columns to compute the correlation for.\n\n    Parameters\n    ----------\n    data : nwt.IntoFrameT\n        The input data\n    l1 : Union[list[str], list[tuple[str, str]]], optional\n        A list of columns to appear as the columns of the correlation matrix,\n        by default None\n    l2 : list[str], optional\n        A list of columns to appear as the rows of the correlation matrix,\n        by default None\n    method : CorrelationMethod, optional\n        How to calculate the correlation, by default \"pearson\"\n    index_name : str, optional\n        The name of the `l2` column in the final output, by default \"\"\n\n        !!! Added in version 0.2.0\n\n    Returns\n    -------\n    pl.DataFrame\n        A correlation matrix with `l1` as the columns and `l2` as the rows\n\n    Added in version 0.0.24\n    -----------------------\n    \"\"\"\n    pf = nw.from_native(data).to_polars().lazy()\n\n    if l1 is None and l2 is None:\n        original = pf.collect_schema().names()\n        new_columns = [f\"{i}\" for i, _ in enumerate(original)]\n        combinations = itertools.combinations(new_columns, r=2)\n        l1 = original[:-1]\n        l2 = original[1:]\n    elif l1 is not None and l2 is None:\n        # In this case the user should pass in the combinations directly as a list of\n        # 2-tuples.\n        original = set()\n        for a, b in l1:\n            original.add(a)\n            original.add(b)\n        original = list(original)\n        mapper = {name: f\"{i}\" for i, name in enumerate(original)}\n        combinations = [(mapper[a], mapper[b]) for a, b in l1]\n        new_columns = list(mapper.values())\n\n        l1 = original\n        l2 = original\n    else:\n        assert l1 is not None\n        assert l2 is not None\n        valid_cols = set(pf.collect_schema().names())\n        l1 = [c for c in l1 if c in valid_cols]\n        l2 = [c for c in l2 if c in valid_cols]\n\n        new_l1 = [f\"l{i}\" for i, _ in enumerate(l1)]\n        new_l2 = [f\"r{i}\" for i, _ in enumerate(l2)]\n        new_columns = new_l1 + new_l2\n        combinations = _pairs(new_l1, new_l2)\n        original = l1 + l2\n\n    old_to_new_mapper = {old: new for old, new in zip(original, new_columns)}\n    new_to_old_mapper = {new: old for new, old in zip(new_columns, original)}\n\n    corr_mat = (\n        pf.select(original)\n        .rename(old_to_new_mapper)\n        .select(_corr_expr(c1, c2, method=method) for c1, c2 in combinations)\n        .unpivot()\n        .with_columns(pl.col(\"variable\").str.split(\"_\"))\n        .with_columns(\n            pl.col(\"variable\").list.get(0).alias(\"c1\"),\n            pl.col(\"variable\").list.get(1).alias(\"c2\"),\n        )\n        .drop(\"variable\")\n        .collect()\n        .pivot(index=\"c2\", on=\"c1\", values=\"value\")\n    )\n\n    new_row_names = corr_mat[\"c2\"]\n    corr_mat = corr_mat.drop(\"c2\")\n\n    # Set the column names\n    valid_old_names = [new_to_old_mapper[c] for c in corr_mat.columns]\n    corr_mat.columns = valid_old_names\n\n    # Set the row names\n    valid_old_row_names = [new_to_old_mapper[c] for c in new_row_names]\n    corr_mat = corr_mat.with_columns(pl.Series(index_name, valid_old_row_names)).select(\n        index_name, *valid_old_names\n    )\n\n    return corr_mat\n</code></pre>"},{"location":"distributions.html","title":"Distributions","text":"<p>Classes:</p> Name Description <code>norm</code> <p>Functions for working with a normal continuous random variable.</p>"},{"location":"distributions.html#rapidstats._distributions.norm","title":"<code>norm</code>","text":"<p>Functions for working with a normal continuous random variable.</p> <p>Methods:</p> Name Description <code>cdf</code> <p>The cumulative distribution function.</p> <code>ppf</code> <p>The percent point function. Also called the quantile, percentile, inverse</p> Source code in <code>python/rapidstats/_distributions.py</code> <pre><code>class norm:\n    \"\"\"Functions for working with a normal continuous random variable.\"\"\"\n\n    @staticmethod\n    def ppf(q: float) -&gt; float:\n        r\"\"\"The percent point function. Also called the quantile, percentile, inverse\n        CDF, or inverse distribution function. Computes the value of a random variable\n        such that its probability is \\( \\leq q \\). If `q` is 0, it returns negative\n        infinity, if `q` is 1, it returns infinity. Any number outside of [0, 1] will\n        result in NaN.\n\n        Parameters\n        ----------\n        q : float\n            Probability value\n\n        Returns\n        -------\n        float\n            Likelihood a random variable is realized in the range at or below `q` for\n            the normal distribution.\n\n        Added in version 0.0.24\n        -----------------------\n        \"\"\"\n        return _norm_ppf(q)\n\n    @staticmethod\n    def cdf(x: float) -&gt; float:\n        r\"\"\"The cumulative distribution function.\n\n        Parameters\n        ----------\n        x : float\n\n        Returns\n        -------\n        float\n            The probability a random variable will take a value \\( \\leq x \\)\n\n        Added in version 0.0.24\n        -----------------------\n        \"\"\"\n        return _norm_cdf(x)\n</code></pre>"},{"location":"distributions.html#rapidstats._distributions.norm.cdf","title":"<code>cdf(x)</code>  <code>staticmethod</code>","text":"<p>The cumulative distribution function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> required <p>Returns:</p> Type Description <code>float</code> <p>The probability a random variable will take a value \\( \\leq x \\)</p> Added in version 0.0.24 Source code in <code>python/rapidstats/_distributions.py</code> <pre><code>@staticmethod\ndef cdf(x: float) -&gt; float:\n    r\"\"\"The cumulative distribution function.\n\n    Parameters\n    ----------\n    x : float\n\n    Returns\n    -------\n    float\n        The probability a random variable will take a value \\( \\leq x \\)\n\n    Added in version 0.0.24\n    -----------------------\n    \"\"\"\n    return _norm_cdf(x)\n</code></pre>"},{"location":"distributions.html#rapidstats._distributions.norm.ppf","title":"<code>ppf(q)</code>  <code>staticmethod</code>","text":"<p>The percent point function. Also called the quantile, percentile, inverse CDF, or inverse distribution function. Computes the value of a random variable such that its probability is \\( \\leq q \\). If <code>q</code> is 0, it returns negative infinity, if <code>q</code> is 1, it returns infinity. Any number outside of [0, 1] will result in NaN.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>float</code> <p>Probability value</p> required <p>Returns:</p> Type Description <code>float</code> <p>Likelihood a random variable is realized in the range at or below <code>q</code> for the normal distribution.</p> Added in version 0.0.24 Source code in <code>python/rapidstats/_distributions.py</code> <pre><code>@staticmethod\ndef ppf(q: float) -&gt; float:\n    r\"\"\"The percent point function. Also called the quantile, percentile, inverse\n    CDF, or inverse distribution function. Computes the value of a random variable\n    such that its probability is \\( \\leq q \\). If `q` is 0, it returns negative\n    infinity, if `q` is 1, it returns infinity. Any number outside of [0, 1] will\n    result in NaN.\n\n    Parameters\n    ----------\n    q : float\n        Probability value\n\n    Returns\n    -------\n    float\n        Likelihood a random variable is realized in the range at or below `q` for\n        the normal distribution.\n\n    Added in version 0.0.24\n    -----------------------\n    \"\"\"\n    return _norm_ppf(q)\n</code></pre>"},{"location":"drift.html","title":"Drift","text":"<p>Functions:</p> Name Description <code>psi</code> <p>Calculates the Population Stability Index (PSI) between two populations. PSI is</p>"},{"location":"drift.html#rapidstats.drift.psi","title":"<code>psi(reference, current, *, bins=None, bin_count='fd', include_nulls=True, epsilon=0.0001)</code>","text":"<p>Calculates the Population Stability Index (PSI) between two populations. PSI is defined as</p> \\[     PSI = \\sum_{i=1}^{n} (\\% \\text{Current}_{i} - \\% \\text{Reference}_{i}) \\times \\ln\\left(\\frac{\\% \\text{Current}_{i}}{\\% \\text{Reference}_{i}}\\right) \\] <p>That is, bin the reference population and compute the percentage of the overall population in each bin. Take the breakpoints from the reference population and bin the current population, and repeat the process. If the bin percentage is 0, add \\(\\epsilon\\) to penalize that bin while preserving the validity of the log.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>ArrayLike</code> <p>The reference population. The bins are always determined on this pouplation.</p> required <code>current</code> <code>ArrayLike</code> <p>The current population. This population is binned using the breakpoints from the reference population.</p> required <code>bins</code> <code>list[float] | None</code> <p>A list of bin edges. Either <code>bins</code> or <code>bin_count</code> must be specified. The <code>bins</code> argument will take priority, by default None</p> <code>None</code> <code>bin_count</code> <code>int | BinMethod</code> <p>If an integer, the number of bins. It can also be a string corresponding to an auto-binning method, by default \"fd\". The possible methods are</p> <ul> <li>\"doane\", see rapidstats.bin.doane</li> <li>\"fd\", see rapidstats.bin.freedman_diaconis</li> <li>\"rice\", see rapidstats.bin.rice</li> <li>\"sturges\", see rapidstats.bin.sturges</li> <li>\"scott\", see rapidstats.bin.scott</li> <li>\"sqrt\", see rapidstats.bin.sqrt</li> </ul> <code>'fd'</code> <code>include_nulls</code> <code>bool</code> <p>Whether nulls should be considered a bin, by default True</p> <code>True</code> <code>epsilon</code> <code>float | None</code> <p>The correction term to add to 0 percentages, by default 1e-4</p> <code>0.0001</code> <p>Returns:</p> Type Description <code>float</code> Added in version 0.3.0 Source code in <code>python/rapidstats/drift.py</code> <pre><code>def psi(\n    reference: ArrayLike,\n    current: ArrayLike,\n    *,\n    bins: list[float] | None = None,\n    bin_count: int | BinMethod = \"fd\",\n    include_nulls: bool = True,\n    epsilon: float | None = 1e-4,\n) -&gt; float:\n    r\"\"\"Calculates the Population Stability Index (PSI) between two populations. PSI is\n    defined as\n\n    \\[\n        PSI = \\sum_{i=1}^{n} (\\% \\text{Current}_{i} - \\% \\text{Reference}_{i}) \\times \\ln\\left(\\frac{\\% \\text{Current}_{i}}{\\% \\text{Reference}_{i}}\\right)\n    \\]\n\n    That is, bin the reference population and compute the percentage of the overall\n    population in each bin. Take the breakpoints from the reference population and\n    bin the current population, and repeat the process. If the bin percentage is 0, add\n    $\\epsilon$ to penalize that bin while preserving the validity of the log.\n\n    Parameters\n    ----------\n    reference : ArrayLike\n        The reference population. The bins are always determined on this pouplation.\n    current : ArrayLike\n        The current population. This population is binned using the breakpoints from the\n        reference population.\n    bins : list[float] | None, optional\n        A list of bin edges. Either `bins` or `bin_count` must be specified. The `bins`\n        argument will take priority, by default None\n    bin_count : int | BinMethod, optional\n        If an integer, the number of bins. It can also be a string corresponding to an\n        auto-binning method, by default \"fd\". The possible methods are\n\n        - \"doane\", see [rapidstats.bin.doane][]\n        - \"fd\", see [rapidstats.bin.freedman_diaconis][]\n        - \"rice\", see [rapidstats.bin.rice][]\n        - \"sturges\", see [rapidstats.bin.sturges][]\n        - \"scott\", see [rapidstats.bin.scott][]\n        - \"sqrt\", see [rapidstats.bin.sqrt][]\n\n    include_nulls : bool, optional\n        Whether nulls should be considered a bin, by default True\n    epsilon : float | None, optional\n        The correction term to add to 0 percentages, by default 1e-4\n\n    Returns\n    -------\n    float\n\n    Added in version 0.3.0\n    ----------------------\n    \"\"\"\n    reference = pl.Series(\"reference\", reference)\n    current = pl.Series(\"current\", current)\n\n    if reference.dtype.is_numeric() and current.dtype.is_numeric():\n        return _numeric_psi(\n            reference=reference,\n            current=current,\n            bins=bins,\n            bin_count=bin_count,\n            include_nulls=include_nulls,\n            epsilon=epsilon,\n        )\n\n    return _categorical_psi(\n        reference=reference,\n        current=current,\n        include_nulls=include_nulls,\n        epsilon=epsilon,\n    )\n</code></pre>"},{"location":"metrics.html","title":"Metrics","text":"<p>Classes:</p> Name Description <code>ConfusionMatrix</code> <p>Result object returned by <code>rapidstats.metrics.confusion_matrix</code></p> <p>Functions:</p> Name Description <code>adverse_impact_ratio</code> <p>Computes the Adverse Impact Ratio (AIR), which is the ratio of negative</p> <code>adverse_impact_ratio_at_thresholds</code> <p>Computes the Adverse Impact Ratio (AIR) at each threshold of <code>y_score</code>. See</p> <code>average_precision</code> <p>Computes Average Precision.</p> <code>brier_loss</code> <p>Computes the Brier loss (smaller is better). The Brier loss measures the mean</p> <code>confusion_matrix</code> <p>Computes confusion matrix metrics (TP, FP, TN, FN, TPR, Fbeta, etc.).</p> <code>confusion_matrix_at_thresholds</code> <p>Computes the confusion matrix at each threshold. When the <code>strategy</code> is</p> <code>max_ks</code> <p>Performs the two-sample Kolmogorov-Smirnov test on the predicted scores of the</p> <code>mean</code> <p>Computes the mean of the input array.</p> <code>mean_squared_error</code> <p>Computes Mean Squared Error (MSE) as</p> <code>predicted_positive_ratio_at_thresholds</code> <p>Computes the Predicted Positive Ratio (PPR) at each threshold, where the PPR is</p> <code>r2</code> <p>Computes R2 as</p> <code>roc_auc</code> <p>Computes Area Under the Receiver Operating Characteristic Curve.</p> <code>root_mean_squared_error</code> <p>Computes Root Mean Squared Error (RMSE) as</p>"},{"location":"metrics.html#rapidstats.metrics.ConfusionMatrix","title":"<code>ConfusionMatrix</code>  <code>dataclass</code>","text":"<p>Result object returned by <code>rapidstats.metrics.confusion_matrix</code></p> <p>Attributes:</p> Name Type Description <code>tn</code> <code>float</code> <p>\u2191Count of True Negatives; y_true == False and y_pred == False</p> <code>fp</code> <code>float</code> <p>\u2193Count of False Positives; y_true == False and y_pred == True</p> <code>fn</code> <code>float</code> <p>\u2193Count of False Negatives; y_true == True and y_pred == False</p> <code>tp</code> <code>float</code> <p>\u2191Count of True Positives; y_true == True, y_pred == True</p> <code>tpr</code> <code>float</code> <p>\u2191True Positive Rate, Recall, Sensitivity; Probability that an actual positive will be predicted positive; \\( \\frac{TP}{TP + FN} \\)</p> <code>fpr</code> <code>float</code> <p>\u2193False Positive Rate, Type I Error; Probability that an actual negative will be predicted positive; \\( \\frac{FP}{FP + TN} \\)</p> <code>fnr</code> <code>float</code> <p>\u2193False Negative Rate, Type II Error; Probability an actual positive will be predicted negative; \\( \\frac{FN}{TP + FN} \\)</p> <code>tnr</code> <code>float</code> <p>\u2191True Negative Rate, Specificity; Probability an actual negative will be predicted negative; \\( \\frac{TN}{FP + TN} \\)</p> <code>prevalence</code> <code>float</code> <p>Prevalence; Proportion of positive classes; \\( \\frac{TP + FN}{TN + FP + FN + TP} \\)</p> <code>prevalence_threshold</code> <code>float</code> <p>Prevalence Threshold; \\( \\frac{\\sqrt{TPR \\times FPR} - FPR}{TPR - FPR} \\)</p> <code>informedness</code> <code>float</code> <p>\u2191Informedness, Youden's J; \\( TPR + TNR - 1 \\)</p> <code>precision</code> <code>float</code> <p>\u2191Precision, Positive Predicted Value (PPV); Probability a predicted positive was actually correct; \\( \\frac{TP}{TP + FP} \\)</p> <code>false_omission_rate</code> <code>float</code> <p>\u2193False Omission Rate (FOR); Proportion of predicted negatives that were wrong \\( \\frac{FN}{FN + TN} \\)</p> <code>plr</code> <code>float</code> <p>\u2191Positive Likelihood Ratio, LR+; \\( \\frac{TPR}{FPR} \\)</p> <code>nlr</code> <code>float</code> <p>Negative Likelihood Ratio, LR-; \\( \\frac{FNR}{TNR} \\)</p> <code>acc</code> <code>float</code> <p>\u2191Accuracy (ACC); Probability of a correct prediction; \\( \\frac{TP + TN}{TN + FP + FN + TP} \\)</p> <code>balanced_accuracy</code> <code>float</code> <p>\u2191Balanced Accuracy (BA); \\( \\frac{TP + TN}{2} \\)</p> <code>fbeta</code> <code>float</code> <p>\u2191\\( F_{\\beta} \\); Harmonic mean of Precision and Recall; \\( \\frac{(1 + \\beta)^2 \\times PPV \\times TPR}{(\\beta^2 \\times PPV) + TPR} \\)</p> <code>folkes_mallows_index</code> <code>float</code> <p>\u2191Folkes Mallows Index (FM); \\( \\sqrt{PPV \\times TPR} \\)</p> <code>mcc</code> <code>float</code> <p>\u2191Matthew Correlation Coefficient (MCC), Yule Phi Coefficient; \\( \\sqrt{TPR \\times TNR \\times PPV \\times NPV} - \\sqrt{FNR \\times FPR \\times FOR \\times FDR} \\)</p> <code>threat_score</code> <code>float</code> <p>\u2191Threat Score (TS), Critical Success Index (CSI), Jaccard Index; \\( \\frac{TP}{TP + FN + FP} \\)</p> <code>markedness</code> <code>float</code> <p>Markedness (MP), deltaP; \\( PPV + NPV - 1 \\)</p> <code>fdr</code> <code>float</code> <p>\u2193False Discovery Rate, Proportion of predicted positives that are wrong; \\( \\frac{FP}{TP + FP} \\)</p> <code>\u2191npv</code> <code>float</code> <p>Negative Predictive Value; Proportion of predicted negatives that are correct; \\( \\frac{TN}{FN + TN} \\)</p> <code>dor</code> <code>float</code> <p>Diagnostic Odds Ratio; \\( \\frac{LR+}{LR-} \\)</p> <code>ppr</code> <code>float</code> <p>Predicted Positive Ratio; Proportion that are predicted positive; \\( \\frac{TP + FP}{TN + FP + FN + TP} \\)</p> <code>pnr</code> <code>float</code> <p>Predicted Negative Ratio; Proportion that are predicted negative; \\( \\frac{TN + FN}{TN + FP + FN + TP} \\)</p> <p>Methods:</p> Name Description <code>to_polars</code> <p>Convert the dataclass to a long Polars DataFrame with columns <code>metric</code> and</p> Source code in <code>python/rapidstats/metrics.py</code> <pre><code>@dataclasses.dataclass\nclass ConfusionMatrix:\n    r\"\"\"Result object returned by `rapidstats.metrics.confusion_matrix`\n\n    Attributes\n    ----------\n    tn : float\n        \u2191Count of True Negatives; y_true == False and y_pred == False\n    fp : float\n        \u2193Count of False Positives; y_true == False and y_pred == True\n    fn : float\n        \u2193Count of False Negatives; y_true == True and y_pred == False\n    tp : float\n        \u2191Count of True Positives; y_true == True, y_pred == True\n    tpr : float\n        \u2191True Positive Rate, Recall, Sensitivity; Probability that an actual positive\n        will be predicted positive; \\( \\frac{TP}{TP + FN} \\)\n    fpr : float\n        \u2193False Positive Rate, Type I Error; Probability that an actual negative will\n        be predicted positive; \\( \\frac{FP}{FP + TN} \\)\n    fnr : float\n        \u2193False Negative Rate, Type II Error; Probability an actual positive will be\n        predicted negative; \\( \\frac{FN}{TP + FN} \\)\n    tnr : float\n        \u2191True Negative Rate, Specificity; Probability an actual negative will be\n        predicted negative; \\( \\frac{TN}{FP + TN} \\)\n    prevalence : float\n        Prevalence; Proportion of positive classes; \\( \\frac{TP + FN}{TN + FP + FN + TP} \\)\n    prevalence_threshold : float\n        Prevalence Threshold; \\( \\frac{\\sqrt{TPR \\times FPR} - FPR}{TPR - FPR} \\)\n    informedness : float\n        \u2191Informedness, Youden's J; \\( TPR + TNR - 1 \\)\n    precision : float\n        \u2191Precision, Positive Predicted Value (PPV); Probability a predicted positive was\n        actually correct; \\( \\frac{TP}{TP + FP} \\)\n    false_omission_rate : float\n        \u2193False Omission Rate (FOR); Proportion of predicted negatives that were wrong\n        \\( \\frac{FN}{FN + TN} \\)\n    plr : float\n        \u2191Positive Likelihood Ratio, LR+; \\( \\frac{TPR}{FPR} \\)\n    nlr : float\n        Negative Likelihood Ratio, LR-; \\( \\frac{FNR}{TNR} \\)\n    acc : float\n        \u2191Accuracy (ACC); Probability of a correct prediction; \\( \\frac{TP + TN}{TN + FP + FN + TP} \\)\n    balanced_accuracy : float\n        \u2191Balanced Accuracy (BA); \\( \\frac{TP + TN}{2} \\)\n    fbeta : float\n        \u2191\\( F_{\\beta} \\); Harmonic mean of Precision and Recall; \\( \\frac{(1 + \\beta)^2 \\times PPV \\times TPR}{(\\beta^2 \\times PPV) + TPR} \\)\n    folkes_mallows_index : float\n        \u2191Folkes Mallows Index (FM); \\( \\sqrt{PPV \\times TPR} \\)\n    mcc : float\n        \u2191Matthew Correlation Coefficient (MCC), Yule Phi Coefficient; \\( \\sqrt{TPR \\times TNR \\times PPV \\times NPV} - \\sqrt{FNR \\times FPR \\times FOR \\times FDR} \\)\n    threat_score : float\n        \u2191Threat Score (TS), Critical Success Index (CSI), Jaccard Index; \\( \\frac{TP}{TP + FN + FP} \\)\n    markedness : float\n        Markedness (MP), deltaP; \\( PPV + NPV - 1 \\)\n    fdr : float\n        \u2193False Discovery Rate, Proportion of predicted positives that are wrong; \\( \\frac{FP}{TP + FP} \\)\n    \u2191npv : float\n        Negative Predictive Value; Proportion of predicted negatives that are correct; \\( \\frac{TN}{FN + TN} \\)\n    dor : float\n        Diagnostic Odds Ratio; \\( \\frac{LR+}{LR-} \\)\n    ppr : float\n        Predicted Positive Ratio; Proportion that are predicted positive; \\( \\frac{TP + FP}{TN + FP + FN + TP} \\)\n    pnr : float\n        Predicted Negative Ratio; Proportion that are predicted negative; \\( \\frac{TN + FN}{TN + FP + FN + TP} \\)\n    \"\"\"\n\n    tn: float\n    fp: float\n    fn: float\n    tp: float\n    tpr: float\n    fpr: float\n    fnr: float\n    tnr: float\n    prevalence: float\n    prevalence_threshold: float\n    informedness: float\n    precision: float\n    false_omission_rate: float\n    plr: float\n    nlr: float\n    acc: float\n    balanced_accuracy: float\n    fbeta: float\n    folkes_mallows_index: float\n    mcc: float\n    threat_score: float\n    markedness: float\n    fdr: float\n    npv: float\n    dor: float\n    ppr: float\n    pnr: float\n\n    def to_polars(self) -&gt; pl.DataFrame:\n        \"\"\"Convert the dataclass to a long Polars DataFrame with columns `metric` and\n        `value`.\n\n        Returns\n        -------\n        pl.DataFrame\n            DataFrame with columns `metric` and `value`\n        \"\"\"\n        dct = self.__dict__\n\n        return pl.DataFrame({\"metric\": dct.keys(), \"value\": dct.values()})\n</code></pre>"},{"location":"metrics.html#rapidstats.metrics.ConfusionMatrix.to_polars","title":"<code>to_polars()</code>","text":"<p>Convert the dataclass to a long Polars DataFrame with columns <code>metric</code> and <code>value</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns <code>metric</code> and <code>value</code></p> Source code in <code>python/rapidstats/metrics.py</code> <pre><code>def to_polars(self) -&gt; pl.DataFrame:\n    \"\"\"Convert the dataclass to a long Polars DataFrame with columns `metric` and\n    `value`.\n\n    Returns\n    -------\n    pl.DataFrame\n        DataFrame with columns `metric` and `value`\n    \"\"\"\n    dct = self.__dict__\n\n    return pl.DataFrame({\"metric\": dct.keys(), \"value\": dct.values()})\n</code></pre>"},{"location":"metrics.html#rapidstats.metrics.adverse_impact_ratio","title":"<code>adverse_impact_ratio(y_pred, protected, control, sample_weight=None)</code>","text":"<p>Computes the Adverse Impact Ratio (AIR), which is the ratio of negative predictions for the protected class and the control class. The ideal ratio is 1. For example, in an underwriting context, this means that the model is equally as likely to approve protected applicants as it is unprotected applicants, given that the model score is probability of bad.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>ArrayLike</code> <p>Predicted negative</p> required <code>protected</code> <code>ArrayLike</code> <p>An array of booleans identifying the protected class</p> required <code>control</code> <code>ArrayLike</code> <p>An array of booleans identifying the control class</p> required <code>sample_weight</code> <code>Optional[ArrayLike]</code> <p>Sample weights, set to 1 if None</p> <p>Version</p> <p>Added 0.2.0</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Adverse Impact Ratio (AIR)</p> Added in version 0.1.0 Source code in <code>python/rapidstats/metrics.py</code> <pre><code>def adverse_impact_ratio(\n    y_pred: ArrayLike,\n    protected: ArrayLike,\n    control: ArrayLike,\n    sample_weight: Optional[ArrayLike] = None,\n) -&gt; float:\n    \"\"\"Computes the Adverse Impact Ratio (AIR), which is the ratio of negative\n    predictions for the protected class and the control class. The ideal ratio is 1.\n    For example, in an underwriting context, this means that the model is equally as\n    likely to approve protected applicants as it is unprotected applicants, given that\n    the model score is probability of bad.\n\n    Parameters\n    ----------\n    y_pred : ArrayLike\n        Predicted negative\n    protected : ArrayLike\n        An array of booleans identifying the protected class\n    control : ArrayLike\n        An array of booleans identifying the control class\n    sample_weight: Optional[ArrayLike], optional\n        Sample weights, set to 1 if None\n\n        !!! Version\n            Added 0.2.0\n\n    Returns\n    -------\n    float\n        Adverse Impact Ratio (AIR)\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    return _adverse_impact_ratio(\n        pl.DataFrame(\n            {\n                \"y_pred\": y_pred,\n                \"protected\": protected,\n                \"control\": control,\n                \"sample_weight\": 1.0 if sample_weight is None else sample_weight,\n            }\n        )\n        .with_columns(pl.col(\"y_pred\", \"protected\", \"control\").cast(pl.Boolean))\n        .with_columns(pl.col(\"y_pred\").cast(pl.Float64))\n    )\n</code></pre>"},{"location":"metrics.html#rapidstats.metrics.adverse_impact_ratio_at_thresholds","title":"<code>adverse_impact_ratio_at_thresholds(y_score, protected, control, sample_weight=None, thresholds=None, strategy='auto')</code>","text":"<p>Computes the Adverse Impact Ratio (AIR) at each threshold of <code>y_score</code>. See rapidstats.metrics.adverse_impact_ratio for more details. When the <code>strategy</code> is <code>cum_sum</code>, computes</p> <pre><code>for t in y_score:\n    is_predicted_negative = y_score &lt; t\n    adverse_impact_ratio(is_predicted_negative, protected, control)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <code>protected</code> <code>ArrayLike</code> <p>An array of booleans identifying the protected class</p> required <code>control</code> <code>ArrayLike</code> <p>An array of booleans identifying the control class</p> required <code>sample_weight</code> <code>Optional[ArrayLike]</code> <p>Sample weights, set to 1 if None</p> <p>Version</p> <p>Added 0.2.0</p> <code>None</code> <code>thresholds</code> <code>Optional[list[float]]</code> <p>The thresholds to compute <code>is_predicted_negative</code> at, i.e. y_score &lt; t. If None, uses every score present in <code>y_score</code>, by default None</p> <code>None</code> <code>strategy</code> <code>LoopStrategy</code> <p>Computation method, by default \"auto\"</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame of <code>threshold</code> and <code>air</code></p> Added in version 0.1.0 Source code in <code>python/rapidstats/metrics.py</code> <pre><code>def adverse_impact_ratio_at_thresholds(\n    y_score: ArrayLike,\n    protected: ArrayLike,\n    control: ArrayLike,\n    sample_weight: Optional[ArrayLike] = None,\n    thresholds: Optional[list[float]] = None,\n    strategy: LoopStrategy = \"auto\",\n) -&gt; pl.DataFrame:\n    \"\"\"Computes the Adverse Impact Ratio (AIR) at each threshold of `y_score`. See\n    [rapidstats.metrics.adverse_impact_ratio][] for more details. When the `strategy` is\n    `cum_sum`, computes\n\n\n    ``` py\n    for t in y_score:\n        is_predicted_negative = y_score &lt; t\n        adverse_impact_ratio(is_predicted_negative, protected, control)\n    ```\n\n    Parameters\n    ----------\n    y_score : ArrayLike\n        Predicted scores\n    protected : ArrayLike\n        An array of booleans identifying the protected class\n    control : ArrayLike\n        An array of booleans identifying the control class\n    sample_weight: Optional[ArrayLike], optional\n        Sample weights, set to 1 if None\n\n        !!! Version\n            Added 0.2.0\n    thresholds : Optional[list[float]], optional\n        The thresholds to compute `is_predicted_negative` at, i.e. y_score &lt; t. If None,\n        uses every score present in `y_score`, by default None\n    strategy : LoopStrategy, optional\n        Computation method, by default \"auto\"\n\n    Returns\n    -------\n    pl.DataFrame\n        A DataFrame of `threshold` and `air`\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    has_sample_weight = sample_weight is not None\n    df = pl.DataFrame(\n        {\n            \"y_score\": y_score,\n            \"protected\": protected,\n            \"control\": control,\n            \"sample_weight\": sample_weight if has_sample_weight else 1.0,\n        }\n    ).with_columns(\n        pl.col(\"protected\", \"control\").cast(pl.Boolean),\n        pl.col(\"y_score\", \"sample_weight\").cast(pl.Float64),\n    )\n\n    strategy = _set_loop_strategy(thresholds, strategy)\n\n    if strategy == \"loop\":\n\n        def _air(t):\n            return {\n                \"threshold\": t,\n                \"air\": _adverse_impact_ratio(\n                    df.select(\n                        pl.col(\"y_score\").lt(t).cast(pl.Float64).alias(\"y_pred\"),\n                        \"protected\",\n                        \"control\",\n                        \"sample_weight\",\n                    )\n                ),\n            }\n\n        airs = _run_concurrent(_air, set(thresholds or y_score))\n\n        res = pl.LazyFrame(airs)\n    elif strategy == \"cum_sum\":\n        res = _air_at_thresholds_core(\n            df, thresholds, has_sample_weight=has_sample_weight\n        )\n\n    return res.pipe(_fill_infinite, None).fill_nan(None).collect()\n</code></pre>"},{"location":"metrics.html#rapidstats.metrics.average_precision","title":"<code>average_precision(y_true, y_score, sample_weight=None)</code>","text":"<p>Computes Average Precision.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <code>sample_weight</code> <code>Optional[ArrayLike]</code> <p>Sample weights, set to 1 if None</p> <p>Version</p> <p>Added 0.2.0</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Average Precision (AP)</p> Added in version 0.1.0 Source code in <code>python/rapidstats/metrics.py</code> <pre><code>def average_precision(\n    y_true: ArrayLike, y_score: ArrayLike, sample_weight: Optional[ArrayLike] = None\n) -&gt; float:\n    \"\"\"Computes Average Precision.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n    sample_weight: Optional[ArrayLike], optional\n        Sample weights, set to 1 if None\n\n        !!! Version\n            Added 0.2.0\n\n    Returns\n    -------\n    float\n        Average Precision (AP)\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    return (\n        _y_true_y_score_to_df(y_true, y_score, sample_weight)\n        .lazy()\n        .rename({\"y_score\": \"threshold\"})\n        .pipe(_base_confusion_matrix_at_thresholds)\n        .pipe(_full_confusion_matrix_from_base)\n        .select(\"threshold\", \"precision\", \"tpr\")\n        .drop_nulls()\n        .unique(\"threshold\")\n        .sort(\"threshold\")\n        .select(_ap_from_pr_curve(pl.col(\"precision\"), pl.col(\"tpr\")).alias(\"ap\"))\n        .collect()[\"ap\"]\n        .item()\n    )\n</code></pre>"},{"location":"metrics.html#rapidstats.metrics.brier_loss","title":"<code>brier_loss(y_true, y_score)</code>","text":"<p>Computes the Brier loss (smaller is better). The Brier loss measures the mean squared difference between the predicted scores and the ground truth target. Calculated as:</p> \\[ \\frac{1}{N} \\sum_{i=1}^N (yt_i - ys_i)^2 \\] <p>where \\( yt \\) is <code>y_true</code> and \\( ys \\) is <code>y_score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>float</code> <p>Brier loss</p> Added in version 0.1.0 Source code in <code>python/rapidstats/metrics.py</code> <pre><code>def brier_loss(y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    r\"\"\"Computes the Brier loss (smaller is better). The Brier loss measures the mean\n    squared difference between the predicted scores and the ground truth target.\n    Calculated as:\n\n    \\[ \\frac{1}{N} \\sum_{i=1}^N (yt_i - ys_i)^2 \\]\n\n    where \\( yt \\) is `y_true` and \\( ys \\) is `y_score`.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    float\n        Brier loss\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score)\n\n    return _brier_loss(df)\n</code></pre>"},{"location":"metrics.html#rapidstats.metrics.confusion_matrix","title":"<code>confusion_matrix(y_true, y_pred, beta=1.0, sample_weight=None)</code>","text":"<p>Computes confusion matrix metrics (TP, FP, TN, FN, TPR, Fbeta, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_pred</code> <code>ArrayLike</code> <p>Predicted target</p> required <code>beta</code> <code>float</code> <p>\\( \\beta \\) to use in \\( F_\\beta \\), by default 1</p> <code>1.0</code> <code>sample_weight</code> <code>Optional[ArrayLike]</code> <p>Sample weights, set to 1 if None</p> <p>Version</p> <p>Added 0.2.0</p> <code>None</code> <p>Returns:</p> Type Description <code>ConfusionMatrix</code> <p>Dataclass of confusion matrix metrics</p> Added in version 0.1.0 Source code in <code>python/rapidstats/metrics.py</code> <pre><code>def confusion_matrix(\n    y_true: ArrayLike,\n    y_pred: ArrayLike,\n    beta: float = 1.0,\n    sample_weight: Optional[ArrayLike] = None,\n) -&gt; ConfusionMatrix:\n    r\"\"\"Computes confusion matrix metrics (TP, FP, TN, FN, TPR, Fbeta, etc.).\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_pred : ArrayLike\n        Predicted target\n    beta : float, optional\n        \\( \\beta \\) to use in \\( F_\\beta \\), by default 1\n    sample_weight: Optional[ArrayLike], optional\n        Sample weights, set to 1 if None\n\n        !!! Version\n            Added 0.2.0\n\n    Returns\n    -------\n    ConfusionMatrix\n        Dataclass of confusion matrix metrics\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    df = _y_true_y_pred_to_df(y_true, y_pred, sample_weight).with_columns(\n        pl.col(\"y_true\").cast(pl.UInt8)\n    )\n\n    return ConfusionMatrix(*_confusion_matrix(df, beta))\n</code></pre>"},{"location":"metrics.html#rapidstats.metrics.confusion_matrix_at_thresholds","title":"<code>confusion_matrix_at_thresholds(y_true, y_score, thresholds=None, metrics=DefaultConfusionMatrixMetrics, strategy='auto', beta=1.0, sample_weight=None)</code>","text":"<p>Computes the confusion matrix at each threshold. When the <code>strategy</code> is \"cum_sum\", computes</p> <pre><code>for t in y_score:\n    y_pred = y_score &gt;= t\n    confusion_matrix(y_true, y_pred)\n</code></pre> <p>using fast DataFrame operations.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <code>thresholds</code> <code>Optional[list[float]]</code> <p>The thresholds to compute <code>y_pred</code> at, i.e. y_score &gt;= t. If None, uses every score present in <code>y_score</code>, by default None</p> <code>None</code> <code>metrics</code> <code>Iterable[ConfusionMatrixMetric]</code> <p>The metrics to compute, by default DefaultConfusionMatrixMetrics</p> <code>DefaultConfusionMatrixMetrics</code> <code>strategy</code> <code>LoopStrategy</code> <p>Computation method, by default \"auto\"</p> <code>'auto'</code> <code>beta</code> <code>float</code> <p>\\( \\beta \\) to use in \\( F_\\beta \\), by default 1</p> <code>1.0</code> <code>sample_weight</code> <code>Optional[ArrayLike]</code> <p>Sample weights, set to 1 if None</p> <p>Version</p> <p>Added 0.2.0</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Polars DataFrame of <code>threshold</code>, <code>metric</code>, and <code>value</code></p> Added in version 0.1.0 Source code in <code>python/rapidstats/metrics.py</code> <pre><code>def confusion_matrix_at_thresholds(\n    y_true: ArrayLike,\n    y_score: ArrayLike,\n    thresholds: Optional[list[float]] = None,\n    metrics: Iterable[ConfusionMatrixMetric] = DefaultConfusionMatrixMetrics,\n    strategy: LoopStrategy = \"auto\",\n    beta: float = 1.0,\n    sample_weight: Optional[ArrayLike] = None,\n) -&gt; pl.DataFrame:\n    r\"\"\"Computes the confusion matrix at each threshold. When the `strategy` is\n    \"cum_sum\", computes\n\n    ``` py\n    for t in y_score:\n        y_pred = y_score &gt;= t\n        confusion_matrix(y_true, y_pred)\n    ```\n\n    using fast DataFrame operations.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n    thresholds : Optional[list[float]], optional\n        The thresholds to compute `y_pred` at, i.e. y_score &gt;= t. If None,\n        uses every score present in `y_score`, by default None\n    metrics : Iterable[ConfusionMatrixMetric], optional\n        The metrics to compute, by default DefaultConfusionMatrixMetrics\n    strategy : LoopStrategy, optional\n        Computation method, by default \"auto\"\n    beta : float, optional\n        \\( \\beta \\) to use in \\( F_\\beta \\), by default 1\n    sample_weight: Optional[ArrayLike], optional\n        Sample weights, set to 1 if None\n\n        !!! Version\n            Added 0.2.0\n\n    Returns\n    -------\n    pl.DataFrame\n        A Polars DataFrame of `threshold`, `metric`, and `value`\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    strategy = _set_loop_strategy(thresholds, strategy)\n\n    if strategy == \"loop\":\n        df = _y_true_y_score_to_df(y_true, y_score)\n\n        def _cm(t):\n            return (\n                confusion_matrix(\n                    df[\"y_true\"],\n                    df[\"y_score\"].ge(t),\n                    beta=beta,\n                    sample_weight=sample_weight,\n                )\n                .to_polars()\n                .with_columns(pl.lit(t).alias(\"threshold\"))\n            )\n\n        cms: list[pl.DataFrame] = _run_concurrent(_cm, set(thresholds or y_score))\n\n        return pl.concat(cms, how=\"vertical\").fill_nan(None)\n    elif strategy == \"cum_sum\":\n        return (\n            pl.LazyFrame(\n                {\n                    \"y_true\": y_true,\n                    \"threshold\": y_score,\n                    \"sample_weight\": 1.0 if sample_weight is None else sample_weight,\n                }\n            )\n            .with_columns(pl.col(\"y_true\").cast(pl.Boolean))\n            .drop_nulls()\n            .pipe(_base_confusion_matrix_at_thresholds)\n            .pipe(_full_confusion_matrix_from_base, beta=beta)\n            .select(\"threshold\", *metrics)\n            .unique(\"threshold\")\n            .pipe(_map_to_thresholds, thresholds)\n            .drop(\"_threshold_actual\", strict=False)\n            .unpivot(index=\"threshold\")\n            .rename({\"variable\": \"metric\"})\n            .collect()\n        )\n</code></pre>"},{"location":"metrics.html#rapidstats.metrics.max_ks","title":"<code>max_ks(y_true, y_score)</code>","text":"<p>Performs the two-sample Kolmogorov-Smirnov test on the predicted scores of the ground truth positive and ground truth negative classes. The KS test measures the highest distance between two CDFs, so the Max-KS metric measures how well the model separates two classes. In pseucode:</p> <pre><code>df = Frame(y_true, y_score)\nclass0 = df.filter(~y_true)[\"y_score\"]\nclass1 = df.filter(y_true)[\"y_score\"]\n\nks(class0, class1)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>float</code> <p>Max-KS</p> Added in version 0.1.0 Source code in <code>python/rapidstats/metrics.py</code> <pre><code>def max_ks(y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    \"\"\"Performs the two-sample Kolmogorov-Smirnov test on the predicted scores of the\n    ground truth positive and ground truth negative classes. The KS test measures the\n    highest distance between two CDFs, so the Max-KS metric measures how well the model\n    separates two classes. In pseucode:\n\n    ``` py\n    df = Frame(y_true, y_score)\n    class0 = df.filter(~y_true)[\"y_score\"]\n    class1 = df.filter(y_true)[\"y_score\"]\n\n    ks(class0, class1)\n    ```\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    float\n        Max-KS\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score)\n\n    return _max_ks(df)\n</code></pre>"},{"location":"metrics.html#rapidstats.metrics.mean","title":"<code>mean(y)</code>","text":"<p>Computes the mean of the input array.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ArrayLike</code> <p>A 1D-array of numbers</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean</p> Added in version 0.1.0 Source code in <code>python/rapidstats/metrics.py</code> <pre><code>def mean(y: ArrayLike) -&gt; float:\n    \"\"\"Computes the mean of the input array.\n\n    Parameters\n    ----------\n    y : ArrayLike\n        A 1D-array of numbers\n\n    Returns\n    -------\n    float\n        Mean\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    return _mean(pl.DataFrame({\"y\": y}))\n</code></pre>"},{"location":"metrics.html#rapidstats.metrics.mean_squared_error","title":"<code>mean_squared_error(y_true, y_score)</code>","text":"<p>Computes Mean Squared Error (MSE) as</p> \\[ \\frac{1}{N} \\sum_{i=1}^{N} (yt_i - ys_i)^2 \\] <p>where \\( yt \\) is <code>y_true</code> and \\( ys \\) is <code>y_score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean Squared Error (MSE)</p> Added in version 0.1.0 Source code in <code>python/rapidstats/metrics.py</code> <pre><code>def mean_squared_error(y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    r\"\"\"Computes Mean Squared Error (MSE) as\n\n    \\[ \\frac{1}{N} \\sum_{i=1}^{N} (yt_i - ys_i)^2 \\]\n\n    where \\( yt \\) is `y_true` and \\( ys \\) is `y_score`.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    float\n        Mean Squared Error (MSE)\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    return _mean_squared_error(_regression_to_df(y_true, y_score))\n</code></pre>"},{"location":"metrics.html#rapidstats.metrics.predicted_positive_ratio_at_thresholds","title":"<code>predicted_positive_ratio_at_thresholds(y_score, sample_weight=None, thresholds=None, strategy='auto')</code>","text":"<p>Computes the Predicted Positive Ratio (PPR) at each threshold, where the PPR is the ratio of predicted positive to the total, and a positive is defined as <code>y_score</code> &gt;= threshold.</p> <p>Parameters:</p> Name Type Description Default <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <code>sample_weight</code> <code>Optional[ArrayLike]</code> <p>Sample weights, set to 1 if None</p> <p>Version</p> <p>Added 0.2.0</p> <code>None</code> <code>thresholds</code> <code>Optional[list[float]]</code> <p>The thresholds to compute <code>y_pred</code> at, i.e. y_score &gt;= t. If None, uses every score present in <code>y_score</code>, by default None</p> <code>None</code> <code>strategy</code> <code>LoopStrategy</code> <p>Computation method, by default \"auto\"</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame of <code>threshold</code> and <code>ppr</code></p> Added in version 0.1.0 Source code in <code>python/rapidstats/metrics.py</code> <pre><code>def predicted_positive_ratio_at_thresholds(\n    y_score: ArrayLike,\n    sample_weight: Optional[ArrayLike] = None,\n    thresholds: Optional[list[float]] = None,\n    strategy: LoopStrategy = \"auto\",\n) -&gt; pl.DataFrame:\n    \"\"\"Computes the Predicted Positive Ratio (PPR) at each threshold, where the PPR is\n    the ratio of predicted positive to the total, and a positive is defined as\n    `y_score` &gt;= threshold.\n\n    Parameters\n    ----------\n    y_score : ArrayLike\n        Predicted scores\n    sample_weight: Optional[ArrayLike], optional\n        Sample weights, set to 1 if None\n\n        !!! Version\n            Added 0.2.0\n    thresholds : Optional[list[float]], optional\n        The thresholds to compute `y_pred` at, i.e. y_score &gt;= t. If None,\n        uses every score present in `y_score`, by default None\n    strategy : LoopStrategy, optional\n        Computation method, by default \"auto\"\n\n    Returns\n    -------\n    pl.DataFrame\n        A DataFrame of `threshold` and `ppr`\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    lf = pl.LazyFrame(\n        {\n            \"y_score\": y_score,\n            \"sample_weight\": 1.0 if sample_weight is None else sample_weight,\n        }\n    ).drop_nulls()\n\n    strategy = _set_loop_strategy(y_score, strategy)\n\n    if strategy == \"loop\":\n        df = lf.collect()\n\n        def _ppr(t: float) -&gt; float:\n            return {\n                \"threshold\": t,\n                \"ppr\": _weighted_mean(df[\"y_score\"].ge(t), df[\"sample_weight\"]),\n            }\n\n        return pl.DataFrame(_run_concurrent(_ppr, set(thresholds or y_score)))\n    elif strategy == \"cum_sum\":\n\n        def _cumulative_ppr(lf: pl.LazyFrame, has_sample_weight: bool):\n            if not has_sample_weight:\n                return lf.with_row_index(\n                    \"cumulative_predicted_positive\", offset=1\n                ).with_columns(\n                    pl.col(\"cumulative_predicted_positive\")\n                    .truediv(pl.len())\n                    .alias(\"ppr\")\n                )\n            else:\n                return lf.with_columns(\n                    pl.col(\"sample_weight\")\n                    .cum_sum()\n                    .alias(\"cumulative_predicted_positive\")\n                ).with_columns(\n                    pl.col(\"cumulative_predicted_positive\")\n                    .truediv(pl.col(\"sample_weight\").sum())\n                    .alias(\"ppr\")\n                )\n\n        return (\n            lf.sort(\"y_score\", descending=True)\n            .pipe(_cumulative_ppr, sample_weight is not None)\n            .rename({\"y_score\": \"threshold\"})\n            .select(\"threshold\", \"ppr\")\n            .unique(\"threshold\")\n            .pipe(_map_to_thresholds, thresholds)\n            .drop(\"_threshold_actual\", strict=False)\n            .collect()\n        )\n</code></pre>"},{"location":"metrics.html#rapidstats.metrics.r2","title":"<code>r2(y_true, y_score)</code>","text":"<p>Computes R2 as</p> \\[     1 - \\frac{\\sum{(y_i - \\hat{y_i})^2}{}}{\\sum{(y_{i} - \\bar{y})^2}} \\] <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>float</code> <p>R2</p> Added in version 0.1.0 Source code in <code>python/rapidstats/metrics.py</code> <pre><code>def r2(y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    r\"\"\"Computes R2 as\n\n    \\[\n        1 - \\frac{\\sum{(y_i - \\hat{y_i})^2}{}}{\\sum{(y_{i} - \\bar{y})^2}}\n    \\]\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    float\n        R2\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    return _r2(_regression_to_df(y_true, y_score))\n</code></pre>"},{"location":"metrics.html#rapidstats.metrics.roc_auc","title":"<code>roc_auc(y_true, y_score, sample_weight=None)</code>","text":"<p>Computes Area Under the Receiver Operating Characteristic Curve.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <code>sample_weight</code> <code>Optional[ArrayLike]</code> <p>Sample weights, set to 1 if None</p> <p>Version</p> <p>Added 0.2.0</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>ROC-AUC</p> Added in version 0.1.0 Source code in <code>python/rapidstats/metrics.py</code> <pre><code>def roc_auc(\n    y_true: ArrayLike, y_score: ArrayLike, sample_weight: Optional[ArrayLike] = None\n) -&gt; float:\n    \"\"\"Computes Area Under the Receiver Operating Characteristic Curve.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n    sample_weight: Optional[ArrayLike], optional\n        Sample weights, set to 1 if None\n\n        !!! Version\n            Added 0.2.0\n\n    Returns\n    -------\n    float\n        ROC-AUC\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    df = _y_true_y_score_to_df(y_true, y_score, sample_weight).with_columns(\n        pl.col(\"y_true\").cast(pl.Float64)\n    )\n\n    return _roc_auc(df)\n</code></pre>"},{"location":"metrics.html#rapidstats.metrics.root_mean_squared_error","title":"<code>root_mean_squared_error(y_true, y_score)</code>","text":"<p>Computes Root Mean Squared Error (RMSE) as</p> \\[ \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (yt_i - ys_i)^2} \\] <p>where \\( yt \\) is <code>y_true</code> and \\( ys \\) is <code>y_score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Ground truth target</p> required <code>y_score</code> <code>ArrayLike</code> <p>Predicted scores</p> required <p>Returns:</p> Type Description <code>float</code> <p>Root Mean Squared Error (RMSE)</p> Added in version 0.1.0 Source code in <code>python/rapidstats/metrics.py</code> <pre><code>def root_mean_squared_error(y_true: ArrayLike, y_score: ArrayLike) -&gt; float:\n    r\"\"\"Computes Root Mean Squared Error (RMSE) as\n\n    \\[ \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (yt_i - ys_i)^2} \\]\n\n    where \\( yt \\) is `y_true` and \\( ys \\) is `y_score`.\n\n    Parameters\n    ----------\n    y_true : ArrayLike\n        Ground truth target\n    y_score : ArrayLike\n        Predicted scores\n\n    Returns\n    -------\n    float\n        Root Mean Squared Error (RMSE)\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    return _root_mean_squared_error(_regression_to_df(y_true, y_score))\n</code></pre>"},{"location":"misc.html","title":"Miscellaneous","text":"<p>Functions:</p> Name Description <code>auc</code> <p>Computes the Area Under the Curve (AUC) via numerical integration.</p>"},{"location":"misc.html#rapidstats._general.auc","title":"<code>auc(x, y, method='trapezoidal', sorted=False)</code>","text":"<p>Computes the Area Under the Curve (AUC) via numerical integration.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>x points</p> required <code>y</code> <code>ArrayLike</code> <p>y points</p> required <code>method</code> <code>Literal['rectangular', 'trapezoidal']</code> <p>Integration method, by default \"trapezoidal\"</p> <code>'trapezoidal'</code> <code>sorted</code> <code>bool</code> <p>If True, assumes arrays are already sorted by <code>x</code>, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>The AUC</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>method</code> is not one of <code>rectangular</code> or <code>trapezoidal</code></p> Added in version 0.1.0 Source code in <code>python/rapidstats/_general.py</code> <pre><code>def auc(\n    x: ArrayLike,\n    y: ArrayLike,\n    method: Literal[\"rectangular\", \"trapezoidal\"] = \"trapezoidal\",\n    sorted: bool = False,\n) -&gt; float:\n    \"\"\"Computes the Area Under the Curve (AUC) via numerical integration.\n\n    Parameters\n    ----------\n    x : ArrayLike\n        x points\n    y : ArrayLike\n        y points\n    method : Literal[\"rectangular\", \"trapezoidal\"], optional\n        Integration method, by default \"trapezoidal\"\n    sorted : bool, optional\n        If True, assumes arrays are already sorted by `x`, by default False\n\n    Returns\n    -------\n    float\n        The AUC\n\n    Raises\n    ------\n    ValueError\n        If `method` is not one of `rectangular` or `trapezoidal`\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n    df = pl.DataFrame({\"x\": x, \"y\": y}).drop_nulls().cast(pl.Float64)\n\n    if not sorted:\n        df = df.sort(\"x\", descending=False)\n\n    if method == \"rectangular\":\n        return _rectangular_auc(df)\n    elif method == \"trapezoidal\":\n        return _trapezoidal_auc(df)\n    else:\n        raise ValueError(\"`method` must be one of `rectangular` or `trapezoidal`\")\n</code></pre>"},{"location":"polars.html","title":"Polars","text":"<p>Functions:</p> Name Description <code>auc</code> <p>Computes the area under the curve (AUC) via numerical integration.</p> <code>is_close</code> <p>Compares the relative equality of the inputs.</p> <p>Functions:</p> Name Description <code>format</code> <p>Format expressions as a string using Python f-string syntax.</p>"},{"location":"polars.html#rapidstats._polars._numeric.auc","title":"<code>auc(x, y, method='trapezoidal')</code>","text":"<p>Computes the area under the curve (AUC) via numerical integration.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Expr | str</code> <p>The x-axis</p> required <code>y</code> <code>Expr | str</code> <p>The y-axis</p> required <code>method</code> <code>Literal['rectangular', 'trapezoidal']</code> <p>If \"rectangular\", use rectangular integration, if \"trapezoidal\", use trapezoidal integration, by default \"trapezoidal\"</p> <code>'trapezoidal'</code> <p>Returns:</p> Type Description <code>Expr</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>method</code> is not one of <code>rectangular</code> or <code>trapezoidal</code></p> <p>Examples:</p> <p><pre><code>import polars as pl\nimport rapidstats.polars as rps\n\ndf = pl.DataFrame({\"x\": [1, 2, 3], \"y\": [5, 6, 7]})\ndf.select(rps.auc(\"x\", \"y\"))\n</code></pre> output<pre><code>shape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x    \u2502\n\u2502 ---  \u2502\n\u2502 f64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 12.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> Added in version 0.2.0 Source code in <code>python/rapidstats/_polars/_numeric.py</code> <pre><code>def auc(\n    x: IntoExprColumn,\n    y: IntoExprColumn,\n    method: Literal[\"rectangular\", \"trapezoidal\"] = \"trapezoidal\",\n) -&gt; pl.Expr:\n    \"\"\"Computes the area under the curve (AUC) via numerical integration.\n\n    Parameters\n    ----------\n    x : pl.Expr | str\n        The x-axis\n    y : pl.Expr | str\n        The y-axis\n    method : Literal[\"rectangular\", \"trapezoidal\"], optional\n        If \"rectangular\", use rectangular integration, if \"trapezoidal\", use\n        trapezoidal integration, by default \"trapezoidal\"\n\n    Returns\n    -------\n    pl.Expr\n\n    Raises\n    ------\n    ValueError\n        If `method` is not one of `rectangular` or `trapezoidal`\n\n    Examples\n    --------\n    ``` py\n    import polars as pl\n    import rapidstats.polars as rps\n\n    df = pl.DataFrame({\"x\": [1, 2, 3], \"y\": [5, 6, 7]})\n    df.select(rps.auc(\"x\", \"y\"))\n    ```\n    ``` title=\"output\"\n    shape: (1, 1)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 x    \u2502\n    \u2502 ---  \u2502\n    \u2502 f64  \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 12.0 \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    ```\n\n    Added in version 0.2.0\n    ----------------------\n    \"\"\"\n    if method == \"trapezoidal\":\n        is_trapezoidal = True\n    elif method == \"rectangular\":\n        is_trapezoidal = False\n    else:\n        raise ValueError(\"`method` must be one of `rectangular` or `trapezoidal`\")\n\n    return pl.plugins.register_plugin_function(\n        plugin_path=_PLUGIN_PATH,\n        function_name=\"pl_auc\",\n        args=[\n            _str_to_expr(x).cast(pl.Float64),\n            _str_to_expr(y).cast(pl.Float64),\n            pl.lit(is_trapezoidal),\n        ],\n        returns_scalar=True,\n    )\n</code></pre>"},{"location":"polars.html#rapidstats._polars._numeric.is_close","title":"<code>is_close(x, y, rtol=1e-05, atol=1e-08, null_equal=False)</code>","text":"<p>Compares the relative equality of the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Expr | str | float</code> required <code>y</code> <code>Expr | str | float</code> required <code>rtol</code> <code>float</code> <p>Relative tolerance, by default 1e-05</p> <code>1e-05</code> <code>atol</code> <code>float</code> <p>Absolute tolerance, by default 1e-08</p> <code>1e-08</code> <code>null_equal</code> <code>bool</code> <p>If True, considers nulls to be equal, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>Expr</code> <p>Examples:</p> <p><pre><code>import polars as pl\nimport rapidstats.polars as rps\n\ndf = pl.DataFrame({\"x\": [1.0, 1.1], \"y\": [.999999999, 5]})\ndf.select(rps.is_close(\"x\", \"y\"))\n</code></pre> output<pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x     \u2502\n\u2502 ---   \u2502\n\u2502 bool  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 true  \u2502\n\u2502 false \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> Added in version 0.2.0 Source code in <code>python/rapidstats/_polars/_numeric.py</code> <pre><code>def is_close(\n    x: IntoExprColumn | NumericLiteral,\n    y: IntoExprColumn | NumericLiteral,\n    rtol: float = 1e-05,\n    atol: float = 1e-08,\n    null_equal: bool = False,\n) -&gt; pl.Expr:\n    \"\"\"Compares the relative equality of the inputs.\n\n    Parameters\n    ----------\n    x : pl.Expr | str | float\n    y : pl.Expr | str | float\n    rtol : float, optional\n        Relative tolerance, by default 1e-05\n    atol : float, optional\n        Absolute tolerance, by default 1e-08\n    null_equal : bool, optional\n        If True, considers nulls to be equal, by default False\n\n    Returns\n    -------\n    pl.Expr\n\n    Examples\n    --------\n    ``` py\n    import polars as pl\n    import rapidstats.polars as rps\n\n    df = pl.DataFrame({\"x\": [1.0, 1.1], \"y\": [.999999999, 5]})\n    df.select(rps.is_close(\"x\", \"y\"))\n    ```\n    ``` title=\"output\"\n    shape: (2, 1)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 x     \u2502\n    \u2502 ---   \u2502\n    \u2502 bool  \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 true  \u2502\n    \u2502 false \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    ```\n\n    Added in version 0.2.0\n    ----------------------\n    \"\"\"\n    x = _numeric_to_expr(x)\n    y = _numeric_to_expr(y)\n\n    res = x.sub(y).abs().le(pl.lit(atol).add(rtol).mul(y.abs()))\n\n    if null_equal:\n        res = res.or_(x.is_null().and_(y.is_null()))\n\n    return res\n</code></pre>"},{"location":"polars.html#rapidstats._polars._format.format","title":"<code>format(f_string, *args)</code>","text":"<p>Format expressions as a string using Python f-string syntax.</p> <p>Parameters:</p> Name Type Description Default <code>f_string</code> <code>str</code> <p>A string with placeholders, mimicing Python f-string syntax. For example, \"{:.3f}\". Currently, the only supported types are \"f\" and \"%\". Width, alignment, and fill are also not supported.</p> required <code>args</code> <code>Union[Expr, str, float]</code> <p>Expression(s) that fill the placeholders. Note that strings are NOT parsed as columns.</p> <code>()</code> <p>Returns:</p> Type Description <code>Expr</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of placeholders does not match the number of expressions</p> <p>Examples:</p> <p><pre><code>import polars as pl\nimport rapidstats.polars as rps\n\ndf = pl.DataFrame({\"x\": 1123.09873, \"y\": \"foo\"})\ndf.select(\n    rps.format(\n        \"{:,.3f} is {} is {}\", pl.col(\"x\"), pl.col(\"y\"), \"bar\"\n    )\n)\n</code></pre> output<pre><code>shape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x                       \u2502\n\u2502 ---                     \u2502\n\u2502 str                     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1,123.099 is foo is bar \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> Added in version 0.2.0 Source code in <code>python/rapidstats/_polars/_format.py</code> <pre><code>def format(f_string: str, *args: Union[pl.Expr, str, float]) -&gt; pl.Expr:\n    \"\"\"Format expressions as a string using Python f-string syntax.\n\n    Parameters\n    ----------\n    f_string : str\n        A string with placeholders, mimicing Python f-string syntax. For example,\n        \"{:.3f}\". Currently, the only supported types are \"f\" and \"%\". Width, alignment,\n        and fill are also not supported.\n    args\n        Expression(s) that fill the placeholders. Note that strings are NOT parsed as\n        columns.\n\n    Returns\n    -------\n    pl.Expr\n\n    Raises\n    ------\n    ValueError\n        If the number of placeholders does not match the number of expressions\n\n    Examples\n    --------\n    ``` py\n    import polars as pl\n    import rapidstats.polars as rps\n\n    df = pl.DataFrame({\"x\": 1123.09873, \"y\": \"foo\"})\n    df.select(\n        rps.format(\n            \"{:,.3f} is {} is {}\", pl.col(\"x\"), pl.col(\"y\"), \"bar\"\n        )\n    )\n    ```\n    ``` title=\"output\"\n    shape: (1, 1)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 x                       \u2502\n    \u2502 ---                     \u2502\n    \u2502 str                     \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 1,123.099 is foo is bar \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    ```\n\n    Added in version 0.2.0\n    ----------------------\n    \"\"\"\n    parts = _parse_format_string(f_string)\n    formatters = [\n        _parse_formatter(p) for p, is_formatter in zip(*parts) if is_formatter\n    ]\n\n    len_formatters = len(formatters)\n    len_args = len(args)\n    if len_formatters != len(args):\n        raise ValueError(\n            f\"Number of placeholders `{len_formatters}` does not match number of arguments `{len_args}`\"\n        )\n\n    outputs = [\n        _apply_formatter(s if isinstance(s, pl.Expr) else pl.lit(s), format_spec)\n        for s, format_spec in zip(args, formatters)\n    ]\n\n    i = 0\n    to_concat = []\n    for p, is_formatter in zip(*parts):\n        if is_formatter:\n            to_concat.append(outputs[i])\n            i += 1\n        else:\n            to_concat.append(pl.lit(p))\n\n    return pl.concat_str(*to_concat)\n</code></pre>"},{"location":"preprocessing.html","title":"Preprocessing","text":"<p>Classes:</p> Name Description <code>MinMaxScaler</code> <p>Scale data using min-max scaling.</p> <code>OneHotEncoder</code> <p>One-hot encodes data.</p> <code>StandardScaler</code> <p>summary</p>"},{"location":"preprocessing.html#rapidstats.preprocessing.MinMaxScaler","title":"<code>MinMaxScaler</code>","text":"<p>Scale data using min-max scaling.</p> <p>Parameters:</p> Name Type Description Default <code>feature_range</code> <code>tuple[float, float]</code> <p>The range to scale the data to, by default (0, 1)</p> <code>(0, 1)</code> Added in version 0.1.0 <p>Methods:</p> Name Description <code>fit</code> <p>summary</p> <code>load</code> <p>summary</p> <code>save</code> <p>summary</p> Source code in <code>python/rapidstats/preprocessing.py</code> <pre><code>class MinMaxScaler:\n    \"\"\"Scale data using min-max scaling.\n\n    Parameters\n    ----------\n    feature_range : tuple[float, float], optional\n        The range to scale the data to, by default (0, 1)\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n\n    def __init__(self, feature_range: tuple[float, float] = (0, 1)):\n        self.feature_range = feature_range\n        self._set_range_vars()\n\n    def _set_range_vars(self):\n        self._range_min, self._range_max = self.feature_range\n        self._range_diff = self._range_max - self._range_min\n\n        return self\n\n    def fit(self, X: nwt.IntoDataFrameT, columns: Optional[str | Iterable[str]] = None):\n        \"\"\"_summary_\n\n        Parameters\n        ----------\n        X : nwt.IntoDataFrameT\n            _description_\n\n        Attributes\n        ----------\n        feature_names_in : list[str]\n        min_: nwt.DataFrameT\n        scale_ : nwt.DataFrameT\n\n        Returns\n        -------\n        self\n            Fitted MinMaxScaler\n        \"\"\"\n        X = nw.from_native(X, eager_only=True)\n\n        self.feature_names_in_ = _resolve_columns(X, columns)\n        data_min = X.select(nw.col(self.feature_names_in_).min())\n        data_max = X.select(nw.col(self.feature_names_in_).max())\n        data_range: nwt.DataFrameT = data_max.select(\n            nw.col(c).__sub__(data_min[c]) for c in self.feature_names_in_\n        )\n\n        self.scale_ = data_range.with_columns(\n            nw.lit(self._range_diff).__truediv__(nw.col(c)).alias(c)\n            for c in self.feature_names_in_\n        )\n\n        self.min_ = data_min.select(\n            nw.lit(self._range_min).__sub__(nw.col(c).__mul__(self.scale_[c])).alias(c)\n            for c in self.feature_names_in_\n        )\n\n        return self\n\n    @nw.narwhalify\n    def transform(self, X: nwt.IntoFrameT) -&gt; nwt.IntoFrameT:\n        return X.with_columns(\n            nw.col(c).__mul__(self.scale_[c]).__add__(self.min_[c])\n            for c in self.feature_names_in_\n        )\n\n    @nw.narwhalify\n    def fit_transform(self, X: nwt.IntoDataFrameT) -&gt; nwt.IntoDataFrameT:\n        return self.fit(X).transform(X)\n\n    @nw.narwhalify\n    def inverse_transform(self, X: nwt.IntoFrameT) -&gt; nwt.IntoFrameT:\n        return X.with_columns(\n            nw.col(c).__sub__(self.min_[c]).__truediv__(self.scale_[c])\n            for c in self.feature_names_in_\n        )\n\n    def _run_one(self, c: str) -&gt; nw.Expr:\n        expr = nw.col(c)\n        min_ = expr.min()\n\n        return (\n            expr.__sub__(min_)\n            .__truediv__(expr.max().__sub__(min_))\n            .__mul__(self._range_diff)\n            .__add__(self._range_min)\n        )\n\n    @nw.narwhalify\n    def run(\n        self, X: nwt.IntoFrameT, columns: Optional[str | Iterable[str]] = None\n    ) -&gt; nwt.IntoFrameT:\n        return X.with_columns(self._run_one(c) for c in _resolve_columns(X, columns))\n\n    def save(self, path: PathLike):\n        \"\"\"_summary_\n\n        Parameters\n        ----------\n        path : PathLike\n            _description_\n\n        Returns\n        -------\n        _type_\n            _description_\n\n        Added in version 0.2.0\n        ----------------------\n        \"\"\"\n        with zipfile.ZipFile(\n            path, \"w\"\n        ) as archive, tempfile.TemporaryDirectory() as tmpdir:\n            tmpdir = Path(tmpdir)\n\n            self.min_.write_parquet(tmpdir / \"min_.parquet\")\n            self.scale_.write_parquet(tmpdir / \"scale_.parquet\")\n            _write_json(\n                {\n                    \"feature_names_in_\": self.feature_names_in_,\n                    \"feature_range\": self.feature_range,\n                },\n                tmpdir / \"instance_vars.json\",\n            )\n\n            archive.write(tmpdir / \"min_.parquet\", \"min_.parquet\")\n            archive.write(tmpdir / \"scale_.parquet\", \"scale_.parquet\")\n            archive.write(tmpdir / \"instance_vars.json\", \"instance_vars.json\")\n\n        return self\n\n    def load(self, path: PathLike):\n        \"\"\"_summary_\n\n        Parameters\n        ----------\n        path : PathLike\n            _description_\n\n        Returns\n        -------\n        _type_\n            _description_\n\n        Added in version 0.2.0\n        ----------------------\n        \"\"\"\n        with zipfile.ZipFile(\n            path, \"r\"\n        ) as archive, tempfile.TemporaryDirectory() as tmpdir:\n            archive.extractall(tmpdir)\n\n            self.min_ = nw.read_parquet(f\"{tmpdir}/min_.parquet\", native_namespace=pl)\n            self.scale_ = nw.read_parquet(\n                f\"{tmpdir}/scale_.parquet\", native_namespace=pl\n            )\n            instance_vars = _read_json(f\"{tmpdir}/instance_vars.json\")\n            self.feature_names_in_ = instance_vars[\"feature_names_in_\"]\n            self.feature_range = tuple(instance_vars[\"feature_range\"])\n\n            self._set_range_vars()\n\n        return self\n</code></pre>"},{"location":"preprocessing.html#rapidstats.preprocessing.MinMaxScaler.fit","title":"<code>fit(X, columns=None)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>IntoDataFrameT</code> <p>description</p> required <p>Attributes:</p> Name Type Description <code>feature_names_in</code> <code>list[str]</code> <code>min_</code> <code>DataFrameT</code> <code>scale_</code> <code>DataFrameT</code> <p>Returns:</p> Type Description <code>self</code> <p>Fitted MinMaxScaler</p> Source code in <code>python/rapidstats/preprocessing.py</code> <pre><code>def fit(self, X: nwt.IntoDataFrameT, columns: Optional[str | Iterable[str]] = None):\n    \"\"\"_summary_\n\n    Parameters\n    ----------\n    X : nwt.IntoDataFrameT\n        _description_\n\n    Attributes\n    ----------\n    feature_names_in : list[str]\n    min_: nwt.DataFrameT\n    scale_ : nwt.DataFrameT\n\n    Returns\n    -------\n    self\n        Fitted MinMaxScaler\n    \"\"\"\n    X = nw.from_native(X, eager_only=True)\n\n    self.feature_names_in_ = _resolve_columns(X, columns)\n    data_min = X.select(nw.col(self.feature_names_in_).min())\n    data_max = X.select(nw.col(self.feature_names_in_).max())\n    data_range: nwt.DataFrameT = data_max.select(\n        nw.col(c).__sub__(data_min[c]) for c in self.feature_names_in_\n    )\n\n    self.scale_ = data_range.with_columns(\n        nw.lit(self._range_diff).__truediv__(nw.col(c)).alias(c)\n        for c in self.feature_names_in_\n    )\n\n    self.min_ = data_min.select(\n        nw.lit(self._range_min).__sub__(nw.col(c).__mul__(self.scale_[c])).alias(c)\n        for c in self.feature_names_in_\n    )\n\n    return self\n</code></pre>"},{"location":"preprocessing.html#rapidstats.preprocessing.MinMaxScaler.load","title":"<code>load(path)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>description</p> required <p>Returns:</p> Type Description <code>_type_</code> <p>description</p> Added in version 0.2.0 Source code in <code>python/rapidstats/preprocessing.py</code> <pre><code>def load(self, path: PathLike):\n    \"\"\"_summary_\n\n    Parameters\n    ----------\n    path : PathLike\n        _description_\n\n    Returns\n    -------\n    _type_\n        _description_\n\n    Added in version 0.2.0\n    ----------------------\n    \"\"\"\n    with zipfile.ZipFile(\n        path, \"r\"\n    ) as archive, tempfile.TemporaryDirectory() as tmpdir:\n        archive.extractall(tmpdir)\n\n        self.min_ = nw.read_parquet(f\"{tmpdir}/min_.parquet\", native_namespace=pl)\n        self.scale_ = nw.read_parquet(\n            f\"{tmpdir}/scale_.parquet\", native_namespace=pl\n        )\n        instance_vars = _read_json(f\"{tmpdir}/instance_vars.json\")\n        self.feature_names_in_ = instance_vars[\"feature_names_in_\"]\n        self.feature_range = tuple(instance_vars[\"feature_range\"])\n\n        self._set_range_vars()\n\n    return self\n</code></pre>"},{"location":"preprocessing.html#rapidstats.preprocessing.MinMaxScaler.save","title":"<code>save(path)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>description</p> required <p>Returns:</p> Type Description <code>_type_</code> <p>description</p> Added in version 0.2.0 Source code in <code>python/rapidstats/preprocessing.py</code> <pre><code>def save(self, path: PathLike):\n    \"\"\"_summary_\n\n    Parameters\n    ----------\n    path : PathLike\n        _description_\n\n    Returns\n    -------\n    _type_\n        _description_\n\n    Added in version 0.2.0\n    ----------------------\n    \"\"\"\n    with zipfile.ZipFile(\n        path, \"w\"\n    ) as archive, tempfile.TemporaryDirectory() as tmpdir:\n        tmpdir = Path(tmpdir)\n\n        self.min_.write_parquet(tmpdir / \"min_.parquet\")\n        self.scale_.write_parquet(tmpdir / \"scale_.parquet\")\n        _write_json(\n            {\n                \"feature_names_in_\": self.feature_names_in_,\n                \"feature_range\": self.feature_range,\n            },\n            tmpdir / \"instance_vars.json\",\n        )\n\n        archive.write(tmpdir / \"min_.parquet\", \"min_.parquet\")\n        archive.write(tmpdir / \"scale_.parquet\", \"scale_.parquet\")\n        archive.write(tmpdir / \"instance_vars.json\", \"instance_vars.json\")\n\n    return self\n</code></pre>"},{"location":"preprocessing.html#rapidstats.preprocessing.OneHotEncoder","title":"<code>OneHotEncoder</code>","text":"<p>One-hot encodes data.</p> Added in version 0.1.0 <p>Methods:</p> Name Description <code>load</code> <p>summary</p> Source code in <code>python/rapidstats/preprocessing.py</code> <pre><code>class OneHotEncoder:\n    \"\"\"One-hot encodes data.\n\n    Added in version 0.1.0\n    ----------------------\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def fit(self, X: nwt.IntoDataFrameT, columns: Optional[str | Iterable[str]] = None):\n        X = nw.from_native(X, eager_only=True)\n\n        self.categories_ = {\n            c: X[c].drop_nulls().unique() for c in _resolve_columns(X, columns)\n        }\n\n        return self\n\n    @nw.narwhalify\n    def transform(self, X: nwt.IntoFrameT) -&gt; nwt.IntoFrameT:\n        for c, unique_vals in self.categories_.items():\n            for val in unique_vals:\n                X = X.with_columns(nw.col(c).__eq__(val).alias(f\"{c}_{val}\"))\n\n        return X\n\n    @nw.narwhalify\n    def fit_transform(\n        self, X: nwt.IntoDataFrameT, columns: Optional[str | Iterable[str]] = None\n    ) -&gt; nwt.IntoDataFrameT:\n        return self.fit(X, columns=columns).transform(X)\n\n    def save(self, path: PathLike):\n        with zipfile.ZipFile(\n            path, \"w\"\n        ) as archive, tempfile.TemporaryDirectory() as tmpdir:\n            tmpdir = Path(tmpdir)\n\n            for k, v in self.categories_.items():\n                v.to_frame().write_parquet(tmpdir / k)\n                archive.write(tmpdir / k, k)\n\n        return self\n\n    def load(self, path: PathLike):\n        \"\"\"_summary_\n\n        Parameters\n        ----------\n        path : PathLike\n            _description_\n\n        Returns\n        -------\n        Self\n            _description_\n\n        Added in version 0.2.0\n        ----------------------\n        \"\"\"\n        with zipfile.ZipFile(\n            path, \"r\"\n        ) as archive, tempfile.TemporaryDirectory() as tmpdir:\n            archive.extractall(tmpdir)\n\n            self.categories_ = {\n                file: nw.read_parquet(f\"{tmpdir}/{file}\", native_namespace=pl)[file]\n                for file in archive.namelist()\n            }\n\n        return self\n</code></pre>"},{"location":"preprocessing.html#rapidstats.preprocessing.OneHotEncoder.load","title":"<code>load(path)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>description</p> required <p>Returns:</p> Type Description <code>Self</code> <p>description</p> Added in version 0.2.0 Source code in <code>python/rapidstats/preprocessing.py</code> <pre><code>def load(self, path: PathLike):\n    \"\"\"_summary_\n\n    Parameters\n    ----------\n    path : PathLike\n        _description_\n\n    Returns\n    -------\n    Self\n        _description_\n\n    Added in version 0.2.0\n    ----------------------\n    \"\"\"\n    with zipfile.ZipFile(\n        path, \"r\"\n    ) as archive, tempfile.TemporaryDirectory() as tmpdir:\n        archive.extractall(tmpdir)\n\n        self.categories_ = {\n            file: nw.read_parquet(f\"{tmpdir}/{file}\", native_namespace=pl)[file]\n            for file in archive.namelist()\n        }\n\n    return self\n</code></pre>"},{"location":"preprocessing.html#rapidstats.preprocessing.StandardScaler","title":"<code>StandardScaler</code>","text":"<p>summary</p> <p>Null</p> <p><code>rapidstats</code> uses narwhals to ingest supported DataFrames. However, null-handling can differ across backends. For example, if using a Polars backend, NaNs are valid numbers, not missing. Therefore, the mean / standard deviation of a column with NaNs will be NaN. Ensure your input is sanitized according to your specific backend before using <code>StandardScaler</code>.</p> Source code in <code>python/rapidstats/preprocessing.py</code> <pre><code>class StandardScaler:\n    \"\"\"_summary_\n\n    !!! Null Handling\n        `rapidstats` uses [narwhals](https://narwhals-dev.github.io/narwhals/) to ingest\n        supported DataFrames. However, null-handling can differ across backends. For\n        example, if using a Polars backend, NaNs are valid numbers, not missing.\n        Therefore, the mean / standard deviation of a column with NaNs will be NaN.\n        Ensure your input is sanitized according to your specific backend before using\n        `StandardScaler`.\n    \"\"\"\n\n    def __init__(self, ddof: int = 1):\n        self.ddof = ddof\n\n    def fit(self, X: nwt.IntoDataFrame, columns: Optional[str | Iterable[str]] = None):\n        X = nw.from_native(X, eager_only=True)\n        self.feature_names_in_ = _resolve_columns(X, columns)\n        selector = nw.col(self.feature_names_in_)\n\n        self.mean_ = X.select(selector.mean())\n        self.std_ = X.select(selector.std(ddof=self.ddof))\n\n        return self\n\n    @nw.narwhalify\n    def transform(self, X: nwt.IntoFrameT) -&gt; nwt.IntoFrameT:\n        return X.with_columns(\n            nw.col(c).__sub__(self.mean_[c]).__truediv__(self.std_[c])\n            for c in self.feature_names_in_\n        )\n\n    @nw.narwhalify\n    def fit_transform(\n        self, X: nwt.IntoDataFrameT, columns: Optional[str | Iterable[str]] = None\n    ) -&gt; nwt.IntoDataFrameT:\n        return self.fit(X, columns=columns).transform(X)\n\n    @nw.narwhalify\n    def inverse_transform(self, X: nwt.IntoFrameT) -&gt; nwt.IntoFrameT:\n        return X.with_columns(\n            nw.col(c).__mul__(self.std_[c]).__add__(self.mean_[c])\n            for c in self.feature_names_in_\n        )\n\n    def _run_one(self, c: str) -&gt; nw.Expr:\n        expr = nw.col(c)\n\n        return expr.__sub__(expr.mean()).__truediv__(expr.std(ddof=self.ddof))\n\n    @nw.narwhalify\n    def run(\n        self, X: nwt.IntoFrameT, columns: Optional[str | Iterable[str]] = None\n    ) -&gt; nwt.IntoFrameT:\n        return X.with_columns(self._run_one(c) for c in _resolve_columns(X, columns))\n\n    def save(self, path: PathLike):\n        with zipfile.ZipFile(\n            path, \"w\"\n        ) as archive, tempfile.TemporaryDirectory() as tmpdir:\n            tmpdir = Path(tmpdir)\n\n            self.mean_.write_parquet(tmpdir / \"mean_.parquet\")\n            self.std_.write_parquet(tmpdir / \"std_.parquet\")\n            _write_json(\n                {\n                    \"feature_names_in_\": self.feature_names_in_,\n                    \"ddof\": self.ddof,\n                },\n                tmpdir / \"instance_vars.json\",\n            )\n\n            archive.write(tmpdir / \"mean_.parquet\", \"mean_.parquet\")\n            archive.write(tmpdir / \"std_.parquet\", \"std_.parquet\")\n            archive.write(tmpdir / \"instance_vars.json\", \"instance_vars.json\")\n\n        return self\n\n    def load(self, path: PathLike):\n        with zipfile.ZipFile(\n            path, \"r\"\n        ) as archive, tempfile.TemporaryDirectory() as tmpdir:\n            archive.extractall(tmpdir)\n\n            self.mean_ = nw.read_parquet(f\"{tmpdir}/mean_.parquet\", native_namespace=pl)\n            self.std_ = nw.read_parquet(f\"{tmpdir}/std_.parquet\", native_namespace=pl)\n            instance_vars = _read_json(f\"{tmpdir}/instance_vars.json\")\n            self.feature_names_in_ = instance_vars[\"feature_names_in_\"]\n            self.ddof = instance_vars[\"ddof\"]\n\n        return self\n</code></pre>"}]}